{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7dca33",
   "metadata": {},
   "source": [
    "This notebook is meant to be used to register copernicus data in csv files.\n",
    "\n",
    "Data source : https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d98c0c",
   "metadata": {},
   "source": [
    "# User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffb4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_variable = 'pr'\n",
    "name_variable = 'precipitation'\n",
    "\n",
    "temporal_resolution = 'daily'\n",
    "\n",
    "y_start = 1950\n",
    "y_end = 2014\n",
    "\n",
    "# wind register at 10 m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93da690",
   "metadata": {},
   "source": [
    "# Functions and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af061b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import netCDF4 as nc#not directly used but needs to be imported for some nc4 files manipulations, use for nc files\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "import datetime # to have actual date\n",
    "import os\n",
    "import os.path\n",
    "import cdsapi # for copernicus function\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e171a3",
   "metadata": {},
   "source": [
    "# Out path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29003c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path=r'\\\\COWI.net\\projects\\A245000\\A248363\\CRVA\\Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a86ad8",
   "metadata": {},
   "source": [
    "# Project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_projects = np.array(['WTP_Mutua_EIB', 'Gorongosa_EIB', 'Chimoio_WTP_EIB', 'Pemba_EIB'])\n",
    "\n",
    "lon_projects_data = np.array([34.5927839939706, 34.07824286310398 , 33.47333313659342, 40.52545156033736])\n",
    "lon_projects = pd.Series(lon_projects_data)\n",
    "\n",
    "lat_projects_data = np.array([-19.495079648575242, -18.68063728746643, -19.125095255188334,-12.973942656747809])\n",
    "lat_projects = pd.Series(lat_projects_data)\n",
    "buffer_area_project=2\n",
    "area_projects = [lat_projects - buffer_area_project, lat_projects+buffer_area_project, lon_projects-buffer_area_project,lon_projects+buffer_area_project] # list format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62069e00",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e49fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52a3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fa0c2a8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ac8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Register data from nc file of Copernicus ############################################\n",
    "# Aim of the function: this function aims to register in a dataframe and a csv file the data from the nc file downloaded with\n",
    "# the function copernicus_data\n",
    "# Actions of this function\n",
    "#     1) Create the string indicating the period of interest\n",
    "#     2) Creating path and file name to register dataframe in csv file\n",
    "#     3) Register data, with its corresponding experiments and models, in dataframe and csv file\n",
    "#        3 a) Test if path does not exists (if dataframe is not registered) : \n",
    "#                1 . Thanks to copernicus_data, download nc fils from copernicus CMIP6 website for each experiment and each model\n",
    "#                2 . Open the dowloaded nc file in the jupyter notebook if it exists\n",
    "#                3 . In a dataframe, register the value in the nc file, for each experiment, model and day\n",
    "#                4 . If there no value for each experiments and models tested, the datfram is empty and the user is informed\n",
    "#        3 b) Test if path exists (dataframe is registered) : no need to register again, return in dataframe the existing \n",
    "#             csv file in a dataframe\n",
    "\n",
    "# Parameters of the function\n",
    "# temporal_resolution: 'daily', 'monthly', or 'fixed'. String type \n",
    "# year_str: list containing all the years under the string type and in the period of interest\n",
    "# experiments: copernicus_elements.experiments\n",
    "# models: copernicus_elements.models\n",
    "# out_path: path were the outputs are registered. Defined by the user at the beginning of the code \n",
    "# global_variable: global name of the climate variable of interest (example: Wind)\n",
    "# name_variable: name of the elements downloaded from copernicus (example: 'near_surface_wind_speed')\n",
    "# name_project: Name of the project for which the data are taken\n",
    "# area: list containing latitudes and logitudes around the project\n",
    "\n",
    "def csv_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_projects,area,lat_projects,lon_projects,source = 'Copernicus-CMIP6',name_area='all-Mozambique'):    \n",
    "    df_final = []\n",
    "    \n",
    "    # create string for name of folder depending on type of period\n",
    "    if temporal_resolution == 'fixed':\n",
    "        period = 'fixed'\n",
    "    else:\n",
    "        period=year_str[0]+'-'+year_str[len(year_str)-1]\n",
    "    \n",
    "    (dates, index_dates)=date_copernicus(temporal_resolution,year_str) # create time vector depending on temporal resolution\n",
    "    \n",
    "    #k = 0 # to find closest latitude and longitude for each project, without making the loop for each, ssp, each model, year and project\n",
    "    #i = 0 # to have indexes of projects    \n",
    "    r''''    \n",
    "    for name_project in name_projects:\n",
    "        print('############################### Project name: '+name_project+' ###############################')\n",
    "        \n",
    "        # modification on name_project str to ensure no problem whent using this str as name of a folder\n",
    "        name_project = name_project.replace('-','_') # take off every blank space of project names\n",
    "        name_project = name_project.replace('/','_') # take off every / of project names\n",
    "        name_project = name_project.replace(r'\"\\\"','_') # take off every \\ of project names\n",
    "        # brackets shouldn't be a problem for name projects'''\n",
    "\n",
    "    title_file = name_project +'_' +period+ '_' + temporal_resolution + '_' +name_variable+'.csv'\n",
    "\n",
    "    path_for_csv = os.path.join(out_path,'csv',source,name_variable,name_project,period) # create path for csv file\n",
    "    \n",
    "    for SSP\n",
    "        for model_simulation\n",
    "            path_for_file= os.path.join(out_path,name_variable,source,period,SSP,model_simulation,name_area)\n",
    "            path_nc_file = search_for_nc(path_for_file)\n",
    "        r'''\n",
    "        if not os.path.isdir(path_for_csv): # test if the path for csv already exists; if not, first part if the if is applied\n",
    "            os.makedirs(path_for_csv) # to ensure the creation of the path\n",
    "            # the dataframe_copernicus functions aims to test if the data with the specific parameters exists (with copernicus_data)\n",
    "            # and then produce a csv file if the data exists\n",
    "            if k == 0:\n",
    "                (df,k,index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon)=dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,[area[0][k],area[1][k],area[2][k],area[3][k]],lat_projects[k],lon_projects[k],period,index_dates,dates,path_for_csv,title_file,source,k,i)\n",
    "            if k ==1:\n",
    "                (df,k,index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon)=dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,[area[0][k],area[1][k],area[2][k],area[3][k]],lat_projects[k],lon_projects[k],period,index_dates,dates,path_for_csv,title_file,source,k,i,index_closest_lat_d=index_closest_lat,index_closest_lon_d=index_closest_lon,closest_value_lat_d=closest_value_lat,closest_value_lon_d=closest_value_lon)\n",
    "                \n",
    "            #return df\n",
    "        else:# test if the data were already downloaded; if yes, this part of the if is applied\n",
    "            if len(os.listdir(path_for_csv)) == 0: #test if the directory is empty\n",
    "                # the csv file does not exist, even if the path exist\n",
    "                # the dataframe_copernicus functions aims to test if the data with the specific parameters exists (with copernicus_data)\n",
    "                # and then produce a csv file if the data exists\n",
    "                if k == 0:\n",
    "                    (df,k,index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon)=dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,[area[0][k],area[1][k],area[2][k],area[3][k]],lat_projects[k],lon_projects[k],period,index_dates,dates,path_for_csv,title_file,source,k,i)\n",
    "                    \n",
    "                if k == 1:\n",
    "                    (df,k,index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon)=dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,[area[0][k],area[1][k],area[2][k],area[3][k]],lat_projects[k],lon_projects[k],period,index_dates,dates,path_for_csv,title_file,source,k,i,index_closest_lat_d=index_closest_lat,index_closest_lon_d=index_closest_lon,closest_value_lat_d=closest_value_lat,closest_value_lon_d=closest_value_lon)\n",
    "            else: # the directory is not empty\n",
    "                df=file_already_downloaded(path_for_csv,title_file,name_variable)\n",
    "                \n",
    "        #df_final = pd.concat([df_final,df])\n",
    "        i+=1 # iterate indexes projects'''\n",
    "\n",
    "    return df#df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem with register_data function --> problem with format of time, vaires between files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adffc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register data concerning each project under the form of a csv, with the model, scenario, period, latitude and longitude\n",
    "def register_data(climate_variable_path,name_project,name_variable,index_dates,dates,experiment,model,index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon,df,i):\n",
    "    print('Registering the data in a dataframe')\n",
    "    #Open_path = Dataset(climate_variable_path) # open netcdf file\n",
    "    #lat_dataframe = np.ma.getdata(Open_path.variables['lat']).data\n",
    "    #lon_dataframe = np.ma.getdata(Open_path.variables['lon']).data\n",
    "    #column_name = find_column_name(Open_path)\n",
    "    #data_with_all = ma.getdata(Open_path.variables[column_name]).data\n",
    "    \n",
    "    ds = xr.open_dataset(climate_variable_path)\n",
    "    r'''\n",
    "    for moment in index_dates: # case if temporal resolution is daily\n",
    "        data_dataframe = ds.variables[global_variable].isel(time=moment,lat=index_closest_lat[i],lon=index_closest_lon[i]) # data_with_all[moment,:,:]\n",
    "        Date = (dates[moment],) # create tuple for iteration of dataframe\n",
    "        Name_Project = (name_project,)\n",
    "\n",
    "        # Create the MultiIndex\n",
    "        midx = pd.MultiIndex.from_product([Name_Project,closest_value_lat[i],closest_value_lon[i],experiment, model, Date],names=['Name project', 'Latitude', 'Longitude','Experiment', 'Model', 'Date'])\n",
    "        # multiindex to name the columns\n",
    "        cols_str = [name_variable]\n",
    "        #cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "        # Create the Dataframe\n",
    "        Variable_dataframe = pd.DataFrame(data = data_dataframe, \n",
    "                                    index = midx,\n",
    "                                    columns = cols_str)\n",
    "        Variable_dataframe\n",
    "        # Concatenate former and new dataframe\n",
    "        df = pd.concat([df,Variable_dataframe])# register information for project\n",
    "    '''\n",
    "    conversion_factor = 1\n",
    "    if global_variable =='pr':\n",
    "        conversion_factor = 86400\n",
    "        # convert precipitation data from kg.m^(-2).s^(-1) to mm/day :  1 kg/m2/s = 86400 mm/day\n",
    "    data_dataframe = ds.variables[global_variable].isel(lat=index_closest_lat[i],lon=index_closest_lon[i]).values*conversion_factor # data_with_all[moment,:,:]\n",
    "    # missing 29.02 ?\n",
    "    if len(ds.variables['time'].values)<len(index_dates):\n",
    "        max(ds.indexes['time'].year)\n",
    "        \n",
    "        max(ds.indexes['time'].day)\n",
    "            # yes, missing 29.02\n",
    "            for j in np.where((dates.month == 2) & (dates.day ==29))[0]:\n",
    "                data_dataframe=np.insert(data_dataframe,j,np.nan)\n",
    "    Date = dates.tolist() # create tuple for iteration of dataframe\n",
    "    Name_Project = (name_project,)\n",
    "    \n",
    "    print('\\ni = '+ str(i))\n",
    "    print('\\nclosest_value_lat[i]'+str(closest_value_lat[i]))\n",
    "    print('\\ntype(closest_value_lat[i])'+str(type(closest_value_lat[i])))\n",
    "    \n",
    "    # Create the MultiIndex\n",
    "    midx = pd.MultiIndex.from_product([Name_Project,(closest_value_lat[i],),(closest_value_lon[i],),experiment, model, Date],names=['Name project', 'Latitude', 'Longitude','Experiment', 'Model', 'Date'])\n",
    "    # multiindex to name the columns\n",
    "    cols_str = [name_variable]\n",
    "    #cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "    # Create the Dataframe\n",
    "    Variable_dataframe = pd.DataFrame(data = data_dataframe, \n",
    "                                index = midx,\n",
    "                                columns = cols_str)\n",
    "    Variable_dataframe\n",
    "    # Concatenate former and new dataframe\n",
    "    df = pd.concat([df,Variable_dataframe])# register information for project\n",
    "    \n",
    "    ds.close() # to spare memory\n",
    "    #Open_path.close # to spare memory\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return column name in the netCDF file\n",
    "# all netCDF file form copernicus have this format for their variables names\n",
    "# ['time', 'time_bnds', 'lat', 'lat_bnds', 'lon', 'lon_bnds', Name of climate variable of interest]\n",
    "# take of 'time', 'time_bnds', 'lat', 'lat_bnds', 'lon', 'lon_bnds'\n",
    "def find_column_name(Open_path):\n",
    "    # make a list with every variables of the netCDF file of interest\n",
    "    climate_variable_variables=list(Open_path.variables)\n",
    "    # variables that are not the column name of interest \n",
    "    elements_not_climate_var =['time', 'time_bnds', 'bnds','lat', 'lat_bnds', 'lon', 'lon_bnds','time_bounds','bounds','lat_bounds','lon_bounds','height']\n",
    "    for str in elements_not_climate_var:\n",
    "        if str in climate_variable_variables:\n",
    "            climate_variable_variables.remove(str)\n",
    "    return climate_variable_variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ef1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_already_downloaded(path_for_csv,title_file,name_variable):\n",
    "    print('The file was already downloaded')\n",
    "    df = pd.read_csv(os.path.join(path_for_csv,title_file)) # read the downloaded data for the analysis\n",
    "\n",
    "    # changing name of columns\n",
    "    name_columns=df.iloc[0].array\n",
    "    df.rename(columns={'Unnamed: 0':'Experiment','Unnamed: 1':'Model','Unnamed: 2':'Date','Unnamed: 3':'Latitude'}, inplace=True)\n",
    "\n",
    "    lon_dataframe=name_columns[4:len(name_columns)] # register data for columns of multiindex\n",
    "\n",
    "    df.drop([0,1], axis=0,inplace=True) # remove 2 first lines\n",
    "\n",
    "    # recreate multiindex \n",
    "\n",
    "    # .... with columns\n",
    "    df.set_index(['Name project', 'Latitude', 'Longitude','Experiment', 'Model', 'Date'],inplace=True)\n",
    "\n",
    "    # .... with lines\n",
    "\n",
    "    cols_str = [name_variable]\n",
    "    df.columns=cols_str\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d784f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seach_for_nc is a function looking in path_for_file for a document in .nc format\n",
    "\n",
    "def search_for_nc(path_for_file):\n",
    "    print('path_for_file does exist Function copernicus search for nc')\n",
    "    for file in os.listdir(path_for_file):\n",
    "        if file.endswith(\".nc\"):\n",
    "            final_path=os.path.join(path_for_file, file)\n",
    "            \n",
    "            print('The file is in the path Function copernicus search for nc\\n')\n",
    "            print('Before path_length, The final path for the nc file is: '+final_path)\n",
    "            answer = str(os.path.isfile(final_path))\n",
    "            print('\\n The final path for nc file exists ? '+answer+'\\n')\n",
    "            final_path=path_length(final_path) # check if length of path is too long\n",
    "            print('After path_length, The final path for the nc file is: '+final_path)\n",
    "            answer = str(os.path.isfile(final_path))\n",
    "            print('\\n The final path for nc file exists ? '+answer+'\\n')\n",
    "            return final_path # the function returns the path of the nc file of interest\n",
    "            break # stop the function if a nc file was found \n",
    "        else:\n",
    "            pass\n",
    "    # the all folder has been search and there is no nc file in it\n",
    "    print('Problem : No nc file was found Function copernicus Function copernicus search for nc')# this line is out of the for loop, \n",
    "    #because it should only appear once all the folder has been examinated and if the break of the if was not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f286a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions test if the path is too long\n",
    "# if the path is more than 250 char, the path wll be modified in order for windows to accept is as a path\n",
    "\n",
    "def path_length(str1):\n",
    "    if len(str1)>250:\n",
    "        path = os.path.abspath(str1) # normalize path\n",
    "        if path.startswith(u\"\\\\\\\\\"):\n",
    "            path=u\"\\\\\\\\?\\\\UNC\\\\\"+path[2:]\n",
    "        else:\n",
    "            path=u\"\\\\\\\\?\\\\\"+path\n",
    "        return path\n",
    "    else:\n",
    "        return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create path for the downloaded file\n",
    "def create_file_download_path(start_path,name_variable,name_area,SSP,model,year,temporal_resolution,source):\n",
    "    # adapt the name of the folder for the period, depending on the type of period\n",
    "    if len(year)==1:\n",
    "        file_download = os.path.join(start_path,name_variable,source,year,SSP,model,name_area)\n",
    "    elif len(year)>1:\n",
    "        period=year[0]+'-'+year[len(year)-1]\n",
    "        file_download = os.path.join(start_path,name_variable,source,period,SSP,model,name_area)\n",
    "    elif temporal_resolution == 'fixed':\n",
    "        file_download = os.path.join(start_path,name_variable,source,'fixed_period',SSP,model,name_area)\n",
    "    return file_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae966801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
