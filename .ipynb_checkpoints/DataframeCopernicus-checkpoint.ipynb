{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8381d14",
   "metadata": {},
   "source": [
    "This notebook is meant to be used to register copernicus data in csv files.\n",
    "\n",
    "Data source : https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e717d",
   "metadata": {},
   "source": [
    "# User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFULE : NEED TO CHECK IF WORKED FOR FUTURE PRECIPITATION, NEED TO CHECK TEMPERATURE NEED COnversion\n",
    "period_of_interest = 'past' # 'past' or 'future'\n",
    "name_variable = 'near_surface_wind_speed'\n",
    "global_variable = 'Wind'\n",
    "name_dataset_variable = 'sfcWind' # name of the column of interest\n",
    "\n",
    "# 'tas' 'near_surface_air_temperature'\n",
    "# 'tasmax' 'daily_maximum_near_surface_air_temperature'\n",
    "# 'tasmin' 'daily_minimum_near_surface_air_temperature'\n",
    "# 'hurs' 'near_surface_specific_humidity'\n",
    "# 'Wind' 'near_surface_wind_speed'\n",
    "\n",
    "temporal_resolution = 'daily'\n",
    "\n",
    "# wind registered at 10 m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d28dd",
   "metadata": {},
   "source": [
    "# Functions and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import netCDF4 as nc#not directly used but needs to be imported for some nc4 files manipulations, use for nc files\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "import datetime # to have actual date\n",
    "import os\n",
    "import os.path\n",
    "import cdsapi # for copernicus function\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c121e3",
   "metadata": {},
   "source": [
    "# Out path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76dbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path=r'\\\\COWI.net\\projects\\A245000\\A248363\\CRVA\\Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda0a31",
   "metadata": {},
   "source": [
    "# Project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b291a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_projects = np.array(['WTP_Mutua_EIB', 'Gorongosa_EIB', 'Chimoio_WTP_EIB', 'Pemba_EIB'])\n",
    "\n",
    "lon_projects_data = np.array([34.5927839939706, 34.07824286310398 , 33.47333313659342, 40.52545156033736])\n",
    "lon_projects = pd.Series(lon_projects_data)\n",
    "\n",
    "lat_projects_data = np.array([-19.495079648575242, -18.68063728746643, -19.125095255188334,-12.973942656747809])\n",
    "lat_projects = pd.Series(lat_projects_data)\n",
    "buffer_area_project=2\n",
    "area_projects = [lat_projects - buffer_area_project, lat_projects+buffer_area_project, lon_projects-buffer_area_project,lon_projects+buffer_area_project] # list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity_name(name_projects):\n",
    "    for name_project in name_projects:\n",
    "        # modification on name_project str to ensure no problem whent using this str as name of a folder\n",
    "        name_project = name_project.replace('-','_') # take off every blank space of project names\n",
    "        name_project = name_project.replace('/','_') # take off every / of project names\n",
    "        name_project = name_project.replace(r'\"\\\"','_') # take off every \\ of project names\n",
    "        # brackets shouldn't be a problem for name projects'''\n",
    "    return name_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc24e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_projects=check_validity_name(name_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd3077",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617772f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class calendar:\n",
    "    default_month = [ \n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                ]\n",
    "    default_day = [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d63829",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of tuples that will be useful to search which data are available or not\n",
    "# make it tuples to make unchangeable\n",
    "class copernicus_elements:\n",
    "    # there is 58 models\n",
    "    models =('access_cm2','awi_cm_1_1_mr','bcc_csm2_mr','cams_csm1_0','canesm5_canoe','cesm2_fv2','cesm2_waccm_fv2','cmcc_cm2_hr4','cmcc_esm2','cnrm_cm6_1_hr','e3sm_1_0','e3sm_1_1_eca','ec_earth3_aerchem','ec_earth3_veg','fgoals_f3_l','fio_esm_2_0','giss_e2_1_g','hadgem3_gc31_ll','iitm_esm','inm_cm5_0','ipsl_cm6a_lr','kiost_esm','miroc6','miroc_es2l','mpi_esm1_2_hr','mri_esm2_0','norcpm1','noresm2_mm','taiesm1','access_esm1_5','awi_esm_1_1_lr','bcc_esm1','canesm5','cesm2','cesm2_waccm','ciesm','cmcc_cm2_sr5','cnrm_cm6_1','cnrm_esm2_1','e3sm_1_1','ec_earth3','ec_earth3_cc','ec_earth3_veg_lr','fgoals_g3','gfdl_esm4','giss_e2_1_h','hadgem3_gc31_mm','inm_cm4_8','ipsl_cm5a2_inca','kace_1_0_g','mcm_ua_1_0','miroc_es2h','mpi_esm_1_2_ham','mpi_esm1_2_lr','nesm3','noresm2_lm','sam0_unicon','ukesm1_0_ll')\n",
    "    experiments = ('ssp1_1_9','ssp1_2_6','ssp4_3_4','ssp5_3_4os','ssp2_4_5','ssp4_6_0','ssp3_7_0','ssp5_8_5')\n",
    "    experiments_historical=('historical',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if period_of_interest == 'past':\n",
    "    scenarios=copernicus_elements.experiments_historical\n",
    "    y_start = 1950\n",
    "    y_end = 2014\n",
    "if period_of_interest == 'future':\n",
    "    scenarios=copernicus_elements.experiments\n",
    "    y_start = 2015\n",
    "    y_end = 2100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b8368",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Register data from nc file of Copernicus ############################################\n",
    "# Aim of the function: this function aims to register in a dataframe and a csv file the data from the nc file downloaded with\n",
    "# the function copernicus_data\n",
    "# Actions of this function\n",
    "#     1) Create the string indicating the period of interest\n",
    "#     2) Creating path and file name to register dataframe in csv file\n",
    "#     3) Register data, with its corresponding experiments and models, in dataframe and csv file\n",
    "#        3 a) Test if path does not exists (if dataframe is not registered) : \n",
    "#                1 . Thanks to copernicus_data, download nc fils from copernicus CMIP6 website for each experiment and each model\n",
    "#                2 . Open the dowloaded nc file in the jupyter notebook if it exists\n",
    "#                3 . In a dataframe, register the value in the nc file, for each experiment, model and day\n",
    "#                4 . If there no value for each experiments and models tested, the datfram is empty and the user is informed\n",
    "#        3 b) Test if path exists (dataframe is registered) : no need to register again, return in dataframe the existing \n",
    "#             csv file in a dataframe\n",
    "\n",
    "# Parameters of the function\n",
    "# temporal_resolution: 'daily', 'monthly', or 'fixed'. String type \n",
    "# year_str: list containing all the years under the string type and in the period of interest\n",
    "# experiments: copernicus_elements.experiments\n",
    "# models: copernicus_elements.models\n",
    "# out_path: path were the outputs are registered. Defined by the user at the beginning of the code \n",
    "# name_dataset_variable: name of the column of interest in the dataset\n",
    "# name_variable: name of the elements downloaded from copernicus (example: 'near_surface_wind_speed')\n",
    "# global_variable: for exemaple, Wind, or temperature\n",
    "# name_project: Name of the project for which the data are taken\n",
    "# area: list containing latitudes and logitudes around the project\n",
    "\n",
    "def csv_copernicus(temporal_resolution,year_str,experiments,models,out_path, name_dataset_variable, name_variable,global_variable, name_projects,area,lat_projects,lon_projects,source = 'Copernicus-CMIP6',name_area='all-Mozambique'):    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # create string for name of folder depending on type of period\n",
    "    if temporal_resolution == 'fixed':\n",
    "        period = 'fixed'\n",
    "    else:\n",
    "        period=year_str[0]+'-'+year_str[len(year_str)-1]\n",
    "    \n",
    "    conversion_factor = 1\n",
    "    conversion_addition = 0\n",
    "    if name_dataset_variable =='pr':\n",
    "        conversion_factor = 86400\n",
    "        unit = 'mm_per_day'\n",
    "        # convert precipitation data from kg.m^(-2).s^(-1) to mm/day :  1 kg/m2/s = 86400 mm/day\n",
    "    if 'temp' in name_variable.lower():\n",
    "        unit = u'\\N{DEGREE SIGN}C'\n",
    "        conversion_addition = -273.15\n",
    "    if 'wind' in name_variable.lower():\n",
    "        unit = 'm_per_s'\n",
    "        \n",
    "    title_file = 'SomeProjectsInMoz_'+period+ '_' + temporal_resolution + '_' +name_variable+'_'+unit+'.csv'\n",
    "\n",
    "    path_for_csv = os.path.join(out_path,global_variable,name_variable,source,'csv',period) # create path for csv file\n",
    "    \n",
    "    if os.path.isfile(os.path.join(path_for_csv,title_file)):\n",
    "        df = file_already_downloaded(path_for_csv,title_file)\n",
    "        return df\n",
    "    else:\n",
    "        for SSP in os.listdir(os.path.join(out_path,global_variable,name_variable,source,'raw_data',period)):\n",
    "            for model_simulation in os.listdir(os.path.join(out_path,global_variable,name_variable,source,'raw_data',period,SSP)):\n",
    "                path_for_file = os.path.join(out_path,global_variable,name_variable,source,'raw_data',period,SSP,model_simulation,name_area)\n",
    "                path_nc_file = search_for_nc(path_for_file)\n",
    "\n",
    "\n",
    "                (index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon) = _lat_lon(path_nc_file,lat_projects,lon_projects)\n",
    "\n",
    "                ds = xr.open_dataset(path_nc_file)\n",
    "\n",
    "                for (name_project,i) in zip(name_projects,np.arange(0,len(name_projects))):\n",
    "                    data_dataframe = ds.variables[name_dataset_variable].isel(lat=index_closest_lat[i],lon=index_closest_lon[i]).values*conversion_factor+conversion_addition\n",
    "\n",
    "                    # prevent problemes with dates\n",
    "                    # 2 types of problems :\n",
    "                    #     dates and time in ds are not overlaping\n",
    "                    #     missing 29.02 in ds\n",
    "                    if type(ds.indexes['time']) == pd.core.indexes.datetimes.DatetimeIndex:\n",
    "                        dates=ds.indexes['time'].floor('D')\n",
    "                    if type(ds.indexes['time']) == xr.coding.cftimeindex.CFTimeIndex:\n",
    "                        dates=ds.indexes['time'].to_datetimeindex().floor('D')\n",
    "                    else:\n",
    "                        type_ = type(ds.indexes['time'])\n",
    "                        print(f'The type of the time index is {type_}')\n",
    "\n",
    "                    Date = dates.tolist() # create tuple for iteration of dataframe\n",
    "\n",
    "                    Name_Project = (name_project,)\n",
    "\n",
    "                    # Create the MultiIndex\n",
    "                    midx = pd.MultiIndex.from_product([Name_Project,(closest_value_lat[i],),(closest_value_lon[i],),(SSP,), (model_simulation,), Date],names=['Name project', 'Latitude', 'Longitude','Experiment', 'Model', 'Date'])\n",
    "                    # multiindex to name the columns\n",
    "                    cols_str = [name_variable+' '+unit]\n",
    "                    #cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "                    # Create the Dataframe\n",
    "                    Variable_dataframe = pd.DataFrame(data = data_dataframe, \n",
    "                                                index = midx,\n",
    "                                                columns = cols_str)\n",
    "                    # Concatenate former and new dataframe\n",
    "                    df = pd.concat([df,Variable_dataframe])# register information for project\n",
    "\n",
    "                ds.close() # to spare memory\n",
    "            \n",
    "    # register in csv\n",
    "    full_path = os.path.join(path_for_csv,title_file)\n",
    "    if not os.path.isfile(path_for_csv):\n",
    "        os.makedirs(path_for_csv)\n",
    "    df.to_csv(full_path)\n",
    "    print(f'csv file registered here : {full_path}')\n",
    "    return df#df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_already_downloaded(path_for_csv,title_file):\n",
    "    print(f'The file {title_file} exists and is in {path_for_csv}')\n",
    "    df = pd.read_csv(os.path.join(path_for_csv,title_file)) # read the downloaded data for the analysis\n",
    "\n",
    "    df.set_index(['Name project', 'Latitude', 'Longitude','Experiment', 'Model', 'Date'],inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seach_for_nc is a function looking in path_for_file for a document in .nc format\n",
    "\n",
    "def search_for_nc(path_for_file):\n",
    "    print('path_for_file does exist Function copernicus search for nc')\n",
    "    for file in os.listdir(path_for_file):\n",
    "        if file.endswith(\".nc\"):\n",
    "            final_path=os.path.join(path_for_file, file)\n",
    "            \n",
    "            print('The file is in the path Function copernicus search for nc\\n')\n",
    "            print('Before path_length, The final path for the nc file is: '+final_path)\n",
    "            answer = str(os.path.isfile(final_path))\n",
    "            print('\\n The final path for nc file exists ? '+answer+'\\n')\n",
    "            final_path=path_length(final_path) # check if length of path is too long\n",
    "            print('After path_length, The final path for the nc file is: '+final_path)\n",
    "            answer = str(os.path.isfile(final_path))\n",
    "            print('\\n The final path for nc file exists ? '+answer+'\\n')\n",
    "            return final_path # the function returns the path of the nc file of interest\n",
    "            break # stop the function if a nc file was found \n",
    "        else:\n",
    "            pass\n",
    "    # the all folder has been search and there is no nc file in it\n",
    "    print('Problem : No nc file was found Function copernicus Function copernicus search for nc')# this line is out of the for loop, \n",
    "    #because it should only appear once all the folder has been examinated and if the break of the if was not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245eda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions test if the path is too long\n",
    "# if the path is more than 250 char, the path wll be modified in order for windows to accept is as a path\n",
    "\n",
    "def path_length(str1):\n",
    "    if len(str1)>250:\n",
    "        path = os.path.abspath(str1) # normalize path\n",
    "        if path.startswith(u\"\\\\\\\\\"):\n",
    "            path=u\"\\\\\\\\?\\\\UNC\\\\\"+path[2:]\n",
    "        else:\n",
    "            path=u\"\\\\\\\\?\\\\\"+path\n",
    "        return path\n",
    "    else:\n",
    "        return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf44526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions aims to return the closest latitudes and longitudes to the projects, and the respectives index \n",
    "#  in the lat and lon vectors of the file\n",
    "def _lat_lon(path,lat_projects,lon_projects):\n",
    "    ds =  xr.open_dataset(path) \n",
    "    # ds.indexes['time'] gives back CFTimeIndex format, with hours. The strftime('%d-%m-%Y') permits to have time \n",
    "    # as an index, with format '%d-%m-%Y'. The .values permits to have an array\n",
    "    lat  = ds.lat.values\n",
    "    lon  = ds.lon.values\n",
    "    ds.close() # to spare memory\n",
    "    # preallocate space for the future vectors\n",
    "    index_closest_lat = []\n",
    "    index_closest_lon = []\n",
    "    closest_value_lat = []\n",
    "    closest_value_lon = []\n",
    "    for j in np.arange(0,len(lat_projects)):\n",
    "        (A,B)=closest_lat_lon_to_proj(lat_projects[j],lat)\n",
    "        #return lat,lat_projects[j]\n",
    "        index_closest_lat.append(A[0])\n",
    "        closest_value_lat.append(B[0])\n",
    "        (C,D)=closest_lat_lon_to_proj(lon_projects[j],lon)\n",
    "        index_closest_lon.append(C[0])\n",
    "        closest_value_lon.append(D[0])\n",
    "    return index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon\n",
    "\n",
    "\n",
    "# this function aims to select the closest point to the geographical point of the project\n",
    "# the function takes as input \n",
    "#     location_project, which is a numpy.float64\n",
    "#     vector, which is a numpy.ndarray\n",
    "# the function returns\n",
    "#     closest_value[0], a numpy.float64\n",
    "\n",
    "def closest_lat_lon_to_proj(location_project,vector):\n",
    "    # the function any() returns a boolean value. Here, the function test if there are elements in the array \n",
    "    # containing the difference between the vector and the location_project, equal to the minimum of the absolute \n",
    "    # value of the difference between the vector and the location_project\n",
    "    if any(np.where((vector - location_project) == min(abs(vector - location_project)))[0]):\n",
    "        # the function any() returned True\n",
    "        # there is an element in the vector that is equal to the minimum of the absolute value of the difference \n",
    "        # between the vector and the location_project\n",
    "        \n",
    "        # the function np.where() returns the index for which (vector - location_project) == min(abs(vector - location_project))\n",
    "        index_closest = np.where((vector - location_project) == min(abs(vector - location_project)))[0]\n",
    "        closest_value = vector[index_closest]\n",
    "    else:\n",
    "        # the function any() returned False\n",
    "        # there is NO element in the vector that is equal to the minimum of the absolute value of the difference \n",
    "        # between the vector and the location_project\n",
    "        \n",
    "        # the function np.where() returns the index for which (vector - location_project) == -min(abs(vector - location_project))\n",
    "        index_closest = np.where((vector - location_project) == -min(abs(vector - location_project)))[0]\n",
    "        closest_value = vector[index_closest]\n",
    "    return index_closest, closest_value \n",
    "    # the function returns\n",
    "    #     first, the value of the index of the element of vector, that is the closest to location_project    \n",
    "    #     second, the array containing the element of vector, that is the closest to location_project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec459c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Period for copernicus function ################################################\n",
    "# Aim of the function: by giving it a first and last year of the period that must analyzed, this function produce several \n",
    "# vectors,containing time informations, useful to download and treat data from CMIP6 projections (https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview )\n",
    "# Those time vectors are used in the copernicus_data and the dataframe_copernicus and csv_copernicus functions\n",
    "\n",
    "# function year_copernicus produce \n",
    "# year: a vector containing all the year in the period of interest\n",
    "# year_str: an array containing all the year in the period of interest in the string format\n",
    "# index: an array containing the index of the year and year_str\n",
    "#### Parameters of the function\n",
    "# first_year: number in int format, of the first year of the period of interest\n",
    "# last_year: number in int format, of the last year of the period of interest\n",
    "def year_copernicus(first_year,last_year):\n",
    "    year = np.arange(first_year,(last_year+1),1) # create vector of years\n",
    "    year_str = [0]*len(year) # create initiale empty vector to convert years in int\n",
    "    index = np.arange(0,len(year)) # create vector of index for year\n",
    "    i = 0 # initialize index\n",
    "    for i in index: # convert all the date in string format\n",
    "        year_str[i]=str(year[i])\n",
    "    return (year, year_str, index)\n",
    "\n",
    "# function date_copernicus produce \n",
    "# dates: the format depend on the temporal reolution, but always contain the dates of the period of interest.\n",
    "#        with temporal_resolution=daily, dates is a DatetimeIndex\n",
    "#        with temporal_resolution=monthly, dates is a list\n",
    "# index_dates: an array containing the index of the dates\n",
    "#### Parameters of the function\n",
    "# temporal_resolution: daily or monthly\n",
    "# year_str: ???? produce by function year_copernicus, containing the year of the period of interest in string format\n",
    "def date_copernicus(temporal_resolution,year_str):\n",
    "    start_date = \"01-01-\"+year_str[0] # string start date based on start year\n",
    "    stop_date = \"31-12-\"+year_str[len(year_str)-1] # string stop date based on stop year\n",
    "    if temporal_resolution =='daily':\n",
    "        # vector of dates between start date and stop date\n",
    "        dates = pd.date_range(start_date,stop_date)# dates is a pandas.core.indexes.datetimes.DatetimeIndex\n",
    "        # By default, freq = 'D', which means calendar day frequency (source : https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases)\n",
    "        #index_dates = np.arange(0,len(dates)) # vector containning index o dates vector\n",
    "    if temporal_resolution =='monthly':\n",
    "        dates = pd.date_range(start_date,stop_date,freq='MS') # vector of dates between start date and stop date\n",
    "        dates=list(dates.strftime('%m-%Y')) # dates is an pandas.core.indexes.base.Index, not a pandas.core.indexes.datetimes.DatetimeIndex\n",
    "    #if temporal_resolution =='fixed': trouver donnees pour gerer cela\n",
    "    index_dates = np.arange(0,len(dates)) # vector containning index o dates vector\n",
    "    return (dates, index_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5daf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ccf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(year, year_str, index)=year_copernicus(y_start,y_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391429d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=csv_copernicus(temporal_resolution,year_str,scenarios,copernicus_elements.models,out_path, name_dataset_variable, name_variable,global_variable, name_projects,area_projects,lat_projects,lon_projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92caf5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd82324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
