{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d17a288",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb0ba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CLMRX\\AppData\\Local\\Temp\\1\\ipykernel_175728\\4193067117.py:4: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "#Import python packages\n",
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import rioxarray #used when calling ncdata.rio.write_crs\n",
    "import xarray as xr\n",
    "import os\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc#not directly used but needs to be imported for some nc4 files manipulations, use for nc files\n",
    "from netCDF4 import Dataset\n",
    "import csv #REMOVE ? not in use ?\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import shutil # to move folders\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore the warnings\n",
    "import cdsapi # for copernicus function\n",
    "import datetime # to have actual date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f7b9c",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffde9fc",
   "metadata": {},
   "source": [
    "### Calendar class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d80deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to define parameter of time that remain constant durinf the whole script\n",
    "class calendar:\n",
    "    default_month = [ \n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                ]\n",
    "    default_day = [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "                ]\n",
    "    actual_date = datetime.date.today()\n",
    "    actual_year = actual_date.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0c9aa",
   "metadata": {},
   "source": [
    "### Map class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f3b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class contains all the latitude and logitude needed to do a map\n",
    "class map_elements:\n",
    "    parallels = np.arange(-360,360,10) # make latitude lines ever 10 degrees\n",
    "    meridians = np.arange(-360,360,10) # make longitude lines every 10 degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6602a",
   "metadata": {},
   "source": [
    "### Copernicus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3080d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of tuples that will be useful to search which data are available or not\n",
    "# make it tuples to make unchangeable\n",
    "class copernicus_elements:\n",
    "    # there is 58 models\n",
    "    models =('access_cm2','awi_cm_1_1_mr','bcc_csm2_mr','cams_csm1_0','canesm5_canoe','cesm2_fv2','cesm2_waccm_fv2','cmcc_cm2_hr4','cmcc_esm2','cnrm_cm6_1_hr','e3sm_1_0','e3sm_1_1_eca','ec_earth3_aerchem','ec_earth3_veg','fgoals_f3_l','fio_esm_2_0','giss_e2_1_g','hadgem3_gc31_ll','iitm_esm','inm_cm5_0','ipsl_cm6a_lr','kiost_esm','miroc6','miroc_es2l','mpi_esm1_2_hr','mri_esm2_0','norcpm1','noresm2_mm','taiesm1','access_esm1_5','awi_esm_1_1_lr','bcc_esm1','canesm5','cesm2','cesm2_waccm','ciesm','cmcc_cm2_sr5','cnrm_cm6_1','cnrm_esm2_1','e3sm_1_1','ec_earth3','ec_earth3_cc','ec_earth3_veg_lr','fgoals_g3','gfdl_esm4','giss_e2_1_h','hadgem3_gc31_mm','inm_cm4_8','ipsl_cm5a2_inca','kace_1_0_g','mcm_ua_1_0','miroc_es2h','mpi_esm_1_2_ham','mpi_esm1_2_lr','nesm3','noresm2_lm','sam0_unicon','ukesm1_0_ll')\n",
    "    experiments = ('ssp1_1_9','ssp1_2_6','ssp4_3_4','ssp5_3_4os','ssp2_4_5','ssp4_6_0','ssp3_7_0','ssp5_8_5')\n",
    "    #'ssp1_1_9',\n",
    "    experiments_historical=('historical',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8168454",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a55390",
   "metadata": {},
   "source": [
    "### read_cckp_ncdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3032e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def read cckp (world bank) nc files\n",
    "#reads data from world bank climate knowledge portal, nc files, with a single band\n",
    "#assigns projection and exports to tif since zonal_stats seems to have issues with it otherwise (not ideal solution)\n",
    "def read_cckp_ncdata(nc_path,output='tempfile.tif'):\n",
    "    with rioxarray.open_rasterio(nc_path,decode_times=False)[0] as ncdata:\n",
    "        ncdata.rio.write_crs('EPSG:4326', inplace=True)\n",
    "        ncdata=ncdata.isel(time=0)\n",
    "        ncdata.rio.to_raster(output)\n",
    "       # output=output #here\n",
    "   # else: \n",
    "      #  print(nc_path,\"not found\") # in this case, the data printed in the table will apply to the previous print.. \n",
    "       # output=0 #here\n",
    "    return output       \n",
    "\n",
    "#def read nc files (copernicus)\n",
    "#reads data from CMIP6 Copernicus, nc files\n",
    "#assigns projection and exports to tif since zonal_stats seems to have issues with it otherwise (not ideal solution)\n",
    "def read_nc_data(nc_path,stats,output='tempfile.tif'):\n",
    "    with rioxarray.open_rasterio(nc_path,decode_times=False)[3] as ncdata:\n",
    "        # calculate statistiques for each variable\n",
    "        if stats == 'mean':\n",
    "            ncdata=ncdata.mean(dim='time')\n",
    "        elif stats == 'median':\n",
    "            ncdata=ncdata.median(dim='time')\n",
    "        elif stats == 'p10':\n",
    "            ncdata=ncdata.quantile(0.1, dim='time')\n",
    "        elif stats == 'p90':\n",
    "            ncdata=ncdata.quantile(0.9, dim='time')\n",
    "        \n",
    "        ncdata.rio.write_crs('EPSG:4326', inplace=True)\n",
    "        ncdata.rio.to_raster(output)\n",
    "    return output       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7b97c",
   "metadata": {},
   "source": [
    "### get_cckp_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3bae119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filename from cckp based on ssp, period and gcm\n",
    "def get_cckp_file_name(var,ssp='ssp245',period='2010-2039',gcm='median'):\n",
    "    data_folder=r'\\\\COWI.net\\projects\\A245000\\A248363\\CRVA\\Datasets'\n",
    "    if period in ['1991-2020']:\n",
    " #cru/era\n",
    "    #Precipitation   \n",
    "        if var in ['climatology-r50mm-annual-mean_era_annual','climatology-rx1day-monthly-mean_era_monthly','climatology-rx1day-annual-mean_era_annual','climatology-pr-annual-mean_era_annual','climatology-pr-monthly-mean_era_monthly']:\n",
    "            filename='precipitation/wb_cckp/climatology-rx5day-annual-mean_era_annual_era5-0.5x0.5-climatology_mean_1991-2020.nc'\n",
    "            filename=filename.replace('climatology-rx5day-annual-mean_era_annual',var)\n",
    "        elif var in ['climatology-pr-annual-mean_cru']:\n",
    "            filename='precipitation/wb_cckp/climatology-pr-annual-mean_cru_annual_cru-ts4.06-climatology_mean_1991-2020.nc'\n",
    "    #Temperature\n",
    "        elif var in ['climatology-tasmax-annual-mean_era','climatology-hd35-annual-mean_era','climatology-tas-annual-mean_era','climatology-hd40-annual-mean_era']:\n",
    "            filename='temperature/wb_cckp/climatology-tasmax-annual-mean_era_annual_era5-0.5x0.5-climatology_mean_1991-2020.nc'\n",
    "            filename=filename.replace('climatology-tasmax-annual-mean_era',var)                                                                                                                                 \n",
    "        elif var in ['climatology-tasmax-annual-mean_cru']: \n",
    "            filename='temperature/wb_cckp/climatology-tasmax-annual-mean_cru_annual_cru-ts4.06-climatology_mean_1991-2020.nc' \n",
    " #Realtime             \n",
    "    elif period not in ['1991-2020']:\n",
    "    #Precipitation     \n",
    "        if var in ['frp100yr-rx1day-period-mean_cmip6_period','climatology-rx1day-annual-mean_cmip6_annual','frp50yr-rx1day-period-mean_cmip6_period','climatology-pr-monthly-mean_cmip6_monthly','climatology-pr-annual-mean_cmip6_annual','climatology-pr-seasonal-mean_cmip6_seasonal','changefactorfaep100yr-rx1day-period-mean_cmip6_period','anomaly-pr-monthly-mean_cmip6_monthly','climatology-rx5day-annual-mean_cmip6_annual']: \n",
    "            filename='precipitation/wb_cckp/frp100yr-rx1day-period-mean_cmip6_period_all-regridded-bct-ssp245-climatology_median_2010-2039.nc'   \n",
    "            filename=filename.replace('2010-2039',period)\n",
    "            filename=filename.replace('frp100yr-rx1day-period-mean_cmip6_period',var)                      \n",
    "    #Temperature\n",
    "        elif var in ['climatology-hd40','anomaly-hd40','anomaly-hd35','anomaly-tasmax','anomaly-txx','climatology-txx','anomaly-tas','climatology-tas']: \n",
    "            filename='temperature/wb_cckp/climatology-hd40-annual-mean_cmip6_annual_all-regridded-bct-ssp245-climatology_median_2020-2039.nc'\n",
    "            filename=filename.replace('2020-2039',period)    \n",
    "            filename=filename.replace('climatology-hd40',var)\n",
    "        filename=filename.replace('ssp245',ssp)\n",
    "        filename=filename.replace('median',gcm)\n",
    "    data_path=os.path.join(data_folder,filename)\n",
    "    return data_path\n",
    "#import data from copernicus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e6b06",
   "metadata": {},
   "source": [
    "### Period for the copernicus function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba755b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Period for copernicus function ################################################\n",
    "# Aim of the function: by giving it a first and last year of the period that must analyzed, this function produce several \n",
    "# vectors,containing time informations, useful to download and treat data from CMIP6 projections (https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview )\n",
    "# Those time vectors are used in the copernicus_data and the dataframe_copernicus and csv_copernicus functions\n",
    "\n",
    "# function year_copernicus produce \n",
    "# year: a vector containing all the year in the period of interest\n",
    "# year_str: a ???? containing all the year in the period of interest in the string format\n",
    "# index: a ????? containing the index of the year and year_str\n",
    "#### Parameters of the function\n",
    "# first_year: number in int format, of the first year of the period of interest\n",
    "# last_year: number in int format, of the last year of the period of interest\n",
    "def year_copernicus(first_year,last_year):\n",
    "    year = np.arange(first_year,(last_year+1),1) # create vector of years\n",
    "    year_str = [0]*len(year) # create initiale empty vector to convert years in int\n",
    "    index = np.arange(0,len(year)) # create vector of index for year\n",
    "    i = 0 # initialize index\n",
    "    for i in index: # convert all the date in string format\n",
    "        year_str[i]=str(year[i])\n",
    "    return (year, year_str, index)\n",
    "\n",
    "# function date_copernicus produce \n",
    "# dates: the format depend on the temporal reolution, but always contain the dates of the period of interest.\n",
    "#        with temporal_resolution=daily, dates is a DatetimeIndex\n",
    "#        with temporal_resolution=monthly, dates is a list\n",
    "# index_dates: an array containing the index of the dates\n",
    "#### Parameters of the function\n",
    "# temporal_resolution: daily or monthly\n",
    "# year_str: ???? produce by function year_copernicus, containing the year of the period of interest in string format\n",
    "def date_copernicus(temporal_resolution,year_str):\n",
    "    start_date = \"01-01-\"+year_str[0] # string start date based on start year\n",
    "    stop_date = \"31-12-\"+year_str[len(year_str)-1] # string stop date based on stop year\n",
    "    if temporal_resolution =='daily':\n",
    "        # vector of dates between start date and stop date\n",
    "        dates = pd.date_range(start_date,stop_date)# dates is a pandas.core.indexes.datetimes.DatetimeIndex\n",
    "        # By default, freq = 'D', which means calendar day frequency (source : https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases)\n",
    "        #index_dates = np.arange(0,len(dates)) # vector containning index o dates vector\n",
    "    if temporal_resolution =='monthly':\n",
    "        dates = pd.date_range(start_date,stop_date,freq='MS') # vector of dates between start date and stop date\n",
    "        dates=list(dates.strftime('%m-%Y')) # dates is an pandas.core.indexes.base.Index, not a pandas.core.indexes.datetimes.DatetimeIndex\n",
    "    #if temporal_resolution =='fixed': trouver donnees pour gerer cela\n",
    "    index_dates = np.arange(0,len(dates)) # vector containning index o dates vector\n",
    "    return (dates, index_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba5ca1",
   "metadata": {},
   "source": [
    "### Copernicus function\n",
    "Some data comes from copernicus and can be directly taken form the website thans to CDS. The following functions serves this purpose\n",
    "#### Parameters of the function :\n",
    "projections-cmip6 : name of the web page, in this case, 'projections-cmip6'\n",
    "format : zip or tar.gz\n",
    "temporal_resolution : daily or monthly or fixed\n",
    "SSP : sscenario that is studied \"Historical\", \"SSP1-1.9\", \"SSP1-2.6\" ...\n",
    "Variable : variable to be studied\n",
    "model: model of projection to choose\n",
    "year: year of study to choose\n",
    "area: area of study\n",
    "month: month to be studied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8637ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### Copernicus data function ###################################################\n",
    "# Aim of the function : read nc data found on copernicus CMIP6 projections (https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview )\n",
    "# Actions of this function\n",
    "#     1) check which parameters are asked or not in the variables dictionnary, and modify the last depend on the parameters \n",
    "#        chosen by the user before\n",
    "#     2) All this step is done in function try_download_copernicus: thanks to c.retrieve function and the variables dictionnary, \n",
    "#        the chosen data are download in zip format, dezipped and registered in a specific folder. \n",
    "#     3) the function looks in the specific folder for a nc format file, and once found, return the path of this nc format file\n",
    "\n",
    "#### Parameters of the function\n",
    "# temporal_resolution : daily or monthly or fixed\n",
    "# SSP : sscenario that is studied \"Historical\", \"SSP1-1.9\", \"SSP1-2.6\" ...\n",
    "# name_variable : variable to be studied\n",
    "# model: model of projection to choose\n",
    "# year: year(s) of study to choose\n",
    "# area: area of study, if not specific, area should be an empty array area=[]\n",
    "# path_for_file: path where the file must be unzipped\n",
    "# out_path: path were all the outputs are registered, defined by the user in the begining of the main code\n",
    "# name_area : to specify if we are only looking data for a project or for a wider zone\n",
    "\n",
    "def copernicus_data(temporal_resolution,SSP,name_variable,model,year,area,path_for_file,out_path,name_area,source): \n",
    "    # create path for the downloaded file\n",
    "    start_path = os.path.join(out_path,'Data_download_zip')\n",
    "    file_download=create_file_download_path(start_path,name_variable,name_area,SSP,model,year,temporal_resolution,source) \n",
    "    if not os.path.isdir(path_for_file):\n",
    "        print('path_for_file does not exist: the data may not have been downloaded') \n",
    "        if not os.path.isdir(file_download):\n",
    "            print('file_download does not exist: the data were not downloaded')\n",
    "            # function try to download from copernicus\n",
    "            path_file = try_download_copernicus(temporal_resolution,SSP,name_variable,model,area,year,path_for_file,file_download,source)\n",
    "            if path_file is None: # for this climate variable, the parameter do not fit\n",
    "                return path_file\n",
    "            final_path=search_for_nc(path_file) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "            print('\\n')\n",
    "            print('---------------  Path to nc file exists ?? ---------------\\n')\n",
    "            print(os.path.isfile(final_path))\n",
    "            print('\\n')\n",
    "            return final_path\n",
    "            \n",
    "        else: # if the path already exist, the data in zip format should also exists\n",
    "            print('file_download does exist, the data have been downloaded, but not extracted')\n",
    "            #path_file=os.path.join(path_for_file,source)# source was added because of a problem during downloading\n",
    "            final_path=download_extract(path_for_file,file_download,source)\n",
    "            #final_path=search_for_nc(path_file) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "            #if final_path is None:# if no nc file exists, need to check again if the file with those parameters exists\n",
    "            #    final_path = try_download_copernicus(temporal_resolution,SSP,name_variable,model,area,year,path_for_file,file_download,source)\n",
    "            final_path = search_for_nc(final_path) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "            return final_path\n",
    "                \n",
    "    else: # the path for the file exists\n",
    "        if not os.listdir(path_for_file): # if the path is empty\n",
    "            final_path=download_extract(path_for_file,file_download,source)\n",
    "            final_path = search_for_nc(final_path) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "        else: # if the path is not empty\n",
    "            path_file=os.path.join(path_for_file,source)# data was added because of a problem during downloading\n",
    "            final_path=search_for_nc(path_file) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "            if final_path is None: # if no nc file exists, need to check again if the file with those parameters exists\n",
    "                test= os.path.join(file_download,source,'download.zip')\n",
    "                if not os.path.join(test):# the file was not downloaded \n",
    "                    final_path = try_download_copernicus(temporal_resolution,SSP,name_variable,model,area,year,path_for_file,file_download,source)\n",
    "                else: # the file was already downloaded but not not extracted\n",
    "                    final_path=download_extract(path_for_file,file_download,source)\n",
    "                final_path = search_for_nc(final_path) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "        return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b02c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_download_copernicus(temporal_resolution,SSP,name_variable,model,area,year,path_for_file,file_download,source):\n",
    "    c = cdsapi.Client()# function to use the c.retrieve\n",
    "    # basic needed dictionnary to give to the c.retrieve function the parameters asked by the user\n",
    "    variables = {\n",
    "                'format': 'zip', # this function is only designed to download and unzip zip files\n",
    "                'temporal_resolution': temporal_resolution,\n",
    "                'experiment': SSP,\n",
    "                'variable': name_variable,\n",
    "                'model': model,\n",
    "    }\n",
    "\n",
    "    if area != []: # the user is interested by a sub region and not the whole region \n",
    "        variables.update({'area':area}) \n",
    "\n",
    "    if name_variable == 'air_temperature':\n",
    "        variables['level'] = '1000' # [hPa], value of the standard pressure at sea level is 1013.25 [hPa], so 1000 [hPa] is the neareste value. Other pressure value are available but there is no interest for the aim of this project\n",
    "\n",
    "    if temporal_resolution != 'fixed':# if 'fixed', no year, month, date to choose\n",
    "        variables['year']=year # period chosen by the user\n",
    "        variables['month']= calendar.default_month  # be default, all the months are given; defined in class calendar\n",
    "        if temporal_resolution == 'daily':\n",
    "            variables['day']= calendar.default_day # be default, all the days are given; defined in class calendar\n",
    "    # c.retrieve download the data from the website\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'projections-cmip6',\n",
    "            variables,\n",
    "            'download.zip') # the file in a zip format is registered in the current directory\n",
    "    except:\n",
    "        print('Some parameters are not matching')\n",
    "        return # stop the function, because some data the user entered are not matching\n",
    "    \n",
    "    # function to extract the downloaded zip\n",
    "    path_file=download_extract(path_for_file,file_download,source)\n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5509d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_extract functions aims to return the path were the downloaded file in zip format is extracted\n",
    "\n",
    "def download_extract(path_for_file,file_download,source):\n",
    "    if not os.path.isdir(path_for_file): # path_for_file does not exists, need to ensure that is is created\n",
    "        os.makedirs(path_for_file) # to ensure the creation of the path\n",
    "    # unzip the downloaded file\n",
    "    if 'download.zip' not in os.listdir(): # check if download is in the working directory\n",
    "        print('The download zip is moved to the working directory')\n",
    "        path_downloaded_zip=os.path.join(file_download,'download.zip')\n",
    "        shutil.move(path_downloaded_zip,r'C:\\Users\\CLMRX\\OneDrive - COWI\\Documents\\GitHub\\CRVA_tool') # move download fil to working directory\n",
    "    \n",
    "    from zipfile import ZipFile\n",
    "    zf = ZipFile('download.zip', 'r')\n",
    "    zf.extractall(source) # if no precision of directory, extract in current directory\n",
    "    zf.close()\n",
    "\n",
    "    if not os.path.isdir(file_download): # path_for_file does not exists, need to ensure that is is created\n",
    "        os.makedirs(file_download) # to ensure the creation of the path\n",
    "    # moving download to appropriate place\n",
    "    #test = os.path.join(file_download,'download.zip')\n",
    "    #if not os.path.isfile(test):\n",
    "    shutil.move('download.zip',file_download) # no need to delete 'download.zip' from inital place\n",
    "    #test = os.path.join(path_for_file,source)\n",
    "    #if not os.path.isdir(test):\n",
    "    shutil.move(source,path_for_file) # move extracted data to the path created for them\n",
    "    path_file=os.path.join(path_for_file,source)\n",
    "    print('\\n The downloaded file is extracted')\n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73373749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seach_for_nc is a function looking in path_for_file for a document in .nc format\n",
    "\n",
    "def search_for_nc(path_for_file):\n",
    "    print('path_for_file does exist Function copernicus search for nc')\n",
    "    for file in os.listdir(path_for_file):\n",
    "        if file.endswith(\".nc\"):\n",
    "            final_path=os.path.join(path_for_file, file)\n",
    "            \n",
    "            print('The file is in the path Function copernicus search for nc\\n')\n",
    "            print('Before path_length, The final path for the nc file is: '+final_path)\n",
    "            answer = str(os.path.isfile(final_path))\n",
    "            print('\\n The final path for nc file exists ? '+answer+'\\n')\n",
    "            final_path=path_length(final_path) # check if length of path is too long\n",
    "            print('After path_length, The final path for the nc file is: '+final_path)\n",
    "            answer = str(os.path.isfile(final_path))\n",
    "            print('\\n The final path for nc file exists ? '+answer+'\\n')\n",
    "            return final_path # the function returns the path of the nc file of interest\n",
    "            break # stop the function if a nc file was found \n",
    "        else:\n",
    "            pass\n",
    "    # the all folder has been search and there is no nc file in it\n",
    "    print('Problem : No nc file was found Function copernicus Function copernicus search for nc')# this line is out of the for loop, \n",
    "    #because it should only appear once all the folder has been examinated and if the break of the if was not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e073f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions test if the path is too long\n",
    "# if the path is more than 250 char, the path wll be modified in order for windows to accept is as a path\n",
    "\n",
    "def path_length(str1):\n",
    "    if len(str1)>250:\n",
    "        path = os.path.abspath(str1) # normalize path\n",
    "        if path.startswith(u\"\\\\\\\\\"):\n",
    "            path=u\"\\\\\\\\?\\\\UNC\\\\\"+path[2:]\n",
    "        else:\n",
    "            path=u\"\\\\\\\\?\\\\\"+path\n",
    "        return path\n",
    "    else:\n",
    "        return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6a099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create path for the downloaded file\n",
    "def create_file_download_path(start_path,name_variable,name_area,SSP,model,year,temporal_resolution,source):\n",
    "    # adapt the name of the folder fot the period, depending on the type of period\n",
    "    if len(year)==1:\n",
    "        file_download = os.path.join(start_path,name_variable,name_area,SSP,model,year,source)\n",
    "    elif len(year)>1:\n",
    "        period=year[0]+'-'+year[len(year)-1]\n",
    "        file_download = os.path.join(start_path,name_variable,name_area,SSP,model,period,source)\n",
    "    elif temporal_resolution == 'fixed':\n",
    "        file_download = os.path.join(start_path,name_variable,name_area,SSP,model,'fixed_period',source)\n",
    "    return file_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9475c",
   "metadata": {},
   "source": [
    "### Registering data in dataframe and csv form copernicus CMIP6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40803b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Register data from nc file of Copernicus ############################################\n",
    "# Aim of the function: this function aims to register in a dataframe and a csv file the data from the nc file downloaded with\n",
    "# the function copernicus_data\n",
    "# Actions of this function\n",
    "#     1) Create the string indicating the period of interest\n",
    "#     2) Creating path and file name to register dataframe in csv file\n",
    "#     3) Register data, with its corresponding experiments and models, in dataframe and csv file\n",
    "#        3 a) Test if path does not exists (if dataframe is not registered) : \n",
    "#                1 . Thanks to copernicus_data, download nc fils from copernicus CMIP6 website for each experiment and each model\n",
    "#                2 . Open the dowloaded nc file in the jupyter notebook if it exists\n",
    "#                3 . In a dataframe, register the value in the nc file, for each experiment, model and day\n",
    "#                4 . If there no value for each experiments and models tested, the datfram is empty and the user is informed\n",
    "#        3 b) Test if path exists (dataframe is registered) : no need to register again, return in dataframe the existing \n",
    "#             csv file in a dataframe\n",
    "\n",
    "# Parameters of the function\n",
    "# temporal_resolution: 'daily', 'monthly', or 'fixed'. String type \n",
    "# year_str: list containing all the years under the string type and in the period of interest\n",
    "# experiments: copernicus_elements.experiments\n",
    "# models: copernicus_elements.models\n",
    "# out_path: path were the outputs are registered. Defined by the user at the beginning of the code \n",
    "# global_variable: global name of the climate variable of interest (example: Wind)\n",
    "# name_variable: name of the elements downloaded from copernicus (example: 'near_surface_wind_speed')\n",
    "# name_project: Name of the project for which the data are taken\n",
    "# area: list containing latitudes and logitudes around the project\n",
    "\n",
    "def csv_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,area,source):    \n",
    "    ### PROBLEM WITH DATES, CAN T just pass one year. year str is a list, so if one year (2020,)\n",
    "    ## PROBLEM WITH PATH: not coherent between data csv, datasets, download. And not achieving to have project name in path for dataset\n",
    "    ## maybe the name for dataset is too long, but even if end at name project, does not work. Try doing one string with name project in it\n",
    "    ## PROBLEM WITH PATH: WORK BUT NOT IDEAL\n",
    "    ## pourquoi mettre toutes les donnees dans un dataframe ?? permet d'avoir cette organisation en multiindex. Sinon, on ne peut pas faire ca\n",
    "    print('############################### Project name: '+name_project+' ###############################')\n",
    "    \n",
    "    # create string for name of folder depending on type of period\n",
    "    if temporal_resolution == 'fixed':\n",
    "        period = 'fixed'\n",
    "    else:\n",
    "        period=year_str[0]+'-'+year_str[len(year_str)-1]\n",
    "    \n",
    "    # modification on name_project str to ensure no problem whent using this str as name of a folder\n",
    "    name_project = name_project.replace('-','_') # take off every blank space of project names\n",
    "    name_project = name_project.replace('/','_') # take off every / of project names\n",
    "    name_project = name_project.replace(r'\"\\\"','_') # take off every \\ of project names\n",
    "    # brackets shouldn't be a problem for name projects\n",
    "        \n",
    "    (dates, index_dates)=date_copernicus(temporal_resolution,year_str) # create time vector depending on temporal resolution\n",
    "\n",
    "    title_file = name_project +'_' +period+ '_' + temporal_resolution + '_' +name_variable#+'.csv'\n",
    "    \n",
    "    path_for_csv = os.path.join(out_path,'csv',source,name_variable,name_project,period) # create path for csv file\n",
    "\n",
    "    if not os.path.isdir(path_for_csv): # test if the data were already downloaded; if not, first part if the if is applied\n",
    "        os.makedirs(path_for_csv) # to ensure the creation of the path\n",
    "        # the dataframe_copernicus functions aims to test if the data with the specific parameters exists (with copernicus_data)\n",
    "        # and then produce a csv file if the data exists\n",
    "        (df,period)=dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,area,period,index_dates,dates,path_for_csv,title_file,source)\n",
    "        return df,period\n",
    "    else:# test if the data were already downloaded; if yes, this part of the if is applied\n",
    "        if len(os.listdir(path_for_csv)) == 0: #test if the directory is empty\n",
    "            # the csv file does not exist, even if the path exist\n",
    "            # the dataframe_copernicus functions aims to test if the data with the specific parameters exists (with copernicus_data)\n",
    "            # and then produce a csv file if the data exists\n",
    "            (df,period)=dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,area,period,index_dates,dates,path_for_csv,title_file,source)\n",
    "        else: # the directory is not empty\n",
    "            df=file_already_downloaded(path_for_csv,title_file)\n",
    "\n",
    "        return df,period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81988850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe_copernicus functions aims to test if the data with the specific parameters exists (with copernicus_data)\n",
    "# and then produce a csv file if the data exists\n",
    "\n",
    "def dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,area,period,index_dates,dates,path_for_csv,title_file,source):    \n",
    "    print('FUNCTION DATAFRAME_COPERNICUS')\n",
    "    df = pd.DataFrame() # create an empty dataframe\n",
    "    for SSP in experiments:\n",
    "        experiment = (SSP,) # create tuple for iteration of dataframe\n",
    "        print('Test with scenario '+SSP)\n",
    "        for model_simulation in models:\n",
    "            model =(model_simulation,) # create tuple for iteration of dataframe\n",
    "            print('Test with model '+model_simulation)\n",
    "            # path were the futur downloaded file is registered\n",
    "            path_for_file= os.path.join(out_path,'Datasets',name_variable,name_project,SSP,model_simulation,period)\n",
    "            # existence of path_for_file tested in copernicus function\n",
    "            climate_variable_path=copernicus_data(temporal_resolution,SSP,name_variable,model_simulation,year_str,area,path_for_file,out_path,name_project,source)\n",
    "            # area is determined in the \"Load shapefiles and plot\" part\n",
    "            if (climate_variable_path is not None):\n",
    "                # register data concerning each project under the form of a csv, with the model, scenario, period, latitude and longitude\n",
    "                df=register_data(climate_variable_path,name_project,index_dates,dates,experiment,model,df)\n",
    "                print('\\nValue were found for the period and the project tested\\n')\n",
    "            else:\n",
    "                print('\\nNo value were found for the period and the project tested\\n')\n",
    "                continue # do the next for loop\n",
    "        # test if dataframe is empty, if values exist for this period\n",
    "    if not df.empty: # if dataframe is not empty, value were registered, the first part is run : a path to register the csv file is created, and the dataframe is registered in a csv file\n",
    "        full_name = os.path.join(path_for_csv,title_file)\n",
    "        print(full_name)\n",
    "        df.to_csv(full_name) # register dataframe in csv file\n",
    "        return df,period \n",
    "    else: # if the dataframe is empty, no value were found, there is no value to register or to return\n",
    "        #os.remove(path_for_file)# remove path\n",
    "        return df,period# there is no dataframe to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94f86710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register data concerning each project under the form of a csv, with the model, scenario, period, latitude and longitude\n",
    "def register_data(climate_variable_path,name_project,index_dates,dates,experiment,model,df):\n",
    "    print('Registering the data in a dataframe')\n",
    "    Open_path = Dataset(climate_variable_path) # open netcdf file\n",
    "    lat_dataframe = np.ma.getdata(Open_path.variables['lat']).data\n",
    "    lon_dataframe = np.ma.getdata(Open_path.variables['lon']).data\n",
    "    column_name = find_column_name(Open_path)\n",
    "    data_with_all = ma.getdata(Open_path.variables[column_name]).data\n",
    "\n",
    "    for moment in index_dates: # case if temporal resolution is daily\n",
    "        data_dataframe = data_with_all[moment,:,:]\n",
    "        Date = (dates[moment],) # create tuple for iteration of dataframe\n",
    "        Name_Project = (name_project,)\n",
    "\n",
    "        # Create the MultiIndex\n",
    "        midx = pd.MultiIndex.from_product([Name_Project,experiment, model, Date, lat_dataframe],names=['Name project','Experiment', 'Model', 'Date', 'Latitude'])\n",
    "        # multiindex to name the columns\n",
    "        lon_str = ('Longitude',)\n",
    "        cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "        # Create the Dataframe\n",
    "        Variable_dataframe = pd.DataFrame(data = data_dataframe, \n",
    "                                    index = midx,\n",
    "                                    columns = cols)\n",
    "        # Concatenate former and new dataframe\n",
    "        df = pd.concat([df,Variable_dataframe])# register information for project\n",
    "\n",
    "    Open_path.close # to spare memory\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8978da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return column name in the netCDF file\n",
    "# all netCDF file form copernicus have this format for their variables names\n",
    "# ['time', 'time_bnds', 'lat', 'lat_bnds', 'lon', 'lon_bnds', Name of climate variable of interest]\n",
    "# take of 'time', 'time_bnds', 'lat', 'lat_bnds', 'lon', 'lon_bnds'\n",
    "def find_column_name(Open_path):\n",
    "    # make a list with every variables of the netCDF file of interest\n",
    "    climate_variable_variables=list(Open_path.variables)\n",
    "    # variables that are not the column name of interest \n",
    "    elements_not_climate_var =['time', 'time_bnds', 'bnds','lat', 'lat_bnds', 'lon', 'lon_bnds','time_bounds','bounds','lat_bounds','lon_bounds']\n",
    "    for str in elements_not_climate_var:\n",
    "        if str in climate_variable_variables:\n",
    "            climate_variable_variables.remove(str)\n",
    "    return climate_variable_variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52efa5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_already_downloaded(path_for_csv,title_file):\n",
    "    print('The file was already downloaded')\n",
    "    df = pd.read_csv(os.path.join(path_for_csv,title_file)) # read the downloaded data for the analysis\n",
    "\n",
    "    # changing name of columns\n",
    "    name_columns=df.iloc[0].array\n",
    "    df.rename(columns={'Unnamed: 0':'Experiment','Unnamed: 1':'Model','Unnamed: 2':'Date','Unnamed: 3':'Latitude'}, inplace=True)\n",
    "\n",
    "    lon_dataframe=name_columns[4:len(name_columns)] # register data for columns of multiindex\n",
    "\n",
    "    df.drop([0,1], axis=0,inplace=True) # remove 2 first lines\n",
    "\n",
    "    # recreate multiindex \n",
    "\n",
    "    # .... with columns\n",
    "    df.set_index(['Name project','Experiment', 'Model', 'Date','Latitude'],inplace=True)\n",
    "\n",
    "    # .... with lines\n",
    "    lon_str = ('Longitude',)\n",
    "    cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "    df.columns=cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c8a29",
   "metadata": {},
   "source": [
    "### Display map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b244943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display a map\n",
    "def Display_map(indexes_lat,indexes_lon,lat,lon,lat_min_wanted,lat_max_wanted,lon_min_wanted,lon_max_wanted,data,title_png,title_to_adapt,label,parallels,meridians):#,projects):\n",
    "\n",
    "    lon_moz, lat_moz = np.meshgrid(lon, lat) # this is necessary to have a map\n",
    "    \n",
    "    # create Map\n",
    "    fig = plt.figure()\n",
    "    plt.title(title_to_adapt) # title of the map # automatized with year\n",
    "    map = Basemap(projection ='merc',llcrnrlon=lon_min_wanted+5,llcrnrlat=lat_min_wanted+2,urcrnrlon=lon_max_wanted-5,urcrnrlat=lat_max_wanted-2,resolution='i', epsg = 4326) # projection, lat/lon extents an\n",
    "    # adding and substracting a quantity to the lon and lat to have a bit of margin when presenting it\n",
    "    # substracting more to longitude because the range of longitude is -180 to 180. The range of latitude is -90 to 90\n",
    "    map.drawcountries()\n",
    "    map.drawcoastlines()\n",
    "    map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "    map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "\n",
    "    temp = map.contourf(lon_moz,lat_moz,data)\n",
    "    #projects.plot(ax=ax) # project in projection EPSG:4326\n",
    "    cb = map.colorbar(temp,\"right\", size=\"5%\", pad=\"2%\") # color scale, second parameter can be locationNone or {'left', 'right', 'top', 'bottom'}\n",
    "    cb.set_label(label) # name for color scale\n",
    "    plt.savefig(os.path.join(out_path,'figures',title_png),format ='png') # savefig or save text must be before plt.show. for savefig, format should be explicity written\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48b05b",
   "metadata": {},
   "source": [
    "### Display map project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e98247f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## Display project on map ############################################\n",
    "# This function aims to display every projects wanted by the user with a map as a background\n",
    "### Parameters of the function : \n",
    "# projects: geopanda.dataframe containing projects information\n",
    "# study_area: geopanda.dataframe containing study_area information\n",
    "# str_interest: string of the climate variable of interest to be represented\n",
    "# title_for_image: title for image composed of subplots\n",
    "# number_rows: the user should indicate the number of rows for the subplots\n",
    "# number_cols: the user should indicate the number of cols for the subplots\n",
    "# out_path: begenning of the path to register the image\n",
    "\n",
    "def Display_map_projects(projects,study_area,str_interest,title_for_image,number_rows, number_cols,out_path):\n",
    "    \n",
    "    # select climate variable to be represented\n",
    "    columns_to_represent= list(projects.filter(regex=str_interest).columns) # select columns that should be presented in plots\n",
    "    columns_to_represent2=[ ('\\n').join(k.split(' ')[-2:])  for k in columns_to_represent] # delete 'water stress' from \n",
    "    # the list column_to_represent and insert \\n to do a back to line\n",
    "    number_plots = len(columns_to_represent)\n",
    "    \n",
    "    # create figure\n",
    "    fig, axs = plt.subplots(nrows=number_rows,ncols=number_cols, sharex=True, sharey=True,figsize=(8,8))\n",
    "    \n",
    "    k=0\n",
    "    # map in the subplot\n",
    "    for i in np.arange(0,3):# columns\n",
    "        for j in np.arange(0,2): # linesfor i in np.arange(0,number_plots):\n",
    "            base = study_area.plot(ax=axs[j][i],color='white', edgecolor='black')# background is map of the study area presenting \n",
    "            # country borders of this area\n",
    "            projects.plot(ax=axs[j][i], column=columns_to_represent[k])# plot the projects as points; legeng = True \n",
    "            # impose a color for the projects point dpeending on the value in the column\n",
    "\n",
    "            # give subplot a title\n",
    "            ax_created = axs[j][i]\n",
    "            ax_created.title.set_text(columns_to_represent2[k])\n",
    "            \n",
    "            k+=1 # incrementation to iterate columns_to_represent\n",
    "\n",
    "    plt.legend()\n",
    "    plt.suptitle(title_for_image) # give a global name to the image\n",
    "    plt.savefig(os.path.join(out_path,'figures',str_interest,title_for_image),format ='png') # savefig or save text must be before plt.show. for savefig, format should be explicity written\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744baf8",
   "metadata": {},
   "source": [
    "## Return period function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit ## given some x_data and some y_data and a model function f thaht depends on unknown parameters bveta,\n",
    "# the goal of curve fitting is to find the optimal set of parameters beta such that the function y = f(x,beta) best resembles the data\n",
    "\n",
    "# two ways to obtain beta parameter\n",
    " #### method of least squares : minimize sum of the square of the difference between the model function and y by adjusting beta\n",
    " #### maximum-likelihood : when y has errors, minimize the sum of the ratio between the suqre of the difference of the model function and y\n",
    "    # and the square of the variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROBELM : when 2 values are the same, what happens ????\n",
    "# curve fitting, how to find equation representing, possible to install scipy : https://www.geeksforgeeks.org/scipy-curve-fitting/\n",
    "\n",
    "def return_period(data_series):\n",
    "    # rank data\n",
    "    data_series.sort(reverse=True)\n",
    "    N = len(data_series)\n",
    "    rank=np.arange(N,0,-1,dtype=int)\n",
    "    # look for duplicates in the list\n",
    "    for value in data_series:\n",
    "        duplicate = list_duplicates_of(randomlist, value)\n",
    "        #if not duplicate.empty:\n",
    "            \n",
    "        #else:\n",
    "        #    continue\n",
    "    # give return period of each value in the time period given\n",
    "    P = rank / (N+1) # rank / (N+1)\n",
    "    T=1/P\n",
    "    plt.scatter(T,data_series)\n",
    "    plt.xlabel('Return period')\n",
    "    plt.ylabel('Climate variable of interest')\n",
    "    \n",
    "    return data_series,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01511d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_duplicates_of(seq,item):\n",
    "    start_at = -1\n",
    "    locs = []\n",
    "    while True:\n",
    "        try:\n",
    "            loc = seq.index(item,start_at+1)\n",
    "        except ValueError:\n",
    "            break\n",
    "        else:\n",
    "            locs.append(loc)\n",
    "            start_at = loc\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa38cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test return_period\n",
    "\n",
    "import random\n",
    "\n",
    "randomlist = random.sample(range(100, 500), 365)\n",
    "randomlist[0]=randomlist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank=np.arange(len(randomlist),0,-1,dtype=int)\n",
    "#rank=rank.tolist()\n",
    "#rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate=list_duplicates_of(randomlist, randomlist[0])\n",
    "#type(duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(rank[duplicate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##mean_rank=sum(rank[duplicate].tolist())/len(rank[duplicate].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94174fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b761138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank[duplicate]=mean_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd13c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ranked_data_series,T)=return_period(randomlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56389711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_f1(x, a, b, c):\n",
    "    return a*(x-b)**2 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def model_f(x, a, b, c):\n",
    "    return a*np.exp(x)**b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_f2(x, a, b, c):\n",
    "    return a*np.log(x)**b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_f3(x, a, b, c):\n",
    "    return a*np.log10(x)**b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_f4(x, a, b, c):\n",
    "    return (a)**(b*x)+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(model_f, T, ranked_data_series, p0=[50,-10,-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65804b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_opt, b_opt, c_opt = popt\n",
    "x_model = np.linspace(min(T), max(T), 100)\n",
    "y_model = model_f(x_model, a_opt, b_opt, c_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(T,ranked_data_series)\n",
    "plt.plot(x_model,y_model, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9bb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(np.abs(pcov)))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scipy \n",
    "import numpy as np\n",
    " \n",
    "# curve-fit() function imported from scipy\n",
    "from scipy.optimize import curve_fit\n",
    " \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f14a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def probability_of_exceedance():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb3e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
