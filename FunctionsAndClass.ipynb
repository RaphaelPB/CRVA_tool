{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d17a288",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb0ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import python packages\n",
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import rioxarray #used when calling ncdata.rio.write_crs\n",
    "import xarray as xr\n",
    "import os\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc#not directly used but needs to be imported for some nc4 files manipulations, use for nc files\n",
    "from netCDF4 import Dataset\n",
    "import csv #REMOVE ? not in use ?\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import shutil # to move folders\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore the warnings\n",
    "import cdsapi # for copernicus function\n",
    "import datetime # to have actual date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f7b9c",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffde9fc",
   "metadata": {},
   "source": [
    "### Time class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d80deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to define parameter of time that remain constant durinf the whole script\n",
    "class time:\n",
    "    default_month = [ \n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                ]\n",
    "    default_day = [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "                ]\n",
    "    actual_date = datetime.date.today()\n",
    "    actual_year = actual_date.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0c9aa",
   "metadata": {},
   "source": [
    "### Map class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f3b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class map_elements:\n",
    "    parallels = np.arange(-360,360,10) # make latitude lines ever 10 degrees\n",
    "    meridians = np.arange(-360,360,10) # make longitude lines every 10 degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6602a",
   "metadata": {},
   "source": [
    "### Copernicus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3080d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of tuples that will be useful to search which data are available or not\n",
    "# make it tuples to make unchangeable\n",
    "class copernicus_elements:\n",
    "    models =('access_cm2','awi_cm_1_1_mr','bcc_csm2_mr','cams_csm1_0')#,'canesm5_canoe','cesm2_fv2','cesm2_waccm_fv2','cmcc_cm2_hr4','cmcc_esm2','cnrm_cm6_1_hr','e3sm_1_0','e3sm_1_1_eca','ec_earth3_aerchem','ec_earth3_veg','fgoals_f3_l','fio_esm_2_0','giss_e2_1_g','hadgem3_gc31_ll','iitm_esm','inm_cm5_0','ipsl_cm6a_lr','kiost_esm','miroc6','miroc_es2l','mpi_esm1_2_hr','mri_esm2_0','norcpm1','noresm2_mm','taiesm1','access_esm1_5','awi_esm_1_1_lr','bcc_esm1','canesm5','cesm2','cesm2_waccm','ciesm','cmcc_cm2_sr5','cnrm_cm6_1','cnrm_esm2_1','e3sm_1_1','ec_earth3','ec_earth3_cc','ec_earth3_veg_lr','fgoals_g3','gfdl_esm4','giss_e2_1_h','hadgem3_gc31_mm','inm_cm4_8','ipsl_cm5a2_inca','kace_1_0_g','mcm_ua_1_0','miroc_es2h','mpi_esm_1_2_ham','mpi_esm1_2_lr','nesm3','noresm2_lm','sam0_unicon','ukesm1_0_ll')\n",
    "    experiments = ('ssp1_1_9','ssp1_2_6','ssp4_3_4')#,'ssp5_3_4os','ssp2_4_5','ssp4_6_0','ssp3_7_0','ssp5_8_5')\n",
    "    experiments_historical=('historical',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8168454",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a55390",
   "metadata": {},
   "source": [
    "### read_cckp_ncdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3032e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def read cckp (world bank) nc files\n",
    "#reads data from world bank climate knowledge portal, nc files, with a single band\n",
    "#assigns projection and exports to tif since zonal_stats seems to have issues with it otherwise (not ideal solution)\n",
    "def read_cckp_ncdata(nc_path,output='tempfile.tif'):\n",
    "    with rioxarray.open_rasterio(nc_path,decode_times=False)[0] as ncdata:\n",
    "        ncdata.rio.write_crs('EPSG:4326', inplace=True)\n",
    "        ncdata=ncdata.isel(time=0)\n",
    "        ncdata.rio.to_raster(output)\n",
    "       # output=output #here\n",
    "   # else: \n",
    "      #  print(nc_path,\"not found\") # in this case, the data printed in the table will apply to the previous print.. \n",
    "       # output=0 #here\n",
    "    return output       \n",
    "\n",
    "#def read nc files (copernicus)\n",
    "#reads data from CMIP6 Copernicus, nc files\n",
    "#assigns projection and exports to tif since zonal_stats seems to have issues with it otherwise (not ideal solution)\n",
    "def read_nc_data(nc_path,stats,output='tempfile.tif'):\n",
    "    with rioxarray.open_rasterio(nc_path,decode_times=False)[3] as ncdata:\n",
    "        # calculate statistiques for each variable\n",
    "        if stats == 'mean':\n",
    "            ncdata=ncdata.mean(dim='time')\n",
    "        elif stats == 'median':\n",
    "            ncdata=ncdata.median(dim='time')\n",
    "        elif stats == 'p10':\n",
    "            ncdata=ncdata.quantile(0.1, dim='time')\n",
    "        elif stats == 'p90':\n",
    "            ncdata=ncdata.quantile(0.9, dim='time')\n",
    "        \n",
    "        ncdata.rio.write_crs('EPSG:4326', inplace=True)\n",
    "        ncdata.rio.to_raster(output)\n",
    "    return output       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7b97c",
   "metadata": {},
   "source": [
    "### get_cckp_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3bae119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filename from cckp based on ssp, period and gcm\n",
    "def get_cckp_file_name(var,ssp='ssp245',period='2010-2039',gcm='median'):\n",
    "    data_folder=r'\\\\COWI.net\\projects\\A245000\\A248363\\CRVA\\Datasets'\n",
    "    if period in ['1991-2020']:\n",
    " #cru/era\n",
    "    #Precipitation   \n",
    "        if var in ['climatology-r50mm-annual-mean_era_annual','climatology-rx1day-monthly-mean_era_monthly','climatology-rx1day-annual-mean_era_annual','climatology-pr-annual-mean_era_annual','climatology-pr-monthly-mean_era_monthly']:\n",
    "            filename='precipitation/wb_cckp/climatology-rx5day-annual-mean_era_annual_era5-0.5x0.5-climatology_mean_1991-2020.nc'\n",
    "            filename=filename.replace('climatology-rx5day-annual-mean_era_annual',var)\n",
    "        elif var in ['climatology-pr-annual-mean_cru']:\n",
    "            filename='precipitation/wb_cckp/climatology-pr-annual-mean_cru_annual_cru-ts4.06-climatology_mean_1991-2020.nc'\n",
    "    #Temperature\n",
    "        elif var in ['climatology-tasmax-annual-mean_era','climatology-hd35-annual-mean_era','climatology-tas-annual-mean_era','climatology-hd40-annual-mean_era']:\n",
    "            filename='temperature/wb_cckp/climatology-tasmax-annual-mean_era_annual_era5-0.5x0.5-climatology_mean_1991-2020.nc'\n",
    "            filename=filename.replace('climatology-tasmax-annual-mean_era',var)                                                                                                                                 \n",
    "        elif var in ['climatology-tasmax-annual-mean_cru']: \n",
    "            filename='temperature/wb_cckp/climatology-tasmax-annual-mean_cru_annual_cru-ts4.06-climatology_mean_1991-2020.nc' \n",
    " #Realtime             \n",
    "    elif period not in ['1991-2020']:\n",
    "    #Precipitation     \n",
    "        if var in ['frp100yr-rx1day-period-mean_cmip6_period','climatology-rx1day-annual-mean_cmip6_annual','frp50yr-rx1day-period-mean_cmip6_period','climatology-pr-monthly-mean_cmip6_monthly','climatology-pr-annual-mean_cmip6_annual','climatology-pr-seasonal-mean_cmip6_seasonal','changefactorfaep100yr-rx1day-period-mean_cmip6_period','anomaly-pr-monthly-mean_cmip6_monthly','climatology-rx5day-annual-mean_cmip6_annual']: \n",
    "            filename='precipitation/wb_cckp/frp100yr-rx1day-period-mean_cmip6_period_all-regridded-bct-ssp245-climatology_median_2010-2039.nc'   \n",
    "            filename=filename.replace('2010-2039',period)\n",
    "            filename=filename.replace('frp100yr-rx1day-period-mean_cmip6_period',var)                      \n",
    "    #Temperature\n",
    "        elif var in ['climatology-hd40','anomaly-hd40','anomaly-hd35','anomaly-tasmax','anomaly-txx','climatology-txx','anomaly-tas','climatology-tas']: \n",
    "            filename='temperature/wb_cckp/climatology-hd40-annual-mean_cmip6_annual_all-regridded-bct-ssp245-climatology_median_2020-2039.nc'\n",
    "            filename=filename.replace('2020-2039',period)    \n",
    "            filename=filename.replace('climatology-hd40',var)\n",
    "        filename=filename.replace('ssp245',ssp)\n",
    "        filename=filename.replace('median',gcm)\n",
    "    data_path=os.path.join(data_folder,filename)\n",
    "    return data_path\n",
    "#import data from copernicus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e6b06",
   "metadata": {},
   "source": [
    "### Period for the copernicus function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba755b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Period for copernicus function ################################################\n",
    "# Aim of the function: by giving it a first and last year of the period that must analyzed, this function produce several \n",
    "# vectors,containing time informations, useful to download and treat data from CMIP6 projections (https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview )\n",
    "# Those time vectors are used in the copernicus_data and the dataframe_csv_copernicus functions\n",
    "\n",
    "def year_copernicus(first_year,last_year):\n",
    "    year = np.arange(first_year,(last_year+1),1) # create vector of years\n",
    "    year_str = [0]*len(year) # create initiale empty vector to convert years in int\n",
    "    index = np.arange(0,len(year)) # create vector of index for year\n",
    "    i = 0 # initialize index\n",
    "    for i in index: # convert all the date in string format\n",
    "        year_str[i]=str(year[i])\n",
    "    return (year, year_str, index)\n",
    "\n",
    "def date_copernicus(temporal_resolution,year_str):\n",
    "    if temporal_resolution =='daily':\n",
    "        start_date = \"01-01-\"+year_str[0] # string start date based on start year\n",
    "        stop_date = \"31-12-\"+year_str[len(year_str)-1] # string stop date based on stop year\n",
    "        dates = pd.date_range(start_date,stop_date) # vector of dates between start date and stop date\n",
    "        index_dates = np.arange(0,len(dates)) # vector containning index o dates vector\n",
    "    if temporal_resolution =='monthly':\n",
    "        date = np.arange(0,len(time.default_month))\n",
    "        k=0\n",
    "        for j in year_str:\n",
    "            for i in time.default_month:\n",
    "                dates[k] = i + '-' + j # vector of dates between start date and stop date\n",
    "        index_dates = np.arange(0,len(dates)) # vector containning index o dates vector\n",
    "    #if temporal_resolution =='fixed': trouver donnees pour gerer cela\n",
    "    return (dates, index_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba5ca1",
   "metadata": {},
   "source": [
    "### Copernicus function\n",
    "Some data comes from copernicus and can be directly taken form the website thans to CDS. The following functions serves this purpose\n",
    "#### Parameters of the function :\n",
    "projections-cmip6 : name of the web page, in this case, 'projections-cmip6'\n",
    "format : zip or tar.gz\n",
    "temporal_resolution : daily or monthly or fixed\n",
    "SSP : sscenario that is studied \"Historical\", \"SSP1-1.9\", \"SSP1-2.6\" ...\n",
    "Variable : variable to be studied\n",
    "model: model of projection to choose\n",
    "year: year of study to choose\n",
    "area: area of study\n",
    "month: month to be studied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8637ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### Copernicus data function ###################################################\n",
    "# Aim of the function : read nc data found on copernicus CMIP6 projections (https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview )\n",
    "# Actions of this function\n",
    "#     1) check which parameters are asked or not in the variables dictionnary, and modify the last depend on the parameters chosen byt the user before\n",
    "#     2) thanks to c.retrieve function and the variables dictionnary, the chosen data are download in zip format\n",
    "#     3) The downloaded file (always in zip format) is dezipped and registered in a specific folder\n",
    "#     4) the function looks in the specific folder for a nc format file, and once found, return the path of this nc format file\n",
    "\n",
    "# Parameters of the function\n",
    "# temporal_resolution : daily or monthly or fixed\n",
    "# SSP : sscenario that is studied \"Historical\", \"SSP1-1.9\", \"SSP1-2.6\" ...\n",
    "# name_variable : variable to be studied\n",
    "# model: model of projection to choose\n",
    "# year: year(s) of study to choose\n",
    "# area: area of study, if not specific, area should be an empty array area=[]\n",
    "# path_for_file: path where the file must be unzipped\n",
    "\n",
    "def copernicus_data(temporal_resolution,SSP,name_variable,model,year,area,path_for_file,out_path): \n",
    "    \n",
    "    # creat a path to register data\n",
    "    if not os.path.isdir(path_for_file):\n",
    "        \n",
    "        start_path = os.path.join(out_path,'Data_download_zip')\n",
    "\n",
    "        if len(year)==1:\n",
    "            file_download = os.path.join(start_path,name_variable,SSP,model,year)\n",
    "        elif len(year)>1:\n",
    "            period=year[0]+'-'+year[len(year)-1]\n",
    "            file_download = os.path.join(start_path,name_variable,SSP,model,period)\n",
    "        elif temporal_resolution == 'fixed':\n",
    "            file_download = os.path.join(start_path,name_variable,SSP,model,'fixed_period')\n",
    "\n",
    "        if not os.path.isdir(file_download):\n",
    "            \n",
    "            c = cdsapi.Client()# function to use the c.retrieve\n",
    "            # basic needed dictionnary to give to the c.retrieve function the parameters asked by the user\n",
    "            variables = {\n",
    "                        'format': 'zip', # this function is only designed to download and unzip zip files\n",
    "                        'temporal_resolution': temporal_resolution,\n",
    "                        'experiment': SSP,\n",
    "                        'variable': name_variable,\n",
    "                        'model': model,\n",
    "            }\n",
    "\n",
    "            if area != []: # the user is interested by a sub region and not the whole region \n",
    "                variables.update({'area':area}) \n",
    "\n",
    "            if name_variable == 'air_temperature':\n",
    "                variables['level'] = '1000' # [hPa], value of the standard pressure at sea level is 1013.25 [hPa], so 1000 [hPa] is the neareste value. Other pressure value are available but there is no interest for the aim of this project\n",
    "\n",
    "            if temporal_resolution != 'fixed':# if 'fixed', no year, month, date to choose\n",
    "                variables['year']=year # period chosen by the user\n",
    "                variables['month']= time.default_month  # be default, all the months are given; defined in class time\n",
    "                if temporal_resolution == 'daily':\n",
    "                    variables['day']= time.default_day # be default, all the days are given; defined in class time\n",
    "            # c.retrieve download the data from the website\n",
    "            try:\n",
    "                c.retrieve(\n",
    "                    'projections-cmip6',\n",
    "                    variables,\n",
    "                    'download.zip') # the file in a zip format is registered in the current directory\n",
    "            except:\n",
    "                print('Some parameters are not matching')\n",
    "                return # stop the function, because some data the user entered are not matching\n",
    "            \n",
    "            os.makedirs (path_for_file) # to ensure the creation of the path\n",
    "            # unzip the downloaded file\n",
    "            from zipfile import ZipFile\n",
    "            zf = ZipFile('download.zip', 'r')\n",
    "            zf.extractall(path_for_file)\n",
    "            zf.close()\n",
    "            \n",
    "            os.makedirs (file_download) # to ensure the creation of the path\n",
    "            # moving download to appropriate place\n",
    "            #file_download = os.path.join(file_download,'download.zip')\n",
    "            shutil.move('download.zip',file_download) # no need to delete 'download.zip' from inital place\n",
    "\n",
    "        else: # if the path already exist, the data should also exists\n",
    "            pass\n",
    "\n",
    "        # look for nc file types in path_for_file. There should only be one nc files for every downloading\n",
    "        for file in os.listdir(path_for_file):\n",
    "            if file.endswith(\".nc\"):\n",
    "                final_path=os.path.join(path_for_file, file)\n",
    "                print('The path exists')\n",
    "                return final_path # the function returns the path of the nc file of interest\n",
    "                break # stop the function if a nc file was found \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        print('Problem : No nc file was found')\n",
    "        \n",
    "    else:\n",
    "        for file in os.listdir(path_for_file):\n",
    "            if file.endswith(\".nc\"):\n",
    "                final_path=os.path.join(path_for_file, file)\n",
    "                print('The path exists')\n",
    "                return final_path # the function returns the path of the nc file of interest\n",
    "                break # stop the function if a nc file was found \n",
    "            else:\n",
    "                pass\n",
    "    print('Problem : No nc file was found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9475c",
   "metadata": {},
   "source": [
    "### Registering data in dataframe and csv form copernicus CMIP6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40803b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Register data from nc file of Copernicus ############################################\n",
    "# Aim of the function: this function aims to register in a dataframe and a csv file the data from the nc file downloaded with\n",
    "# the function copernicus_data\n",
    "# Actions of this function\n",
    "#     1) Create the string indicating the period of interest\n",
    "#     2) Creating path and file name to register dataframe in csv file\n",
    "#     3) Register data, with its corresponding experiments and models, in dataframe and csv file\n",
    "#        3 a) Test if path does not exists (if dataframe is not registered) : \n",
    "#                1 . Thanks to copernicus_data, download nc fils from copernicus CMIP6 website for each experiment and each model\n",
    "#                2 . Open the dowloaded nc file in the jupyter notebook if it exists\n",
    "#                3 . In a dataframe, register the value in the nc file, for each experiment, model and day\n",
    "#                4 . If there no value for each experiments and models tested, the datfram is empty and the user is informed\n",
    "#        3 b) Test if path exists (dataframe is registered) : no need to register again, return in dataframe the existing \n",
    "#             csv file in a dataframe\n",
    "\n",
    "# Parameters of the function\n",
    "\n",
    "def dataframe_csv_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, column_name,area):    \n",
    "    ### PROBLEM WITH DATES, CAN T just pass one year\n",
    "    \n",
    "    \n",
    "    # create string for name of folder depending on type of period\n",
    "    if temporal_resolution == 'fixed':\n",
    "        period = 'fixed'\n",
    "    else:\n",
    "        period=year_str[0]+'-'+year_str[len(year_str)-1]\n",
    "        \n",
    "    (dates, index_dates)=date_copernicus(temporal_resolution,year_str) # create time vector depending on temporal resolution\n",
    "\n",
    "    title_file = period + '_' + temporal_resolution + '_' +name_variable\n",
    "    \n",
    "    path_for_csv = os.path.join('outputs','csv','data',period,name_variable) # create path for csv file\n",
    "    \n",
    "    if not os.path.isdir(path_for_csv): # test if the data were already downloaded; if not, first part if the if is applied\n",
    "        df = pd.DataFrame() # create an empty dataframe\n",
    "        for SSP in experiments:\n",
    "            experiment = (SSP,) # create tuple for iteration of dataframe\n",
    "            print(SSP)\n",
    "            for model_simulation in models:\n",
    "                model =(model_simulation,) # create tuple for iteration of dataframe\n",
    "                print(model)\n",
    "                # path were the futur downloaded file is registered\n",
    "                path_for_file= os.path.join(out_path,'Datasets', global_variable, name_variable, SSP, model_simulation,period)#,'')\n",
    "                # existence of path_for_file tested in copernicus function\n",
    "                wind_path=copernicus_data(temporal_resolution,SSP,name_variable,model_simulation,year_str,area,path_for_file,out_path)\n",
    "                # area is determined in the \"Load shapefiles and plot\" part\n",
    "                if (wind_path is not None):\n",
    "                    Open_path = Dataset(wind_path) # open netcdf file\n",
    "                    lat_dataframe = np.ma.getdata(Open_path.variables['lat']).data\n",
    "                    lon_dataframe = np.ma.getdata(Open_path.variables['lon']).data\n",
    "                    data_with_all = ma.getdata(Open_path.variables[column_name]).data\n",
    "\n",
    "                    for moment in index_dates: # case if temporal resolution is daily\n",
    "                        data_dataframe = data_with_all[moment,:,:]\n",
    "                        time = (dates[moment],) # create tuple for iteration of dataframe\n",
    "                        print(time)\n",
    "                        # Create the MultiIndex\n",
    "                        midx = pd.MultiIndex.from_product([experiment, model, time, lat_dataframe],names=['Experiment', 'Model', 'Date', 'Latitude'])\n",
    "                        # multiindex to name the columns\n",
    "                        lon_str = ('Longitude',)\n",
    "                        cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "                        # Create the Dataframe\n",
    "                        Variable_dataframe = pd.DataFrame(data = data_dataframe, \n",
    "                                                    index = midx,\n",
    "                                                    columns = cols)\n",
    "                        # Concatenate former and new dataframe\n",
    "                        df = pd.concat([df,Variable_dataframe])# register information for project\n",
    "\n",
    "                    Open_path.close # to spare memory\n",
    "                else:\n",
    "                    print(\"Path does not exist\")\n",
    "                    pass\n",
    "        # test if dataframe is empty, if values exist for this period\n",
    "        if not df.empty: # if dataframe is not empty, value were registered, the first part is run : a path to register the csv file is created, and the dataframe is registered in a csv file\n",
    "            if not os.path.exists(path_for_csv):\n",
    "                os.makedirs(path_for_csv) # to ensure the creation of the path\n",
    "            fullname = os.path.join(path_for_csv, title_file)   \n",
    "            df.to_csv(fullname) # register dataframe in csv file\n",
    "            return df \n",
    "        else: # if the dataframe is empty, no value were found, there is no value to register or to return\n",
    "            print('No value were found for the period tested')\n",
    "            return # there is no dataframe to return\n",
    "    else:# test if the data were already downloaded; if yes, this part of the if is applied\n",
    "        print('The file was already downloaded') ##### PROBLEME : UNABLE TO TAKE DAT FROM CSV\n",
    "        csv_file=os.path.join(path_for_csv,title_file)\n",
    "        df = pd.read_csv(csv_file) # read the downloaded data for the analysis\n",
    "        \n",
    "        # register data for longitude, experiment, model time and latitude\n",
    "        lon_dataframe = df.loc[0]\n",
    "        lon_dataframe= lon_dataframe.dropna()# remove NAN of longitude series\n",
    "        lon_dataframe= lon_dataframe.reset_index(drop=True) # drop to avoid old index\n",
    "        \n",
    "        \n",
    "        experiment_serie=df['Unnamed: 0']\n",
    "        experiment_serie=experiment_serie.drop(index=[0,1])\n",
    "        experiment_serie=experiment_serie.drop_duplicates(keep='first')\n",
    "        experiment_serie=experiment_serie.reset_index(drop=True)\n",
    "        \n",
    "        model_serie=df['Unnamed: 1']\n",
    "        model_serie=model_serie.drop(index=[0,1])\n",
    "        model_serie=model_serie.drop_duplicates(keep='first')\n",
    "        model_serie=model_serie.reset_index(drop=True)\n",
    "        \n",
    "        time_serie=df['Unnamed: 2']\n",
    "        time_serie=time_serie.drop(index=[0,1])\n",
    "        time_serie=time_serie.drop_duplicates(keep='first')\n",
    "        time_serie=time_serie.reset_index(drop=True)\n",
    "        \n",
    "        lat_dataframe_serie=df['Unnamed: 3']\n",
    "        lat_dataframe_serie=lat_dataframe_serie.drop(index=[0,1])\n",
    "        lat_dataframe_serie=lat_dataframe_serie.drop_duplicates(keep='first')\n",
    "        lat_dataframe_serie=lat_dataframe_serie.reset_index(drop=True)\n",
    "        \n",
    "        # select data in dataframe\n",
    "        df.drop([0,1], axis=0,inplace=True) # remove 2 first lines\n",
    "        df.drop(['Unnamed: 0','Unnamed: 1','Unnamed: 2','Unnamed: 3'], axis=1,inplace=True) # remove 4 first columns\n",
    "        \n",
    "        midx = pd.MultiIndex.from_product([experiment_serie, model_serie, time_serie, lat_dataframe_serie],names=['Experiment', 'Model', 'Date', 'Latitude'])\n",
    "        # multiindex to name the columns\n",
    "        lon_str = ('Longitude',)\n",
    "        cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "        # Create the Dataframe\n",
    "        df = pd.DataFrame(data = df, \n",
    "                                index = midx,\n",
    "                                columns = cols)\n",
    "\n",
    "        #df.columns = ['Experiment', 'Model', 'Date', 'Latitude']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c8a29",
   "metadata": {},
   "source": [
    "### Display map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b244943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display a map\n",
    "def Display_map(indexes_lat,indexes_lon,lat,lon,lat_min_wanted,lat_max_wanted,lon_min_wanted,lon_max_wanted,data,title_png,title_to_adapt,label,parallels,meridians):#,projects):\n",
    "\n",
    "    lon_moz, lat_moz = np.meshgrid(lon, lat) # this is necessary to have a map\n",
    "    \n",
    "    # create Map for Mozambique coast\n",
    "    fig = plt.figure()\n",
    "    plt.title(title_to_adapt) # title of the map # automatized with year\n",
    "    map = Basemap(projection ='merc',llcrnrlon=lon_min_wanted+5,llcrnrlat=lat_min_wanted+2,urcrnrlon=lon_max_wanted-5,urcrnrlat=lat_max_wanted-2,resolution='i', epsg = 4326) # projection, lat/lon extents an\n",
    "    # adding and substracting a quantity to the lon and lat to have a bit of margin when presenting it\n",
    "    # substracting more to longitude because the range of longitude is -180 to 180. The range of latitude is -90 to 90\n",
    "    map.drawcountries()\n",
    "    map.drawcoastlines()\n",
    "    map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "    map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "\n",
    "    temp = map.contourf(lon_moz,lat_moz,data)\n",
    "    #projects.plot(ax=ax) # project in projection EPSG:4326\n",
    "    cb = map.colorbar(temp,\"right\", size=\"5%\", pad=\"2%\") # color scale, second parameter can be locationNone or {'left', 'right', 'top', 'bottom'}\n",
    "    cb.set_label(label) # name for color scale\n",
    "    plt.savefig(os.path.join(out_path,'figures',title_png),format ='png') # savefig or save text must be before plt.show. for savefig, format should be explicity written\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2e898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
