{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffefe9a6",
   "metadata": {},
   "source": [
    "# User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'pr_' # add list of available name variable\n",
    "# the user should indicate the years of the period of interest\n",
    "start_year = 2021\n",
    "stop_year = 2022 # if the user only wants one year, the same year as the start_year should be indicated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956bc9f3",
   "metadata": {},
   "source": [
    "# Import Packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "# to measure elapsed time\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import multiprocessing as mp# to download several file in parrallel\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13356092",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99aae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the name of the file from its url\n",
    "# the input is an url\n",
    "def extract_name_file(url):\n",
    "    index_before_name=url.rfind('/') # returns the highest index where the last character '/' was found, which is just before the name of the file    \n",
    "    name = url[index_before_name+1:len(url)] # return the name of the file as a string, with the suffix '.nc'\n",
    "    return name\n",
    "\n",
    "# function 'produce_name_list' produce a list of files' name, with the suffix '.nc'\n",
    "# 'produce_name_list' use the function 'extract_name_file' to have the name of a file from its url\n",
    "# the input is a list of url, from which we want to extract the corresponding names of files\n",
    "def produce_name_list(url_list):\n",
    "    name_list=[] # create empty list\n",
    "    for file in url_list:\n",
    "        f_name = extract_name_file(file) # return the name of the file as a string, with the suffix '.nc'\n",
    "        name_list.append(f_name) # add extracted name in the list\n",
    "    return name_list # return the list of names in the url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0318615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function produce_year produce:\n",
    "#      year: a vector containing all the year in the period of interest\n",
    "#      year_str: a array containing all the year in the period of interest in the string format\n",
    "#      index: a array containing the index of the year and year_str\n",
    "#### Parameters of the function\n",
    "#      first_year: number in int format, of the first year of the period of interest\n",
    "#      last_year: number in int format, of the last year of the period of interest\n",
    "def produce_year(first_year,last_year):\n",
    "    year = np.arange(first_year,(last_year+1),1) # create vector of years\n",
    "    year_str = [0]*len(year) # create initiale empty vector to convert years in int\n",
    "    index = np.arange(0,len(year)) # create vector of index for year\n",
    "    i = 0 # initialize index\n",
    "    for i in index: # convert all the date in string format\n",
    "        year_str[i]=str(year[i])\n",
    "    return (year, year_str, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions aims to regroup all the scenarios, models, time_aggregation and variables in vectors\n",
    "# the function use the function 'data_information'\n",
    "\n",
    "def information_files_in_vectors(name_list):\n",
    "    variables= []\n",
    "    time_aggregations= []\n",
    "    models= []\n",
    "    scenarios= []\n",
    "    for file_name in name_list:\n",
    "        (variable, time_aggregation, model, scenario, year) = data_information(file_name) \n",
    "        # use function data_information to find information concerning the file_name\n",
    "        if variable not in variables:\n",
    "            variables.append(variable)\n",
    "        if time_aggregation not in time_aggregations:\n",
    "            time_aggregations.append(time_aggregation)\n",
    "        if model not in models:\n",
    "            models.append(model)\n",
    "        if scenario not in scenarios:\n",
    "            scenarios.append(scenario)\n",
    "    return variables, time_aggregations,models,scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ead19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions aims to return the closest latitudes and longitudes to the projects, and the respectives index \n",
    "#  in the lat and lon vectors of the file\n",
    "def _lat_lon(path,lat_projects,lon_projects):\n",
    "    ds =  xr.open_dataset(path) \n",
    "    # ds.indexes['time'] gives back CFTimeIndex format, with hours. The strftime('%d-%m-%Y') permits to have time \n",
    "    # as an index, with format '%d-%m-%Y'. The .values permits to have an array\n",
    "    lat  = ds.lat.values\n",
    "    lon  = ds.lon.values\n",
    "    ds.close() # to spare memory\n",
    "    # preallocate space for the future vectors\n",
    "    index_closest_lat = []\n",
    "    index_closest_lon = []\n",
    "    closest_value_lat = []\n",
    "    closest_value_lon = []\n",
    "    for j in np.arange(0,len(lat_projects)):\n",
    "        (A,B)=closest_lat_lon_to_proj(lat_projects[j],lat)\n",
    "        index_closest_lat.append(A[0])\n",
    "        closest_value_lat.append(B[0])\n",
    "        (C,D)=closest_lat_lon_to_proj(lon_projects[j],lon)\n",
    "        index_closest_lon.append(C[0])\n",
    "        closest_value_lon.append(D[0])\n",
    "    return index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360414af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function aims to select the closest point to the geographical point of the project\n",
    "# the function takes as input \n",
    "#     location_project, which is a numpy.float64\n",
    "#     vector, which is a numpy.ndarray\n",
    "# the function returns\n",
    "#     closest_value[0], a numpy.float64\n",
    "\n",
    "def closest_lat_lon_to_proj(location_project,vector):\n",
    "    # the function any() returns a boolean value. Here, the function test if there are elements in the array \n",
    "    # containing the difference between the vector and the location_project, equal to the minimum of the absolute \n",
    "    # value of the difference between the vector and the location_project\n",
    "    if any(np.where((vector - location_project) == min(abs(vector - location_project)))[0]):\n",
    "        # the function any() returned True\n",
    "        # there is an element in the vector that is equal to the minimum of the absolute value of the difference \n",
    "        # between the vector and the location_project\n",
    "        \n",
    "        # the function np.where() returns the index for which (vector - location_project) == min(abs(vector - location_project))\n",
    "        index_closest = np.where((vector - location_project) == min(abs(vector - location_project)))[0]\n",
    "        closest_value = vector[index_closest]\n",
    "    else:\n",
    "        # the function any() returned False\n",
    "        # there is NO element in the vector that is equal to the minimum of the absolute value of the difference \n",
    "        # between the vector and the location_project\n",
    "        \n",
    "        # the function np.where() returns the index for which (vector - location_project) == -min(abs(vector - location_project))\n",
    "        index_closest = np.where((vector - location_project) == -min(abs(vector - location_project)))[0]\n",
    "        closest_value = vector[index_closest]\n",
    "    return index_closest, closest_value \n",
    "    # the function returns\n",
    "    #     first, the value of the index of the element of vector, that is the closest to location_project    \n",
    "    #     second, the array containing the element of vector, that is the closest to location_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73370f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## those three function are used to have the information concerning a file\n",
    "## information are in the name of the file, so the name of the file is used to find its related information\n",
    "## information mean variable, time_aggregation, model, scenario, year of the file\n",
    "\n",
    "### this function permit to extract the word before the first character '_' in the input 'name'\n",
    "### the input name is in format str\n",
    "### returning the new_name, without the word found, will permit to re-use the function to find all \n",
    "#     the information concerning the studied file\n",
    "def name_next_boundary(name):\n",
    "    index_before_name=name.find('_') # returns the lowest index where the character '_' was found\n",
    "    word = name[0:index_before_name] # first word in the string 'name', before the first character '_'\n",
    "    new_name = name.replace(word+'_','') # delete the word found from the string 'name'\n",
    "    return word, new_name # return, in string format, the word found (which is an information of the studied file), \n",
    "                    # and the string 'new_name', which is 'name' without the word found\n",
    "\n",
    "# this function permit to extract the year of the studied file\n",
    "# the year is always writen at the end of the name's file\n",
    "# the input name is in format str\n",
    "def find_year(name):\n",
    "    index_before_name=name.rfind('_') # returns the highest index where the character '_' was found\n",
    "    # the last character '_' is just before the year in the string 'name'\n",
    "    # determine if the string 'name' ends with '.nc'\n",
    "    if name.endswith('.nc'):\n",
    "        # 'name' ends with '.nc'\n",
    "        name_end = 3 # the three last character of the string name will be removed to find the year of the studied file\n",
    "    else:\n",
    "        # 'name' does not end with '.nc'\n",
    "        name_end = 0 # no character will be removed at the end of 'name' to find the year of the studied file\n",
    "    year = name[index_before_name+1:len(name)-name_end] # the year is extracted from the name of the file studied\n",
    "    # based on the index_before_name (highest index where the character '_' was found) and the suffix of 'name'\n",
    "    return year # the year in string format is returned\n",
    "\n",
    "# This function use the functions 'name_next_boundary' and 'find_year' to extract the information of the file studied\n",
    "# the input name is in format str, the name of the file from which we want information\n",
    "def data_information(name):\n",
    "    #### use of the function 'name_next_boundary': each time it is used, \n",
    "    # returns an information, and the name of the studied file without this information\n",
    "    (variable, shorten_name) = name_next_boundary(name)\n",
    "    (time_aggregation, shorten_name) = name_next_boundary(shorten_name)\n",
    "    (model, shorten_name) = name_next_boundary(shorten_name)\n",
    "    (scenario, shorten_name) = name_next_boundary(shorten_name)\n",
    "    #### use the function 'find_year' to extract the information 'year' from the string 'shorten_name'\n",
    "    year = find_year(shorten_name)\n",
    "    # the function returns all the information of the studied file\n",
    "    return variable, time_aggregation, model, scenario, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36aeb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function aims to create the empty dataframe that will be filled\n",
    "\n",
    "def create_empty_dataframe(name_project,scenarios,models,time,closest_value_lat,closest_value_lon):\n",
    "    df = pd.DataFrame()\n",
    "    for i in np.arange(0,len(name_project)):\n",
    "        midx = pd.MultiIndex.from_product([(name_project[i],),scenarios, models, time, (closest_value_lat[i],)],names=['Name project','Experiment', 'Model', 'Date', 'Latitude'])\n",
    "        cols = pd.MultiIndex.from_product([('Longitude',),(closest_value_lon[i],)])\n",
    "        Variable_dataframe = pd.DataFrame(data = [], \n",
    "                                    index = midx,\n",
    "                                    columns = cols)\n",
    "        df = pd.concat([df,Variable_dataframe])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used in 'create_dataframe'. The function aims to return the path of the file of interest\n",
    "# The function looks into a list of name which name in the list has every input \n",
    "# The inputs are:\n",
    "#    out_path: a general file path where the files are registered, \n",
    "#    name_file_list: a list of files' names\n",
    "#    variable: the name of the variable of interest\n",
    "#    model: the model of interest (example: ACCESS-CM2)\n",
    "#    scenario: the scenario of interest (example:ssp245)\n",
    "#    year: the year of interest\n",
    "#    ensemble: the ensemble of interest (example: r1i1p1f1_gn)\n",
    "# the output is:\n",
    "#    the path of the file corresponding to all the parameters indicated in input\n",
    "\n",
    "def find_path_file(out_path,name_file_list,variable,temporal_resolution,model,scenario,year,ensemble):\n",
    "    # look into the list of names if find a name with every parameter indicated in inputs\n",
    "    name_found = [name for name in name_file_list if scenario in name and model in name and year in name and ensemble in name and temporal_resolution in name]\n",
    "    print('The name of the file is ' + name_found[0])\n",
    "    if name_found == []:\n",
    "        # no name with all the parameters indicated as inputs was found\n",
    "        return name_found # return an empty element instead of a path, the function does not run the following lines\n",
    "    # the name was found, so prepare the path of the file of interest\n",
    "    path = os.path.join(out_path,name_found[0])\n",
    "    return path # return the path of the file of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f70e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the register_data_in_dataframe function aims to test if data with the specific parameters exist in the folder of concern\n",
    "# As inputs :\n",
    "#      the list of urls of the files of interest. The name of the file will be extracted from them\n",
    "#      temporal_resolution: the temporal resolution of the climate variable in question in string format\n",
    "#      year_str: a vector containing the year of the period of interest in a string format\n",
    "#      scenarios: a list of the scenorios of interest in string format\n",
    "#      models: a list of the models of interest in string format\n",
    "#      out_path: the out_path in a string format\n",
    "#      name_variable: the name of the variable of interest (example: 'pr' for precipitation)\n",
    "#      name_project: the list of names of the project of interest\n",
    "#      index_closest_lat: array containing an index for each project, \n",
    "#                           corresponding to the index of the value in latitude vector which is the closest to \n",
    "#                           the project latitude\n",
    "#      index_closest_lat: array containing an index for each project, \n",
    "#                           corresponding to the index of the value in longitude vector which is the closest to \n",
    "#                           the project longitude\n",
    "#      closest_value_lat: array containing a value for each project, corresponding to the value in the \n",
    "#                           latitude vector which is the closest to the project's latitude\n",
    "#      closest_value_lon: array containing a value for each project, corresponding to the value in the \n",
    "#                           longitude vector which is the closest to the project's longitude\n",
    "#      df : empty dataframe to fill\n",
    "\n",
    "# Outputs are:\n",
    "#      df: the filled dataframe with the values of interest\n",
    "#      path_file_not_found: the list of files that were not found with the parameters asked\n",
    "#      ds_did_not_open: the list of files that could not be read\n",
    "\n",
    "def register_data_in_dataframe(name_list,temporal_resolution,year_str,scenarios,models,out_path, name_variable, name_project,index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon,df):    \n",
    "    path_file_not_found = [] # create empty list to register names of files that were not found with the corresponding parameters\n",
    "    ds_did_not_open = [] # create empty list to register names of files that couldn't be opened\n",
    "    for year in year_str:\n",
    "        for SSP in scenarios:\n",
    "            for model_simulation in models:\n",
    "                # for each year, each scenarios and each models, test if there is a corresponding file existing\n",
    "                # with function 'find_path_file'\n",
    "                climate_variable_path = find_path_file(out_path,name_list,name_variable,temporal_resolution,model_simulation,SSP,year,'r1i1p1f1_gn')\n",
    "                if climate_variable_path!= []:\n",
    "                    # a file with the corresponding parameters were found\n",
    "                    try: # to register information from the dataset ds in the dataframe df\n",
    "                        ds =  xr.open_dataset(climate_variable_path) # open the file corresponding to the parameters\n",
    "                        time = ds.indexes['time'].strftime('%d-%m-%Y').values # register the time in the file\n",
    "                        for i in np.arange(0,len(name_project)):\n",
    "                            print('For the year '+year+' and project '+name_project[i]+', test with scenario '+SSP+', with model '+model_simulation)\n",
    "                            # for each year, scenarios, models and each project, the values of the opened dataset ds\n",
    "                            # are registered in the empty dataframe df, to a specific place corresponding to the parameters of the loop\n",
    "                            df.loc[(name_project[i],SSP,model_simulation,closest_value_lat[i],closest_value_lon[i],time)] = ds.pr.isel(lat=index_closest_lat[i],lon=index_closest_lon[i]).values\n",
    "                        ds.close() # the opened dataset is closed to spare memory\n",
    "                    except: # the dataset ds can not be read\n",
    "                        # add information of the dataset that can't be read in the empty list ds_did_not_open\n",
    "                        ds_did_not_open.append(climate_variable_path)\n",
    "                        print(climate_variable_path + ' did not open with ds')\n",
    "                        ds.close() # the opened dataset is closed to spare memory\n",
    "                        continue # try with next model\n",
    "                else:\n",
    "                    # NO file with the corresponding parameters were found\n",
    "                    # add information of the missing file in the empty list path_file_not_found\n",
    "                    path_file_not_found.append(name_variable+'_'+temporal_resolution+'_'+model_simulation+'_'+SSP+'_'+year+'_'+'r1i1p1f1_gn')\n",
    "                    continue # try another file\n",
    "    return df,path_file_not_found,ds_did_not_open\n",
    "\n",
    "# the function df_to_csv aims to return the filled dataframe in a csv format\n",
    "# Inputs are:\n",
    "#       df: the dataframe that should be register in a csv file\n",
    "#      path_for_csv: this is the path where the csv file should be registered, in a string format\n",
    "#      title_file: this is the name of the csv file to be created in a string format\n",
    "#                  CAREFUL --> title_file MUST have the extension of the file in the string (.csv for example)\n",
    "# Output is:\n",
    "#      in the case where the dataframe is not empty, the ouput is the full path to the created csv file\n",
    "#      in the case where the dataframe is empty, the output is an empty list\n",
    "def df_to_csv(df,path_for_csv,title_file):\n",
    "    # test if dataframe is empty, if values exist for this period\n",
    "    if not df.empty: \n",
    "        # if dataframe is not empty, value were registered, the first part is run : \n",
    "        # a path to register the csv file is created, .....\n",
    "        if not os.path.isdir(path_for_csv):\n",
    "            # the path to the file does not exist\n",
    "            os.makedirs(path_for_csv) # to ensure creation of the folder\n",
    "            # creation of the path for the csv file, in a string format\n",
    "        full_name = os.path.join(path_for_csv,title_file)\n",
    "        # ..... and the dataframe is registered in a csv file\n",
    "        df.to_csv(full_name) # register dataframe in csv file\n",
    "        print('Path for csv file is: ' + full_name)\n",
    "        return full_name # return the full path that leads to the created csv file\n",
    "    else: # if the dataframe is empty, no value were found, there is no value to register or to return\n",
    "        print('The dataframe is empty')\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88288ad3",
   "metadata": {},
   "source": [
    "# Projects information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project information\n",
    "\n",
    "name_project_data = np.array(['WTP_Mutua_EIB', 'Gorongosa_EIB', 'Chimoio_WTP_EIB', 'Pemba_EIB'])\n",
    "name_project = pd.Series(name_project_data)\n",
    "\n",
    "lon_projects_data = np.array([34.5927839939706, 34.07824286310398 , 33.47333313659342, 40.52545156033736])\n",
    "lon_projects = pd.Series(lon_projects_data)\n",
    "\n",
    "lat_projects_data = np.array([-19.495079648575242, -18.68063728746643, -19.125095255188334,-12.973942656747809])\n",
    "lat_projects = pd.Series(lat_projects_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0efe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "data_folder=r'\\\\COWI.net\\projects\\A245000\\A248363\\CRVA\\Datasets'\n",
    "project_location_path=os.path.join(data_folder,'Mozambique_PPIS/EIB_locations_few.shp')\n",
    "#study boundary (optional)\n",
    "study_area_path=os.path.join(data_folder,'Mozambique_PPIS/mozambique.shp')\n",
    "\n",
    "\n",
    "#projection CRS (default = 'EPSG:4326')\n",
    "bCRS='EPSG:4326'\n",
    "\n",
    "mCRS='EPSG:31983' #metric CRS for buffer in meters (find relevant metric CRS for location!)\n",
    "\n",
    "#load shapefiles\n",
    "projects = gpd.read_file(project_location_path).to_crs(bCRS)\n",
    "project_id='Name' #name of column used as id\n",
    "\n",
    "study_area = gpd.read_file(study_area_path).to_crs(bCRS)\n",
    "\n",
    "# prepare name_project for use \n",
    "name_project = projects['Name'].str.replace(' ','_') # take off every blank space of project names\n",
    "name_project = name_project.str.replace('(','') # take off every ( of project names\n",
    "name_project = name_project.str.replace(')','') # take off every ) of project names\n",
    "name_project = name_project.str.replace('-','') # take off every - of project names\n",
    "name_project = name_project.str.replace('/','_') # take off every / of project names\n",
    "name_project = name_project.str.replace(r'\"\\\"','_') # take off every \\ of project names\n",
    "\n",
    "# register geographic information concerning projects\n",
    "lon_projects = projects['geometry'].x\n",
    "lat_projects = projects['geometry'].y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e579f9",
   "metadata": {},
   "source": [
    "# Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08376b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path=r'\\\\COWI.net\\projects\\A245000\\A248363\\CRVA\\Datasets\\NEX-GDDP-CMIP6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7866458",
   "metadata": {},
   "source": [
    "# Complete list of url with files to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register information from csv file\n",
    "#all_urls = pd.read_csv(r'C:\\Users\\CLMRX\\OneDrive - COWI\\Documents\\GitHub\\CRVA_tool\\outputs\\NEX-GDDP-CMIP6\\gddp-cmip6-thredds-fileserver.csv')\n",
    "csv_path = os.path.join(out_path,'gddp-cmip6-thredds-fileserver.csv')\n",
    "all_urls = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "### make all elements of the csv into a readable list\n",
    "\n",
    "temp_list = all_urls[[' fileUrl']].T# transpose csv\n",
    "temp_list=temp_list.values.tolist()\n",
    "temp_list=temp_list[0]\n",
    "url_list=[s.replace(' ', '') for s in temp_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## download only precipitation data\n",
    "# select only precipitation files, between 2040 and 2080\n",
    "url_list_climate_var = [url for url in url_list if variable_of_interest in url and int(url[len(url)-7:len(url)-3])>=start_year and int(url[len(url)-7:len(url)-3])<=stop_year and 'r1i1p1f1_gn' in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(url_list_climate_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79797cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list_climate_var = produce_name_list(url_list_climate_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list_climate_var[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000550ba",
   "metadata": {},
   "source": [
    "# Produce csv files with data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488dc623",
   "metadata": {},
   "outputs": [],
   "source": [
    "(year, year_str, index_year) = produce_year(start_year,stop_year)\n",
    "time = pd.date_range('01-01-'+str(start_year),'31-12-'+str(stop_year), freq='D').strftime('%d-%m-%Y').values # use to create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.date_range('01-01-'+str(start_year),'31-12-'+str(stop_year), freq='D').strftime('%d-%m-%Y').values # use to create dataframe\n",
    "months = [time[i][3:5] for i in np.arange(0,len(time))]\n",
    "years = [time[i][6:10] for i in np.arange(0,len(time))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e285ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables, time_aggregations,models,scenarios in the name_list_precipitation\n",
    "(variables, time_aggregations,models,scenarios)=information_files_in_vectors(name_list_climate_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d93775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell aims to extract the index in the lat_projects and lon_projects vectors, corresponding to the values of the\n",
    "# closest latitude and longitude to the projects\n",
    "index_closest_lat = []\n",
    "while index_closest_lat == []: # once the information where extracted, no need to continue looking\n",
    "    for name in name_list_climate_var: # for loop to test the following names if the precedent one did not work\n",
    "        try: # test to use function '_lat_lon' with this path\n",
    "            path = os.path.join(out_path,name)\n",
    "            print(path)\n",
    "            (index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon)=_lat_lon(path,lat_projects,lon_projects)\n",
    "            print(index_closest_lat)\n",
    "            # this function '_lat_lon' use xr.open_dataset(path). If there is a problem with this opening process\n",
    "            # (because the path given has a problem for example), the function can continue to test with the next \n",
    "            # file to extract the information of interest (the index and value of the closest latitude and longitude)\n",
    "            break\n",
    "        except:\n",
    "            continue # the informations where not extracted. Continue the for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911250c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the empty dataframe, based on the information from the names' files\n",
    "#df_climate_var=create_empty_dataframe(name_project,scenarios,models,time,closest_value_lat,closest_value_lon)\n",
    "df_climate_var=create_empty_dataframe([name_project[0]],[scenarios[0]],[models[0]],time,[closest_value_lat[0]],[closest_value_lon[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9186615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate_var#.loc[('PT_Revubue_2_Rev_2_01','ssp245','ACCESS-CM2','01-12-2021')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752099b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_climate_var=register_data_in_dataframe(name_list_climate_var,time_aggregations[0],year_str,scenarios,models,out_path, variable_of_interest, name_project,lon_projects,lat_projects,index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon,df_climate_var)\n",
    "start_t = timer()\n",
    "(df_climate_var,path_file_not_found,ds_did_not_open)=register_data_in_dataframe(name_list_climate_var,time_aggregations[0],year_str,[scenarios[0]],[models[0]],out_path, variable_of_interest, [name_project[0]],[index_closest_lat[0]],[index_closest_lon[0]],[closest_value_lat[0]],[closest_value_lon[0]],df_climate_var)\n",
    "end_t = timer()\n",
    "print('It took '+str(round(end_t - start_t,2))+' seconds to register the data of interest in dataframe')\n",
    "print('It took '+str(round((end_t - start_t)/3600,2))+' hours to register the data of interest in dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51654a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42407788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register information\n",
    "\n",
    "# register dataframe in a csv format\n",
    "title_file = 'All_projects_moz_'+variable_of_interest+'_'+str(start_year)+'-'+str(stop_year)+'.csv'\n",
    "path_for_csv = os.path.join(out_path,'csv_file',variable_of_interest+'_'+time_aggregations[0]+'_'+str(start_year)+'-'+str(stop_year))\n",
    "path_csv = df_to_csv(df_climate_var,path_for_csv,title_file)\n",
    "\n",
    "# register path_file_not_found in a file format\n",
    "if path_file_not_found != []:\n",
    "    txt_file_path = os.path.join(path_for_csv,'Path_file_not_found')\n",
    "    with open(txt_file_path, 'w') as text_file:\n",
    "        text_file.write(path_file_not_found)\n",
    "    print('Not every files were found')\n",
    "else:\n",
    "    print('Every files were found')\n",
    "        \n",
    "# register ds_did_not_open in a file format\n",
    "if ds_did_not_open != []:\n",
    "    txt_file_path = os.path.join(path_for_csv,'ds_did_not_open')\n",
    "    with open(txt_file_path, 'w') as text_file:\n",
    "        text_file.write(ds_did_not_open)\n",
    "    print('Not every files were opened successfully')\n",
    "else:\n",
    "    print('Every files were opened')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9a2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b38c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb632fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237acdc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108e0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbc0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28594fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82fc7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115dafcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3e704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b48f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7916a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd6921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
