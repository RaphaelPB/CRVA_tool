{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebaef011",
   "metadata": {},
   "source": [
    "This notebook aims to contain all functions for indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gumbel_r\n",
    "from scipy.stats import gumbel_l\n",
    "import os\n",
    "import os.path\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471b1eb",
   "metadata": {},
   "source": [
    "# Treat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_station(df,name_col,name_station):\n",
    "    df_name_station = df[df[name_col]==name_station]\n",
    "    return df_name_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b53d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Year, Month and Season to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_year_month_season(df,column_date):\n",
    "    # add Year, month and season columns for graphs\n",
    "    Year = df[[column_date]].values.reshape(len(df[[column_date]].values),)\n",
    "    Month = df[[column_date]].values.reshape(len(df[[column_date]].values),)\n",
    "    Season = df[[column_date]].values.reshape(len(df[[column_date]].values),)\n",
    "    \n",
    "    if str(Year[1]).find('-')==2 or str(Year[1]).find('/')==2:\n",
    "        for i in np.arange(0,len(df[[column_date]].values)):\n",
    "            Year[i]=int(Year[i][6:10])\n",
    "            Month[i]=int(Month[i][3:5])\n",
    "            if Month[i]>3 and Month[i]<10: # dry season in Mozambique is between April and September\n",
    "                Season[i]='Dry'\n",
    "            else:# humid season is between October and March\n",
    "                Season[i]='Humid'\n",
    "            \n",
    "            Month[i]=str_month(Month[i])\n",
    "            \n",
    "    if str(Year[1]).find('-')==4 or str(Year[1]).find('/')==4:\n",
    "        for i in np.arange(0,len(df[[column_date]].values)):\n",
    "            Year[i]=int(Year[i][0:4])\n",
    "            Month[i]=int(Month[i][5:7])\n",
    "            if Month[i]>3 and Month[i]<10: # dry season in Mozambique is between April and September\n",
    "                Season[i]='Dry'\n",
    "            else:# humid season is between October and March\n",
    "                Season[i]='Humid'\n",
    "            \n",
    "            Month[i]=str_month(Month[i])\n",
    "                \n",
    "    df['Year'] = Year\n",
    "    df['Month'] = Month\n",
    "    df['Season'] = Season\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_month(int_m):\n",
    "    if int_m==1:\n",
    "        str_m = 'Jan'\n",
    "    if int_m==2:\n",
    "        str_m = 'Feb'    \n",
    "    if int_m==3:\n",
    "        str_m = 'Mar'\n",
    "    if int_m==4:\n",
    "        str_m = 'Apr'\n",
    "    if int_m==5:\n",
    "        str_m = 'May'\n",
    "    if int_m==6:\n",
    "        str_m = 'Jun'\n",
    "    if int_m==7:\n",
    "        str_m = 'Jul'\n",
    "    if int_m==8:\n",
    "        str_m = 'Aug'    \n",
    "    if int_m==9:\n",
    "        str_m = 'Sep'\n",
    "    if int_m==10:\n",
    "        str_m = 'Oct'\n",
    "    if int_m==11:\n",
    "        str_m = 'Nov'\n",
    "    if int_m==12:\n",
    "        str_m = 'Dec'\n",
    "    return str_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced5bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is meant to filter the data wnated \n",
    "def filter_dataframe(df,name_projects,list_model_to_kill,start_y=1950,stop_y=2100):\n",
    "    df = df.reset_index() # to take out multiindex that may exist and complicate filtering process\n",
    "    \n",
    "    if list_model_to_kill!=[]:\n",
    "        for name_model in list_model_to_kill:\n",
    "            df = df[df['Model']!=name_model]\n",
    "    \n",
    "    df_final= pd.DataFrame()\n",
    "    for name_project in name_projects:\n",
    "        df_temp = df[df['Name project']==name_project] # select only data of interest\n",
    "        df_final = pd.concat([df_final,df_temp])\n",
    "    \n",
    "    if 'Year' not in list(df_final.columns):\n",
    "        df_final=add_year_month_season(df_final,'Date') # add column 'Year', 'Month', 'Season'\n",
    "    \n",
    "    if start_y!=1950 or stop_y!=2100:\n",
    "        df_final = df_final[df_final['Year'].between(start_y,stop_y)] # select only the years of interest\n",
    "    if 'index' in df_final.columns:\n",
    "        df_final= df_final.drop('index',axis=1)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dacb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function aims to find the correct name of the column of interest\n",
    "\n",
    "def find_name_col(df,climate_var_longName):\n",
    "    try:\n",
    "        try:\n",
    "            try:\n",
    "                old_title_column=df.filter(like=climate_var_longName, axis=1).columns[0]\n",
    "            except:\n",
    "                old_title_column=df.filter(like=climate_var_longName.capitalize(), axis=1).columns[0]\n",
    "        except:\n",
    "            old_title_column=df.filter(like=climate_var_longName.upper(), axis=1).columns[0]\n",
    "    except:\n",
    "        old_title_column=df.filter(like=climate_var_longName.lower(), axis=1).columns[0]\n",
    "    return old_title_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function df_to_csv aims to return the filled dataframe in a csv format\n",
    "# Inputs are:\n",
    "#       df: the dataframe that should be register in a csv file\n",
    "#      path_for_csv: this is the path where the csv file should be registered, in a string format\n",
    "#      title_file: this is the name of the csv file to be created in a string format\n",
    "#                  CAREFUL --> title_file MUST have the extension of the file in the string (.csv for example)\n",
    "# Output is:\n",
    "#      in the case where the dataframe is not empty, the ouput is the full path to the created csv file\n",
    "#      in the case where the dataframe is empty, the output is an empty list\n",
    "\n",
    "def df_to_csv(df,path_for_csv,title_file):\n",
    "    # test if dataframe is empty, if values exist for this period\n",
    "    if not df.empty: \n",
    "        # if dataframe is not empty, value were registered, the first part is run : \n",
    "        # a path to register the csv file is created, .....\n",
    "        if not os.path.isdir(path_for_csv):\n",
    "            # the path to the file does not exist\n",
    "            os.makedirs(path_for_csv) # to ensure creation of the folder\n",
    "            # creation of the path for the csv file, in a string format\n",
    "        full_name = os.path.join(path_for_csv,title_file)\n",
    "        # ..... and the dataframe is registered in a csv file\n",
    "        df.to_csv(full_name) # register dataframe in csv file\n",
    "        print('Path for csv file is: ' + full_name)\n",
    "        return full_name # return the full path that leads to the created csv file\n",
    "    else: # if the dataframe is empty, no value were found, there is no value to register or to return\n",
    "        print('The dataframe is empty')\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b3b6c",
   "metadata": {},
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85f54c",
   "metadata": {},
   "source": [
    "### Return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return period for each project, model, scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd81392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function value for return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_coresponding_to_return_period(loc,scale,T):\n",
    "    p_non_exceedance = 1 - (1/T)\n",
    "    try:\n",
    "        threshold_coresponding = round(gumbel_r.ppf(p_non_exceedance,loc,scale))\n",
    "    except OverflowError: # the result is not finite\n",
    "        if math.isinf(gumbel_r.ppf(p_non_exceedance,loc,scale)) and gumbel_r.ppf(p_non_exceedance,loc,scale)<0:\n",
    "            # ppf is the inverse of cdf\n",
    "            # the result is -inf\n",
    "            threshold_coresponding = 0 # the value of wero is imposed\n",
    "    return threshold_coresponding\n",
    "    # ppf: Percent point function\n",
    "    #print('Threshold '+str(threshold_coresponding)+' mm/day will be exceeded at least once in '+str(n)+' year, with a probability of '+str(round(p_exceedance*100))+ ' %')\n",
    "    #print('This threshold corresponds to a return period of '+str(round(return_period))+ ' year event over a '+str(n)+' year period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return return period for dataframe of modelled data\n",
    "def dataframe_threshold_coresponding_to_return_period_model(df,name_col):\n",
    "    df_copy=df.copy(deep=True)\n",
    "    df_copy=df_copy.drop(labels='Date',axis=1)\n",
    "    df_max = df_copy.groupby(['Name project','Experiment','Model','Year']).max() # maximum    \n",
    "    midx = pd.MultiIndex.from_product([list(set(df_copy[df_copy.columns[0]])),list(set(df_copy[df_copy.columns[1]])),list(set(df_copy[df_copy.columns[2]]))],names=['Name project','Experiment', 'Model'])\n",
    "    cols = ['Value for return period 50 years mm/day','Value for return period 100 years mm/day']\n",
    "    return_period = pd.DataFrame(data = [], \n",
    "                                index = midx,\n",
    "                                columns = cols)\n",
    "    for name_p in return_period.index.levels[0].tolist():\n",
    "        for ssp in return_period.index.levels[1].tolist():\n",
    "            for model in return_period.index.levels[2].tolist():\n",
    "                print('Name project '+name_p+ ' ssp '+ssp+ ' model '+model)\n",
    "                Z=df_max.loc[(name_p,ssp,model)][name_col].values.reshape(len(df_max.loc[(name_p,ssp,model)][name_col]),)\n",
    "                (loc1,scale)=stats.gumbel_r.fit(Z) # return the function necessary to establish the continous function\n",
    "                # choice of gumbel because suits to extreme precipitation\n",
    "                return_period.loc[(name_p,ssp,model),('Value for return period 50 years mm/day')] = threshold_coresponding_to_return_period(loc1,scale,50)\n",
    "                return_period.loc[(name_p,ssp,model),('Value for return period 100 years mm/day')] = threshold_coresponding_to_return_period(loc1,scale,100)\n",
    "                \n",
    "    return return_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return return period for dataframe of observed data\n",
    "def dataframe_threshold_coresponding_to_return_period_obs(df,name_col):\n",
    "    df_copy=df.copy(deep=True)\n",
    "    df_copy=df_copy.drop(labels='Date',axis=1)\n",
    "    df_max = df_copy.groupby(['Name project','Year'])[[name_col]].max() # maximum    \n",
    "    midx = pd.MultiIndex.from_product([list(set(df_copy[df_copy.columns[0]]))],names=['Name project'])\n",
    "    cols = ['Value for return period 50 years mm/day','Value for return period 100 years mm/day']\n",
    "    return_period = pd.DataFrame(data = [], \n",
    "                                index = midx,\n",
    "                                columns = cols)\n",
    "    for name_p in return_period.index.levels[0].tolist():\n",
    "        print('Name project '+name_p)\n",
    "        Z=df_max.loc[(name_p)][name_col].values.reshape(len(df_max.loc[(name_p)][name_col]),)\n",
    "        (loc1,scale)=stats.gumbel_r.fit(Z) # return the function necessary to establish the continous function\n",
    "        # choice of gumbel because suits to extreme precipitation\n",
    "        return_period.loc[(name_p,ssp,model),('Value for return period 50 years mm/day')] = threshold_coresponding_to_return_period(loc1,scale,50)\n",
    "        return_period.loc[(name_p,ssp,model),('Value for return period 100 years mm/day')] = threshold_coresponding_to_return_period(loc1,scale,100)\n",
    "\n",
    "    return return_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5491bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function a check\n",
    "def return_period_coresponding_to_threshold(Z):\n",
    "    (loc,scale)=stats.gumbel_r.fit(Z) # return the function necessary to establish the continous function\n",
    "    # gumbel_r is chosen because\n",
    "    #try:\n",
    "    p_non_exceedance = round(gumbel_r.cdf(max(Z),loc,scale))\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.ppf.html\n",
    "    #except OverflowError: # the result is not finite\n",
    "        \n",
    "   #     if math.isinf(gumbel_r.cdf(threshold,loc,scale)) and gumbel_r.cdf(max(Z),loc,scale)<0:\n",
    "   #         # the result is -inf\n",
    "    #        threshold_coresponding = 0 # the value of wero is imposed\n",
    "    return_period_coresponding = 1/(1-p_non_exceedance)\n",
    "    return return_period_coresponding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sure de cette function\n",
    "def return_period(Z,T,start_y,stop_y):\n",
    "    Z = df[df['Year'].between(start_y,stop_y)].groupby('Year')[['pr']].agg(np.nanmax)#.reshape(len(pr_obs_gorongosa_from_gorongosa.groupby('Year')[['pr']].max()),)\n",
    "    #Z = Z[~np.isnan(Z)]\n",
    "    (loc1,scale1)=scipy.stats.gumbel_r.fit(Z) # return the function necessary to establish the continous function\n",
    "    value_for_T=threshold_coresponding_to_return_period(loc1,scale1,T)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01687c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions Temps retour :\n",
    "#      \n",
    "#      besoin de caler mieux distribution ? package pour le faire automatiquement ? si on fiat pas avec maxima, mais on va faire que avec maxima pour le moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19779648",
   "metadata": {},
   "source": [
    "## calculation Yearly average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bca0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function only works for projections\n",
    "def temporal_avg(df,climate_var_long_name,title_column,temporal_resolution):\n",
    "    df_yearly_avg = df.copy(deep =True)\n",
    "    old_name_column = find_name_col(df,climate_var_long_name)\n",
    "    df_yearly_avg=df_yearly_avg.rename(columns={old_name_column:title_column})\n",
    "    if 'pr' in title_column.lower():\n",
    "        if temporal_resolution == 'year':\n",
    "            df_yearly_avg = df_yearly_avg.groupby(['Name project','Experiment','Model','Year'])[[title_column]].mean()*365.25\n",
    "        if temporal_resolution == 'month':\n",
    "            df_yearly_avg = df_yearly_avg.groupby(['Name project','Experiment','Model','Month'])[[title_column]].mean()*30\n",
    "    else:\n",
    "        if temporal_resolution == 'year':\n",
    "            df_yearly_avg = df_yearly_avg.groupby(['Name project','Experiment','Model','Year'])[[title_column]].mean()\n",
    "        if temporal_resolution == 'month':\n",
    "            df_yearly_avg = df_yearly_avg.groupby(['Name project','Experiment','Model','Month'])[[title_column]].mean()\n",
    "    return df_yearly_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function only works for projections\n",
    "def df_stat_distr(df):\n",
    "    df = df.groupby(['Name project']).describe(percentiles=[.1, .5, .9])\n",
    "    # if describe() does not return al wanted statistics, it is maybe because the elements in it are not recognized as int\n",
    "    # add astype(int) as in following example; df.astype(int).groupby(['Name project']).describe(percentiles=[.1, .5, .9])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a9615",
   "metadata": {},
   "source": [
    "# Precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfaec75",
   "metadata": {},
   "source": [
    "### N-day event "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some models to not have any values for some scenarios\n",
    "# need to delete them from the global dataset\n",
    "\n",
    "# PROBLEM AVEC CETTE FUNCTION\n",
    "\n",
    "def delete_NaN_model(df):\n",
    "    df_copy=df.copy(deep=True) # copy the original dataframe, not to modify the original one    \n",
    "    model_to_delete =[]\n",
    "    longitude=[]\n",
    "    for project in df_copy.index.levels[0].tolist(): # projects\n",
    "        # look value of longitude for each project\n",
    "        for j in np.arange(0,len(df_copy.loc[[project]].columns)):\n",
    "            if ~df_copy[[df_copy.columns[j]]].isnull().values.all():\n",
    "                # can check if a pandas DataFrame contains NaN/None values in any cell \n",
    "                # (all rows & columns ). This method returns True if it finds NaN/None \n",
    "                # on any cell of a DataFrame, returns False when not found\n",
    "                longitude.append(df_copy.columns[j])\n",
    "                continue\n",
    "        \n",
    "        for scenario in df_copy.index.levels[1].tolist(): # scenarios\n",
    "            for model in df_copy.index.levels[2].tolist(): # models\n",
    "                if df_copy.loc[(project,scenario, model)].isnull().values.all():\n",
    "                    print('No data for Project '+ project+', scenario '+scenario+', model '+model)\n",
    "                    # all the values for the given project, scenario and model are NaN\n",
    "                    if model not in model_to_delete:\n",
    "                        model_to_delete.append(model)# keep unique values\n",
    "    \n",
    "    if model_to_delete!=[]:\n",
    "        # for some given project, scenario and model, there is no values\n",
    "        for model in model_to_delete:\n",
    "            models_index = df_copy.index.levels[2].tolist()\n",
    "            models_index.remove(model)\n",
    "            df_copy.drop(labels=model,level=2,inplace=True)\n",
    "        \n",
    "        return models_index\n",
    "        # create new dataframe with correct index\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051817a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions aims to calculate the n_day_event\n",
    "def n_day_maximum_rainfall(number_day,df):\n",
    "    df1=df.copy(deep=True)\n",
    "    # df.use function rolling(n).sum() to calculate cumulative precipitation over n days\n",
    "    df1[['Precipitation mm']]=df1[['Precipitation mm']].rolling(number_day).sum()\n",
    "    time=df1.index.tolist()\n",
    "    for k in np.arange(len(time)-number_day,-1,-1):\n",
    "        time[number_day-1+k] = time[k] + ' to '+time[number_day-1+k]\n",
    "    df1.drop(df1.index[np.arange(0,number_day-1)], inplace=True) # delete first elements which are NaNs\n",
    "    del time[0:number_day-1] # delete firsts elements, which have no value associated with\n",
    "    #midx = pd.MultiIndex.from_product([ time],names=['Date'])\n",
    "    name_col = ['Precipitation mm']\n",
    "    Dataframe_n_day_event = pd.DataFrame(data = df1.values, \n",
    "                                index = [time],\n",
    "                                columns = name_col)\n",
    "    return Dataframe_n_day_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c8fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function aims to create the empty dataframe that will be filled\n",
    "\n",
    "def fill_dataframe(name_project,scenario,model,time,data_df,name_col):\n",
    "    #df = pd.DataFrame()\n",
    "    #for i in np.arange(0,len(name_project)):\n",
    "    midx = pd.MultiIndex.from_product([name_project,scenario,model , time],names=['Name project','Experiment', 'Model', 'Date'])\n",
    "    name_col = [name_col]#['Precipitation '+str(number_day)+' day event mm']\n",
    "    Variable_dataframe = pd.DataFrame(data = data_df, \n",
    "                                index = midx,\n",
    "                                columns = name_col)\n",
    "        #df = pd.concat([df,Variable_dataframe])\n",
    "    return Variable_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function dataframe_n_day_event produce a dataframe, with the n_day event precipitation for the period, models and scenarios asked\n",
    "# this function use the function : 'delete_NaN_model', 'n_day_maximum_rainfall' and 'fill_dataframe'\n",
    "\n",
    "\n",
    "def dataframe_n_day_event(df,number_day):\n",
    "    df_copy=df.copy(deep=True) # copy the original dataframe, not to modify the original one    \n",
    "    df_n_day_event = pd.DataFrame() # create empty dataframe, that will be filled later\n",
    "    # extract years of the period of interest, make a vector containing all the years of interest\n",
    "    years = np.arange(int(df.index.levels[3].tolist()[0][6:10]),int(df.index.levels[3].tolist()[len(df.index.levels[3].tolist())-1][6:10])+1)\n",
    "    #models_index=delete_NaN_model(df_copy) # use function 'delete_NaN_model' to know which models have no Nan values\n",
    "    models_index = df_copy.index.levels[2].tolist()\n",
    "    models_index.remove('NESM3')\n",
    "    df_copy=df_copy.droplevel(level=4) # drop latitude index\n",
    "    df_copy.columns = df_copy.columns.droplevel(0) # drop first level of column name\n",
    "    for project in df_copy.index.levels[0].tolist(): # projects\n",
    "        for scenario in df_copy.index.levels[1].tolist(): # scenarios\n",
    "            for model in models_index: # models\n",
    "                print('Project '+ project+', scenario '+scenario+', model '+model)\n",
    "                # select on project, one scenario, one model and drop Latitude index\n",
    "                df_temp_all_years = df_copy.loc[(project,scenario,model)]\n",
    "                # find which columns does not only have NaN\n",
    "                for j in np.arange(0,len(df_temp_all_years.columns)): # for loop to have number of the column\n",
    "                    if ~df_temp_all_years[[df_temp_all_years.columns[j]]].isnull().values.all():\n",
    "                        # the column does not only have Nan values\n",
    "                        df_temp_all_years=df_temp_all_years[[df_temp_all_years.columns[j]]] # register only column with values, and not the NaN values\n",
    "                        df_temp_all_years=df_temp_all_years.rename(columns={df_temp_all_years.columns[0]:'Precipitation mm'})\n",
    "                        # rename the column\n",
    "                        break # stop the for loop with the number of columns, because values were found\n",
    "                        # go to line if df_temp_all_years.columns.nlevels!=1:\n",
    "                if df_temp_all_years.columns.nlevels!=1:\n",
    "                    # the dataframe still has two levels of columns, so the precedent if condition was never fullfilled\n",
    "                    print('The model '+model+' has no data')\n",
    "                    continue # try with the next model\n",
    "                else:\n",
    "                    # the dataframe still has one level of columns, there was one column not containing only NaN values\n",
    "                    for year in years:\n",
    "                        print(year)\n",
    "                        df_temp_one_year = df_temp_all_years.filter(like = str(year), axis=0) # select only data for one year\n",
    "                        #return df_temp_one_year\n",
    "                        df_temp_one_year_n_event=n_day_maximum_rainfall(number_day,df_temp_one_year) # use function to calculate cumulative precipitation\n",
    "                        #return df_temp_one_year_n_event\n",
    "                        # format time vector differently\n",
    "                        time = [df_temp_one_year_n_event.index.tolist()[i][0] for i in np.arange(0,len(df_temp_one_year_n_event.index.tolist()))]\n",
    "                        # fill dataframe\n",
    "                        df_temp_one_year_n_event = fill_dataframe((project,),(scenario,),(model,),time,df_temp_one_year_n_event.values,'Maximum '+str(number_day)+' days rainfall mm')\n",
    "                        df_n_day_event = pd.concat([df_n_day_event,df_temp_one_year_n_event])\n",
    "    return df_n_day_event # return a dataframe, with all the projects, scenarios, models and period of n day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_1_day_event(df):\n",
    "    df_copy = df.copy(deep=True)\n",
    "    df_max = df_copy.groupby(['Name project','Experiment','Model','Year']).max() # maximum\n",
    "    df_max=df_max.drop(labels='Date',axis=1)# drop columns Date\n",
    "    df_max=df_max.rename(columns={df_max.columns[0]:'Maximum 1 day rainfall mm '+str(df_max.index.levels[3][0])+'-'+str(df_max.index.levels[3][len(df_max.index.levels[3])-1])})\n",
    "    return df_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e354b6",
   "metadata": {},
   "source": [
    "### Seasonal average precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function a check\n",
    "def avg_dry_season_precipitation(df,title_column):\n",
    "    df_season = df.copy(deep=True)\n",
    "    df_season=df_season.rename(columns={df_season.columns[4]:title_column})\n",
    "    \n",
    "    Month = df_season[['Date']].values.reshape(len(df_season[['Date']].values),)\n",
    "    Season = df_season[['Date']].values.reshape(len(df_season[['Date']].values),)\n",
    "    for i in np.arange(0,len(df_season[['Date']].values)):\n",
    "        Month[i]=Month[i][3:5]\n",
    "        if int(Month[i])>3 and int(Month[i])<10:\n",
    "            Season[i]='Dry'\n",
    "        else:\n",
    "            Season[i]='Humid'\n",
    "\n",
    "    #df_season['Month'] = Month\n",
    "    df_season['Season'] = Season\n",
    "    df_season=df_season.drop(labels='Date',axis=1)\n",
    "    df_season=df_season.groupby(['Name project','Experiment','Model','Season','Year']).sum()\n",
    "    df_season=df_season.groupby(['Name project','Experiment','Model','Season']).mean()\n",
    "    df_season=df_season.groupby(['Name project','Season']).describe(percentiles=[.1, .5, .9])\n",
    "    pr_dry_season_mean_distribution=df_season.query('Season==\"Dry\"')\n",
    "    pr_dry_season_mean_distribution=pr_dry_season_mean_distribution.reset_index().drop('Season',axis=1).set_index('Name project')\n",
    "    return pr_dry_season_mean_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b648914",
   "metadata": {},
   "source": [
    "# Changes in indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse columns and rows for final df\n",
    "\n",
    "def changes_in_indicators(df_past,df_futur,title_indicator, unit,climate_var):\n",
    "    # create empty dataframe\n",
    "    #midx = pd.MultiIndex.from_product([df_years_avg_2041_2060_distribution.index.tolist(),precipitation_2021_2060_copy.index.levels[1].tolist(),models],names=['Name project','Experiment', 'Model'])\n",
    "    cols = pd.MultiIndex.from_product([(climate_var,),(title_indicator,),('Median for the past period '+unit,'Change in the median in %','10-th percentile for the past period '+unit, 'Change in 10-th percentile %','90-th percentile for the past period '+unit,'Change in 90-th percentile %')])\n",
    "    changes_past_future_indicator = pd.DataFrame(data = [], \n",
    "        index = df_past.index.tolist(),\n",
    "        columns = cols)\n",
    "    \n",
    "    changes_past_future_indicator[[changes_past_future_indicator.columns[0]]]=df_past[[df_past.columns[5]]]\n",
    "    \n",
    "    changes_past_future_indicator[[changes_past_future_indicator.columns[1]]]=(((df_futur[[df_futur.columns[5]]].values-df_past[[df_past.columns[5]]].values)/df_past[[df_past.columns[5]]].values)*100).reshape(len(df_past[[df_past.columns[5]]].values,),1)\n",
    "    \n",
    "    changes_past_future_indicator[[changes_past_future_indicator.columns[2]]]=df_past[[df_past.columns[4]]]\n",
    "    \n",
    "    changes_past_future_indicator[[changes_past_future_indicator.columns[3]]]=(((df_futur[[df_futur.columns[4]]].values-df_past[[df_past.columns[4]]].values)/df_past[[df_past.columns[4]]].values)*100).reshape(len(df_past[[df_past.columns[4]]].values,),1)\n",
    "    \n",
    "    changes_past_future_indicator[[changes_past_future_indicator.columns[4]]]=df_past[[df_past.columns[6]]]\n",
    "    \n",
    "    changes_past_future_indicator[[changes_past_future_indicator.columns[5]]]=(((df_futur[[df_futur.columns[6]]].values-df_past[[df_past.columns[6]]].values)/df_past[[df_past.columns[6]]].values)*100).reshape(len(df_past[[df_past.columns[6]]].values,),1)\n",
    "    \n",
    "    return changes_past_future_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb04ed1",
   "metadata": {},
   "source": [
    "# Level of exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462fe840",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inverse columns and rows for final df\n",
    "\n",
    "def level_exposure(df):\n",
    "    # level of exposure by climate variable\n",
    "    \n",
    "    # create empty dataframe\n",
    "    #midx = pd.MultiIndex.from_product([df_years_avg_2041_2060_distribution.index.tolist(),precipitation_2021_2060_copy.index.levels[1].tolist(),models],names=['Name project','Experiment', 'Model'])\n",
    "    cols = pd.MultiIndex.from_product([('Exposure level',),df.columns.levels[0].tolist()]) # df.columns.levels[0].tolist() is liste of climate variable\n",
    "    ExposureLevel = pd.DataFrame(data = [],\n",
    "                            index = df.index.tolist(),\n",
    "                            columns = cols)\n",
    "    \n",
    "    for name_p in ExposureLevel.index.tolist():\n",
    "        for climate_variable in df.columns.levels[0].tolist():\n",
    "            print('For project '+name_p+', climate variable '+climate_variable)\n",
    "            if ExposureLevel.loc[name_p,('Exposure level',climate_variable)] != 'High':\n",
    "                # for the moemnt, no other indicator for the climate variable made the exposure high (big changes with bug uncertainty)\n",
    "                \n",
    "                # select the columns of interest in the list of columns\n",
    "                col_interest_med= [cols for cols in df.columns.tolist() if climate_variable in cols and 'Change in the median in %' in cols]\n",
    "                col_interest_p10= [cols for cols in df.columns.tolist() if climate_variable in cols and 'Change in 10-th percentile %' in cols]\n",
    "                col_interest_p90= [cols for cols in df.columns.tolist() if climate_variable in cols and 'Change in 90-th percentile %' in cols]\n",
    "                \n",
    "                if ExposureLevel.loc[name_p,('Exposure level',climate_variable)] != 'Medium':\n",
    "                    if (df.loc[(name_p),col_interest_p10][abs(df.loc[(name_p),col_interest_p10])<20].notnull().values.any() or df.loc[(name_p),col_interest_p90][abs(df.loc[(name_p),col_interest_p90])<20].notnull().values.any()):\n",
    "                        # test if there are any True, if any value is under the threshold indicated\n",
    "                        ExposureLevel.loc[name_p,('Exposure level',climate_variable)] = 'No' # attribute value to exposure level\n",
    "\n",
    "                    if (df.loc[(name_p),col_interest_p10][abs(df.loc[(name_p),col_interest_p10])>20].notnull().values.any() or df.loc[(name_p),col_interest_p90][abs(df.loc[(name_p),col_interest_p90])>20].notnull().values.any()):\n",
    "                    # test if there are any True, if any value is over the threshold indicated\n",
    "                        ExposureLevel.loc[name_p,('Exposure level',climate_variable)] = 'Medium' # attribute value to exposure level\n",
    "\n",
    "\n",
    "                if (df.loc[(name_p),col_interest_med][abs(df.loc[(name_p),col_interest_med])>20].notnull().values.any()) or (df.loc[(name_p),col_interest_p10][abs(df.loc[(name_p),col_interest_p10])>50].notnull().values.any() or df.loc[(name_p),col_interest_p90][abs(df.loc[(name_p),col_interest_p90])>50].notnull().values.any()):\n",
    "                    # test if there are any True, if any value is over the threshold indicated\n",
    "                    ExposureLevel.loc[name_p,('Exposure level',climate_variable)] = 'High' # attribute value to exposure level\n",
    "    \n",
    "    ExposureLevel=ExposureLevel.style.apply(exposureColor) # apply color depending on value of Exposure\n",
    "    ExposureLevel=ExposureLevel.set_table_styles([{'selector': 'th.col_heading', 'props': 'text-align: left;'}],[{'selector': 'td', 'props': 'text-align: center;'}],overwrite = True) # place first level column to the left\n",
    "    # meant to place element in dataframe, but do not work very well\n",
    "    return ExposureLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function use in function 'level_exposure' to color result depending on the result\n",
    "def exposureColor(series):\n",
    "    green = 'background-color: lightgreen'\n",
    "    orange = 'background-color: orange'\n",
    "    red = 'background-color: red'\n",
    "    return [red if value == 'High' else orange if value == 'Medium' else green for value in series]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
