{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fb3dd0",
   "metadata": {},
   "source": [
    "<a id='beginning_CRVA'></a>\n",
    "# Climate Risk and Vulnerability Analysis\n",
    "This notebook helps you download and visualize climate data to perform Climate Risk and Vulnerability analyses\n",
    "\n",
    "## Usage\n",
    "this is how to use this script\n",
    "\n",
    "1. [Define user input](#user_input)\n",
    "2. [Run wanted climate variables](#climate_variables)\n",
    "    1. [Atmosphere's climate variables](#atmosphere_climate_variables)\n",
    "    2. [Land climate's variables](#land_climate_variables)\n",
    "    3. [Hydrosphere's climate variables](#hydrosphere_climate_variables)\n",
    "    4. [Second effect's climate variables](#second_effect_climate_variables)\n",
    "3. [Export data](#Export)\n",
    "\n",
    "## Input data\n",
    "\n",
    "* [Shapefile with project location(s) as points or polygon](#load_shapefile)\n",
    "* List of variables\n",
    "* List of SSP's\n",
    "* List of time horizons\n",
    "\n",
    "## [Climate variables](#climate_variables)\n",
    "### [Atmosphere's climate variables](#atmosphere_climate_variables)\n",
    "* [precipitation](#worldbank)\n",
    "* [temperature](#worldbank)\n",
    "* [Wind](#wind)\n",
    "* [Humidity](#humidity)\n",
    "* Solar radiation\n",
    "* Air quality deterioration\n",
    "### [Land's climate variables](#land_climate_variables)\n",
    "* [landslides](#landslides)\n",
    "* Coastal erosion\n",
    "* Soil erosion\n",
    "* Soil salinity\n",
    "### [Hydrosphere's climate variables](#hydrosphere_climate_variables)\n",
    "* Relative sea level rise\n",
    "* Seawater temperature\n",
    "* water availability\n",
    "* floods (fluvial)\n",
    "* floods (coastal)\n",
    "* forest fires\n",
    "### [Second effects of climate variables](#second_effect_climate_variables)\n",
    "* Storms\n",
    "* Dust storms\n",
    "* Wildfire\n",
    "* Urban heat island\n",
    "* Growinf season length\n",
    "* [cyclone risk](#cyclone_risk)\n",
    "\n",
    "\n",
    "\n",
    "* earthquakes (not climate related)\n",
    "\n",
    "## Climate impacts\n",
    "* calculate impact on yields\n",
    "\n",
    "## [Climate change information concerning the projects in the study area](#climate_change_info_for_each_project_in_study_area)\n",
    "\n",
    "\n",
    "## Contact\n",
    "\n",
    "rapy@cowi.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe4ed9",
   "metadata": {},
   "source": [
    "<a id='user_input'></a>\n",
    "## User inputs\n",
    "\n",
    "[Home](#beginning_CRVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f07a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User inputs\n",
    "import os.path\n",
    "\n",
    "\n",
    "#project locations (shapefile with location of elements to evaluate)\n",
    "data_folder=r'\\\\COWI.net\\projects\\A245000\\A248363\\CRVA\\Datasets'\n",
    "#data_folder=os.path.join(os.pardir,'dataset')\n",
    "project_location_path=os.path.join(data_folder,'Mozambique_PPIS/EIB_locations_few.shp')\n",
    "project_id='Name' #name of column used as id\n",
    "\n",
    "#study boundary (optional)\n",
    "study_area_path=os.path.join(data_folder,'Mozambique_PPIS/mozambique.shp')\n",
    "\n",
    "#output folder\n",
    "#out_path=r'\\\\COWI.net\\projects\\A245000\\A248363\\CRVA\\Scripts\\outputs'\n",
    "#out_path=r'C:\\Users\\CLMRX\\OneDrive - COWI\\Documents\\GitHub\\CRVA_tool\\outputs'\n",
    "out_path=r'C:\\Users\\CLMRX\\OneDrive - COWI\\Documents\\GitHub\\CRVA_tool\\outputs'\n",
    "\n",
    "#projection CRS (default = 'EPSG:4326')\n",
    "bCRS='EPSG:4326'\n",
    "\n",
    "#buffer for climate/grid variables\n",
    "buffer=40000 #buffer in meters, 0 = no buffer is computed\n",
    "\n",
    "mCRS='EPSG:31983' #metric CRS for buffer in meters (find relevant metric CRS for location!)\n",
    "\n",
    "\n",
    "#### Year of study\n",
    "#IDEA IS TO GIVE POSSIBILITY FOR THE USER TO CHOOSE DEPENDING on PROJECT (could choose on year or a period but precise period make more sense)\n",
    "#SET by default for the moment\n",
    "\n",
    "first_year = 2025 # start year included\n",
    "last_year = 2026 # last year included\n",
    "\n",
    "\n",
    "# For data coming from copernicus, historical data are commonly 1850-2005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ceae0",
   "metadata": {},
   "source": [
    "## Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd37531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import python packages\n",
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import rioxarray #used when calling ncdata.rio.write_crs\n",
    "import xarray as xr\n",
    "import os\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc#not directly used but needs to be imported for some nc4 files manipulations, use for nc files\n",
    "from netCDF4 import Dataset\n",
    "import csv #REMOVE ? not in use ?\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import shutil # to move folders\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore the warnings\n",
    "#import numpy as np\n",
    "#import io\n",
    "import cdsapi # for copernicus function\n",
    "import datetime # to have actual date\n",
    "\n",
    "# import functions and class defined in another file named FunctionsAndClass\n",
    "## Functions\n",
    "from FunctionsAndClass import copernicus_data\n",
    "from FunctionsAndClass import year_copernicus\n",
    "from FunctionsAndClass import dataframe_csv_copernicus\n",
    "\n",
    "## Class\n",
    "from FunctionsAndClass import time\n",
    "from FunctionsAndClass import copernicus_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907e131",
   "metadata": {},
   "source": [
    "<a id='load_shapefile'></a>\n",
    "## Load shapefiles and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load shapefiles\n",
    "projects = gpd.read_file(project_location_path).to_crs(bCRS)\n",
    "study_area = gpd.read_file(study_area_path).to_crs(bCRS)\n",
    "\n",
    "#calculate buffer around points/shape\n",
    "if buffer != 0:\n",
    "    projects_buf=projects.to_crs(mCRS)  #project to crs with metric units to get buffer in meters\n",
    "    projects_buf['geometry']=projects.to_crs(mCRS).buffer(buffer) #assign the buffer as the new geometry - \n",
    "    projects_buf=projects_buf.to_crs(bCRS)#project back to orginal crs\n",
    "\n",
    "    \n",
    "#plot shapefiles\n",
    "study_area.plot()\n",
    "projects.plot()\n",
    "projects_buf.plot() \n",
    "\n",
    "#show table\n",
    "projects[[project_id]]\n",
    "\n",
    "#### determination of the geographical zone of interest \n",
    "lat_min_wanted = min(study_area['LAT'])-10\n",
    "lat_max_wanted = max(study_area['LAT'])+10\n",
    "lon_min_wanted = min(study_area['LON'])-20\n",
    "lon_max_wanted = max(study_area['LON'])+20\n",
    "# addind and substracting to lon and lat to have margin\n",
    "# substracting more to longitude because the range of longitude is -180 to 180. The range of latitude is -90 to 90\n",
    "\n",
    "area = [lat_min_wanted, lon_min_wanted, lat_max_wanted,lon_max_wanted,] # used to download from copernicus\n",
    "\n",
    "### YEAR\n",
    "year = np.arange(first_year,(last_year+1),1) # create vector of years\n",
    "year_str = [0]*len(year) # create initiale empty vector to convert years in int\n",
    "index = np.arange(0,len(year)) # create vector of index for year\n",
    "i = 0 # initialize index\n",
    "for i in index: # convert all the date in string format\n",
    "    year_str[i]=str(year[i])\n",
    "\n",
    "start_date = \"01-01-\"+year_str[0] # string start date based on start year\n",
    "stop_date = \"31-12-\"+year_str[len(year)-1] # string stop date based on stop year\n",
    "dates = pd.date_range(start_date,stop_date) # vector of dates between start date and stop date\n",
    "index_dates = np.arange(0,len(dates)) # vector containning index o dates vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431210ff",
   "metadata": {},
   "source": [
    "## Import functions\n",
    "### read_cckp_ncdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1143921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def read cckp (world bank) nc files\n",
    "#reads data from world bank climate knowledge portal, nc files, with a single band\n",
    "#assigns projection and exports to tif since zonal_stats seems to have issues with it otherwise (not ideal solution)\n",
    "def read_cckp_ncdata(nc_path,output='tempfile.tif'):\n",
    "    with rioxarray.open_rasterio(nc_path,decode_times=False)[0] as ncdata:\n",
    "        ncdata.rio.write_crs('EPSG:4326', inplace=True)\n",
    "        ncdata=ncdata.isel(time=0)\n",
    "        ncdata.rio.to_raster(output)\n",
    "       # output=output #here\n",
    "   # else: \n",
    "      #  print(nc_path,\"not found\") # in this case, the data printed in the table will apply to the previous print.. \n",
    "       # output=0 #here\n",
    "    return output       \n",
    "\n",
    "#def read nc files (copernicus)\n",
    "#reads data from CMIP6 Copernicus, nc files\n",
    "#assigns projection and exports to tif since zonal_stats seems to have issues with it otherwise (not ideal solution)\n",
    "def read_nc_data(nc_path,stats,output='tempfile.tif'):\n",
    "    with rioxarray.open_rasterio(nc_path,decode_times=False)[3] as ncdata:\n",
    "        # calculate statistiques for each variable\n",
    "        if stats == 'mean':\n",
    "            ncdata=ncdata.mean(dim='time')\n",
    "        elif stats == 'median':\n",
    "            ncdata=ncdata.median(dim='time')\n",
    "        elif stats == 'p10':\n",
    "            ncdata=ncdata.quantile(0.1, dim='time')\n",
    "        elif stats == 'p90':\n",
    "            ncdata=ncdata.quantile(0.9, dim='time')\n",
    "        \n",
    "        ncdata.rio.write_crs('EPSG:4326', inplace=True)\n",
    "        ncdata.rio.to_raster(output)\n",
    "    return output       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2cb335",
   "metadata": {},
   "source": [
    "### get_cckp_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filename from cckp based on ssp, period and gcm\n",
    "def get_cckp_file_name(var,ssp='ssp245',period='2010-2039',gcm='median'):\n",
    "    if period in ['1991-2020']:\n",
    " #cru/era\n",
    "    #Precipitation   \n",
    "        if var in ['climatology-r50mm-annual-mean_era_annual','climatology-rx1day-monthly-mean_era_monthly','climatology-rx1day-annual-mean_era_annual','climatology-pr-annual-mean_era_annual','climatology-pr-monthly-mean_era_monthly']:\n",
    "            filename='precipitation/wb_cckp/climatology-rx5day-annual-mean_era_annual_era5-0.5x0.5-climatology_mean_1991-2020.nc'\n",
    "            filename=filename.replace('climatology-rx5day-annual-mean_era_annual',var)\n",
    "        elif var in ['climatology-pr-annual-mean_cru']:\n",
    "            filename='precipitation/wb_cckp/climatology-pr-annual-mean_cru_annual_cru-ts4.06-climatology_mean_1991-2020.nc'\n",
    "    #Temperature\n",
    "        elif var in ['climatology-tasmax-annual-mean_era','climatology-hd35-annual-mean_era','climatology-tas-annual-mean_era','climatology-hd40-annual-mean_era']:\n",
    "            filename='temperature/wb_cckp/climatology-tasmax-annual-mean_era_annual_era5-0.5x0.5-climatology_mean_1991-2020.nc'\n",
    "            filename=filename.replace('climatology-tasmax-annual-mean_era',var)                                                                                                                                 \n",
    "        elif var in ['climatology-tasmax-annual-mean_cru']: \n",
    "            filename='temperature/wb_cckp/climatology-tasmax-annual-mean_cru_annual_cru-ts4.06-climatology_mean_1991-2020.nc' \n",
    " #Realtime             \n",
    "    elif period not in ['1991-2020']:\n",
    "    #Precipitation     \n",
    "        if var in ['frp100yr-rx1day-period-mean_cmip6_period','climatology-rx1day-annual-mean_cmip6_annual','frp50yr-rx1day-period-mean_cmip6_period','climatology-pr-monthly-mean_cmip6_monthly','climatology-pr-annual-mean_cmip6_annual','climatology-pr-seasonal-mean_cmip6_seasonal','changefactorfaep100yr-rx1day-period-mean_cmip6_period','anomaly-pr-monthly-mean_cmip6_monthly','climatology-rx5day-annual-mean_cmip6_annual']: \n",
    "            filename='precipitation/wb_cckp/frp100yr-rx1day-period-mean_cmip6_period_all-regridded-bct-ssp245-climatology_median_2010-2039.nc'   \n",
    "            filename=filename.replace('2010-2039',period)\n",
    "            filename=filename.replace('frp100yr-rx1day-period-mean_cmip6_period',var)                      \n",
    "    #Temperature\n",
    "        elif var in ['climatology-hd40','anomaly-hd40','anomaly-hd35','anomaly-tasmax','anomaly-txx','climatology-txx','anomaly-tas','climatology-tas']: \n",
    "            filename='temperature/wb_cckp/climatology-hd40-annual-mean_cmip6_annual_all-regridded-bct-ssp245-climatology_median_2020-2039.nc'\n",
    "            filename=filename.replace('2020-2039',period)    \n",
    "            filename=filename.replace('climatology-hd40',var)\n",
    "        filename=filename.replace('ssp245',ssp)\n",
    "        filename=filename.replace('median',gcm)\n",
    "    data_path=os.path.join(data_folder,filename)\n",
    "    return data_path\n",
    "#import data from copernicus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b184d0e",
   "metadata": {},
   "source": [
    "### Display Map function\n",
    "This function's aim is to display map of the data with the basemap package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b853c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum and minimum latitude wanted for the study area\n",
    "\n",
    "parallels = np.arange(-360,360,10) # make latitude lines ever 10 degrees\n",
    "meridians = np.arange(-360,360,10) # make longitude lines every 10 degrees\n",
    "\n",
    "# create function to display a map\n",
    "# example for title_figure = 'sea_level_rise.png'\n",
    "# title_to_adapt='Projection of median sea level values near\\nMozambique for '+str(year)\n",
    "def Display_map(indexes_lat,indexes_lon,lat,lon,lat_min_wanted,lat_max_wanted,lon_min_wanted,lon_max_wanted,data,title_png,title_to_adapt,label,parallels,meridians):#,projects):\n",
    "\n",
    "    lon_moz, lat_moz = np.meshgrid(lon, lat) # this is necessary to have a map\n",
    "    \n",
    "    # create Map for Mozambique coast\n",
    "    fig = plt.figure()\n",
    "    plt.title(title_to_adapt) # title of the map # automatized with year\n",
    "    map = Basemap(projection ='merc',llcrnrlon=lon_min_wanted+5,llcrnrlat=lat_min_wanted+2,urcrnrlon=lon_max_wanted-5,urcrnrlat=lat_max_wanted-2,resolution='i', epsg = 4326) # projection, lat/lon extents an\n",
    "    # adding and substracting a quantity to the lon and lat to have a bit of margin when presenting it\n",
    "    # substracting more to longitude because the range of longitude is -180 to 180. The range of latitude is -90 to 90\n",
    "    map.drawcountries()\n",
    "    map.drawcoastlines()\n",
    "    map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "    map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "\n",
    "    temp = map.contourf(lon_moz,lat_moz,data)\n",
    "    #projects.plot(ax=ax) # project in projection EPSG:4326\n",
    "    cb = map.colorbar(temp,\"right\", size=\"5%\", pad=\"2%\") # color scale, second parameter can be locationNone or {'left', 'right', 'top', 'bottom'}\n",
    "    cb.set_label(label) # name for color scale\n",
    "    plt.savefig(os.path.join(out_path,'figures',title_png),format ='png') # savefig or save text must be before plt.show. for savefig, format should be explicity written\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84709b2",
   "metadata": {},
   "source": [
    "<a id='climate_variables'></a>\n",
    "# Climate variables\n",
    "\n",
    "[Home](#beginning_CRVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded3e5a",
   "metadata": {},
   "source": [
    "<a id='atmosphere_climate_variables'></a>\n",
    "# ATMOSPHERE\n",
    "\n",
    "[Home](#beginning_CRVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df87640",
   "metadata": {},
   "source": [
    "<a id='worldbank'></a>\n",
    "## World Bank Climate knowledge portal, Precipitation and Temperature - Atmosphere\n",
    "data source: https://climateknowledgeportal.worldbank.org/download-data\n",
    "\n",
    "List of different variables:\n",
    "\n",
    "### Precipitation \n",
    "* return period of current 50 and 100 year event\n",
    "* yearly average precipitation\n",
    "* seasonal average precipitation - OR MONTH? i word documnet: average dry season precipitation \n",
    "* maximum precipitation in one day - NOT REALLY, instead \"Average of the largest daily precipitation amount\" rx1day \n",
    "* maximum precipitation in five days - NOT REALLY, instead \"Average of the largest 5-day consecutive precipitation amount\".\n",
    "* FORSLAG: Could also include: precipitation amount from very wet days/ days >50mm. eg. \n",
    "\n",
    "### Temperature\n",
    "* Yearly mean temperature (average) - tas \n",
    "* Number of hot days >40 degrees C (average)\n",
    "* Number of hot days >35 degrees C (average) \n",
    "* Maximum of daily max-temperature (per year) -txx\n",
    "* Average of maximum temperature (daily??) - tasmax \n",
    "\n",
    "### How to use\n",
    "define \n",
    "\n",
    "`variables = {variablename1:{'periods':['2020-2039'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']}, variablename2:...}`\n",
    "\n",
    "where variablename corresponds to a variable name in the cckp, periods include the desired periods, ssps the desired ssp-rcp scenarios, and gcms can be either median, p10, or p90 (50%, 10%, and 90% percentile of ensemble models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b0eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read downloaded precipitation and temperature data\n",
    "#settings\n",
    "\n",
    "variables= {\n",
    "    #Temperature\n",
    "        #era/cru\n",
    "            #'climatology-hd35-annual-mean_era':{'periods':['1991-2020']},\n",
    "            'climatology-hd40-annual-mean_era':{'periods':['1991-2020']},\n",
    "            'climatology-tasmax-annual-mean_era':{'periods':['1991-2020']}, \n",
    "            'climatology-tas-annual-mean_era':{'periods':['1991-2020']},\n",
    "            'climatology-tasmax-annual-mean_cru':{'periods':['1991-2020']}, \n",
    "        #realtime\n",
    "            'climatology-hd40':{'periods':['2020-2039'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            'anomaly-hd40':{'periods':['2020-2039'],'ssps':['ssp245','ssp370'],'gcms':['median']},\n",
    "            #'anomaly-hd35':{'periods':['2020-2039'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            #'anomaly-tasmax':{'periods':['2020-2039','2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']}, \n",
    "            'anomaly-txx':{'periods':['2020-2039','2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']}, #txx=maximum of daily max temperature\n",
    "            'climatology-txx':{'periods':['2020-2039','2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            'anomaly-tas':{'periods':['2020-2039'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            'climatology-tas':{'periods':['2020-2039','2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']}, #data not downloaded \n",
    "    #Precipitation\n",
    "       #era/cru\n",
    "            #'climatology-r50mm-annual-mean_era_annual':{'periods':['1991-2020']},\n",
    "            'climatology-rx1day-monthly-mean_era_monthly':{'periods':['1991-2020']},\n",
    "            'climatology-rx1day-annual-mean_era_annual':{'periods':['1991-2020']},\n",
    "            'climatology-pr-annual-mean_era_annual':{'periods':['1991-2020']},\n",
    "            'climatology-pr-monthly-mean_era_monthly':{'periods':['1991-2020']},\n",
    "            'climatology-pr-annual-mean_cru':{'periods':['1991-2020']},\n",
    "        #realtime\n",
    "            'frp100yr-rx1day-period-mean_cmip6_period':{'periods':['2010-2039','2035-2064'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']}, \n",
    "            'frp50yr-rx1day-period-mean_cmip6_period':{'periods':['2010-2039'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            'climatology-rx5day-annual-mean_cmip6_annual':{'periods':['2020-2039','2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            'climatology-rx1day-annual-mean_cmip6_annual':{'periods':['2020-2039','2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            'climatology-pr-monthly-mean_cmip6_monthly':{'periods':['2020-2039','2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            'climatology-pr-annual-mean_cmip6_annual':{'periods':['2020-2039','2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            'climatology-pr-seasonal-mean_cmip6_seasonal':{'periods':['2020-2039','2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            'changefactorfaep100yr-rx1day-period-mean_cmip6_period':{'periods':['2010-2039'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']},\n",
    "            #'anomaly-pr-monthly-mean_cmip6_monthly':{'periods':['2040-2059'],'ssps':['ssp245','ssp370'],'gcms':['median','p10','p90']}\n",
    "            }\n",
    "       ## 'timeseries-rx1day':{'periods':['2015-2100'],'ssps':['ssp245'],'gcms':['median']} # Den kan ikke læse den - brokker sig over Dimension time=0. \n",
    "       \n",
    "#Create multi index for cckp_output\n",
    "idx=pd.IndexSlice\n",
    "mindex=[]\n",
    "for var in variables.keys():\n",
    "    for period in variables[var]['periods']:\n",
    "        if period in ['1991-2020']:\n",
    "            mindex.append((var,period,None,None))\n",
    "        else:\n",
    "            for ssp in variables[var]['ssps']:\n",
    "                    for gcm in variables[var]['gcms']:\n",
    "                        mindex.append((var,period,ssp,gcm))\n",
    "mindex=pd.MultiIndex.from_tuples(mindex, names=['variable', 'period','ssp','gcm'])\n",
    "#cckp_output=pd.DataFrame(index=projects[project_id],columns=mindex)\n",
    "cckp_output=pd.DataFrame(index=mindex,columns=projects[project_id])\n",
    "\n",
    "#read data from cckp and spatial statistics\n",
    "for var in variables.keys():\n",
    "    for period in variables[var]['periods']:\n",
    "        if period in ['1991-2020']: # for era/cru data\n",
    "            varname='_'.join((var,period))\n",
    "            data_path=get_cckp_file_name(var,period=period)\n",
    "            if os.path.exists(data_path):\n",
    "                data=read_cckp_ncdata(data_path)\n",
    "                stats=pd.DataFrame(zonal_stats(vectors=projects_buf, raster=data,all_touched=True, stats='mean'))\n",
    "                projects[varname]=stats\n",
    "                #cckp_output.loc[:,idx[var,period,:,:]]=stats.values\n",
    "                cckp_output.loc[idx[var,period,:,:],:]=stats.T.values\n",
    "            else:\n",
    "                print(data_path,\"not found\")\n",
    "                projects[varname]='not found'\n",
    "        elif period not in ['1991-2020']: #for realtime data \n",
    "                for ssp in variables[var]['ssps']:\n",
    "                    for gcm in variables[var]['gcms']:\n",
    "                        varname='_'.join((var,period,ssp,gcm))\n",
    "                        data_path=get_cckp_file_name(var,ssp=ssp,period=period,gcm=gcm)\n",
    "                        if os.path.exists(data_path):\n",
    "                            data=read_cckp_ncdata(data_path)\n",
    "                            stats=pd.DataFrame(zonal_stats(vectors=projects_buf, raster=data,all_touched=True, stats='mean'))\n",
    "                            projects[varname]=stats\n",
    "                            #cckp_output.loc[:,idx[var,period,ssp,gcm]]=stats.values\n",
    "                            cckp_output.loc[idx[var,period,ssp,gcm],:]=stats.T.values\n",
    "                        else:\n",
    "                            print(data_path,\"not found\")\n",
    "                            projects[varname]='not found'\n",
    "                           \n",
    "\n",
    "#plot, show #ADD enlarge the clip + something wierd with nodata value\n",
    "cckp_output.to_csv('outputs/csv/cckp_output.csv')\n",
    "projects[[project_id,varname]].head()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "precip = rioxarray.open_rasterio(data)\n",
    "precip_clip = precip.rio.clip(study_area.geometry, study_area.crs, drop=True)\n",
    "precip_clip.plot(ax=ax)\n",
    "projects.plot(ax=ax)\n",
    "\n",
    "#close\n",
    "precip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84cafc",
   "metadata": {},
   "source": [
    "<a id='wind'></a>\n",
    "## Wind Speed\n",
    "\n",
    "Daily wind speed, historical and projected.\n",
    "\n",
    "Data source : https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extraction of Near-surface wind speed data\n",
    "global_variable = 'Wind'\n",
    "name_variable = 'near_surface_wind_speed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(year_historical, year_str_historical, index_historical, dates_historical, index_dates_historical)=year_copernicus(1850,1851)#,2020)\n",
    "temporal_resolution = 'daily'\n",
    "\n",
    "Wind = dataframe_csv_copernicus(temporal_resolution,year_str_historical,copernicus_elements.experiments_historical,copernicus_elements.models,out_path,global_variable, name_variable,area,index_dates_historical,dates_historical)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a67eb2",
   "metadata": {},
   "source": [
    "### Historical data for wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d0681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1850 to 2020\n",
    "\n",
    "(year_historical, year_str_historical, index_historical, dates_historical, index_dates_historical)=year_copernicus(1850,2020)\n",
    "\n",
    "SSP = 'historical'\n",
    "for model_simulation in copernicus_elements.models:\n",
    "    model =(model_simulation,) # create tuple for iteration of dataframe\n",
    "    print(model)\n",
    "    # path were the futur downloaded file is registered\n",
    "    path_for_file= os.path.join(out_path,'Datasets', global_variable, name_variable, SSP, model_simulation,period)\n",
    "    # existence of path_for_file tested in copernicus function\n",
    "    wind_path=copernicus_data(temporal_resolution,SSP,name_variable,model_simulation,year_str_historical,area,path_for_file,out_path)\n",
    "    # area is determined in the \"Load shapefiles and plot\" part\n",
    "    if (wind_path is not None):\n",
    "        Open_path = Dataset(wind_path) # open netcdf file\n",
    "        lat_dataframe = np.ma.getdata(Open_path.variables['lat']).data\n",
    "        lon_dataframe = np.ma.getdata(Open_path.variables['lon']).data\n",
    "        data_with_all = ma.getdata(Open_path.variables['sfcWind']).data\n",
    "            \n",
    "        for day in index_dates_historical:\n",
    "            print('FINAAAAL')\n",
    "            data_dataframe = data_with_all[day,:,:]\n",
    "            time = (dates_historical[day],) # create tuple for iteration of dataframe\n",
    "            # Create the MultiIndex\n",
    "            midx = pd.MultiIndex.from_product([experiment, model, time, lat_dataframe],names=['Experiment', 'Model', 'Date', 'Latitude'])\n",
    "            # multiindex to name the columns\n",
    "            lon_str = ('Longitude',)\n",
    "            cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "            # Create the Dataframe\n",
    "            Variable_dataframe = pd.DataFrame(data = data_dataframe, \n",
    "                                        index = midx,\n",
    "                                        columns = cols)\n",
    "            # Concatenate former and new dataframe\n",
    "            df = pd.concat([df,Variable_dataframe])\n",
    "                \n",
    "            # register information for project\n",
    "                \n",
    "                \n",
    "        Open_path.close # to spare memory\n",
    "    else:\n",
    "        print(\"Path does not exist\")\n",
    "        pass\n",
    "\n",
    "path_for_csv=os.path.join('outputs','csv',name_variable)    \n",
    "df.to_csv(path_for_csv)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f51f42",
   "metadata": {},
   "source": [
    "### Average wind speed\n",
    "#### Extracting wind datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the copernicus_data function\n",
    "temporal_resolution = 'daily'\n",
    "# create string for name of folder depending on type of period\n",
    "if temporal_resolution == 'fixed':\n",
    "    period = 'fixed'\n",
    "else:\n",
    "    period=year_str[0]+'-'+year_str[len(year_str)-1]\n",
    "\n",
    "df = pd.DataFrame() # create an empty dataframe\n",
    "data_projects = []\n",
    "\n",
    "for SSP in copernicus_elements.experiments:\n",
    "    experiment = (SSP,) # create tuple for iteration of dataframe\n",
    "    print(SSP)\n",
    "    for model_simulation in copernicus_elements.models:\n",
    "        model =(model_simulation,) # create tuple for iteration of dataframe\n",
    "        print(model)\n",
    "        # path were the futur downloaded file is registered\n",
    "        path_for_file= os.path.join(out_path,'Datasets', global_variable, name_variable, SSP, model_simulation,period)#,'')\n",
    "        # existence of path_for_file tested in copernicus function\n",
    "        wind_path=copernicus_data(temporal_resolution,SSP,name_variable,model_simulation,year_str,area,path_for_file,out_path)\n",
    "        # area is determined in the \"Load shapefiles and plot\" part\n",
    "        if (wind_path is not None):\n",
    "            Open_path = Dataset(wind_path) # open netcdf file\n",
    "            lat_dataframe = np.ma.getdata(Open_path.variables['lat']).data\n",
    "            lon_dataframe = np.ma.getdata(Open_path.variables['lon']).data\n",
    "            data_with_all = ma.getdata(Open_path.variables['sfcWind']).data\n",
    "            \n",
    "            for day in index_dates:\n",
    "                print('FINAAAAL')\n",
    "                data_dataframe = data_with_all[day,:,:]\n",
    "                time = (dates[day],) # create tuple for iteration of dataframe\n",
    "                # Create the MultiIndex\n",
    "                midx = pd.MultiIndex.from_product([experiment, model, time, lat_dataframe],names=['Experiment', 'Model', 'Date', 'Latitude'])\n",
    "                # multiindex to name the columns\n",
    "                lon_str = ('Longitude',)\n",
    "                cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "                # Create the Dataframe\n",
    "                Variable_dataframe = pd.DataFrame(data = data_dataframe, \n",
    "                                            index = midx,\n",
    "                                            columns = cols)\n",
    "                # Concatenate former and new dataframe\n",
    "                df = pd.concat([df,Variable_dataframe])\n",
    "                \n",
    "                # register information for project\n",
    "                \n",
    "                \n",
    "            Open_path.close # to spare memory\n",
    "        else:\n",
    "            print(\"Path does not exist\")\n",
    "            pass\n",
    "\n",
    "path_for_csv=os.path.join('outputs','csv',name_variable)    \n",
    "df.to_csv(path_for_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1dd182",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistiques calculation\n",
    "\n",
    "# average wind speed accross the models\n",
    "# max wind speed\n",
    "\n",
    "df_reordered = df.reorder_levels(['Experiment', 'Latitude', 'Model', 'Date']) # reordering to ...\n",
    "df_mean=df_reordered.groupby(level=[0,1],axis=0,group_keys=True).mean() # .... calculate mean of average winf accross models\n",
    "# level =[0,1] to keep the 2 levels experiments and latitude\n",
    "# axis = 0 calculates statistics accross rows (1 is for columns)\n",
    "# group_keys is True to keep the original names of the indexes\n",
    "df_max=df_reordered.groupby(level=[0,1],axis=0,group_keys=True).max()\n",
    "df_min=df_reordered.groupby(level=[0,1],axis=0,group_keys=True).min()\n",
    "df_p10=df_reordered.groupby(level=[0,1],axis=0,group_keys=True).quantile(0.1)\n",
    "df_p90=df_reordered.groupby(level=[0,1],axis=0,group_keys=True).quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0efb289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reordered2 = df.reorder_levels(['Experiment', 'Model', 'Latitude', 'Date']) # reordering to ...\n",
    "df_reordered2_mean=df_reordered2.groupby(level=[0,1,2],axis=0,group_keys=True).mean()\n",
    "numpy_array = df_reordered2_mean.loc['ssp1_2_6','access_cm2'].values\n",
    "numpy_array=numpy_array.flatten() # make the array a vector to use it in boxplot function\n",
    "numpy_array = numpy_array[~np.isnan(numpy_array)]\n",
    "# boxplot do not deal with NaN, have to take them out\n",
    "dict_boxplot = plt.boxplot(numpy_array,notch=True, whis =(10,90),widths = 0.10, patch_artist=True,labels=('access_cm2',))#,labels = 'access_cm2')# ... present boxplot over the period for each models\n",
    "# this functions returns varius parameters of the boxplot in the dict_boxplot. This funcitons also returns an image of it\n",
    "# here, numpy_array is a vector. But can also include array with several columns. Each columns will have a boxplot\n",
    "# 'notch' is true to enhance part where the median is\n",
    "# 'whis' is the percentile value for the whiskers, every data out of the range indicted by those 2 floats are represented as points\n",
    "# 'widths' determine width of the boxes\n",
    "# 'patch_artist' colors the boxplots\n",
    "# 'labels' gives a name to every column included in the data part\n",
    "str = 'Distribution of the average wind speed for\\nthe concerned period and under scenario ssp1_2_6'\n",
    "plt.title(str)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Average wind speed m/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0aa069",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=pd.DataFrame(zonal_stats(vectors=projects_buf, raster=data,all_touched=True, stats='mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02559b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEST\n",
    "\n",
    "### Extraction of Near-surface wind speed data\n",
    "global_variable = 'Wind'\n",
    "name_variable = 'near_surface_wind_speed'\n",
    "\n",
    "# Parameters for the copernicus_data function\n",
    "temporal_resolution = 'daily'\n",
    "# create string for name of folder depending on type of period\n",
    "if temporal_resolution == 'fixed':\n",
    "    period = 'fixed'\n",
    "else:\n",
    "    period=year_str[0]+'-'+year_str[len(year_str)-1]\n",
    "\n",
    "df = pd.DataFrame() # create an empty dataframe\n",
    "\n",
    "SSP = experiments[1]\n",
    "model =models[1] # create tuple for iteration of dataframe\n",
    "print(model)\n",
    "# path were the futur downloaded file is registered\n",
    "path_for_file= os.path.join(out_path,'Datasets', global_variable, name_variable, SSP, model,period)#,'')\n",
    "# existence of path_for_file tested in copernicus function\n",
    "wind_path=copernicus_data(temporal_resolution,SSP,name_variable,model,year_str,area,path_for_file)\n",
    "# area is determined in the \"Load shapefiles and plot\" part\n",
    "if (wind_path is not None):\n",
    "    Open_path = Dataset(wind_path) # open netcdf file\n",
    "    lat_dataframe = np.ma.getdata(Open_path.variables['lat']).data\n",
    "    lon_dataframe = np.ma.getdata(Open_path.variables['lon']).data\n",
    "    data_with_all = ma.getdata(Open_path.variables['sfcWind']).data\n",
    "\n",
    "    SSP =(SSP,)\n",
    "    model =(model,)\n",
    "    for day in index_dates:\n",
    "        print('FINAAAAL')\n",
    "        data_dataframe = data_with_all[day,:,:]\n",
    "        time = (dates[day],) # create tuple for iteration of dataframe\n",
    "        # Create the MultiIndex for the rows\n",
    "        midx = pd.MultiIndex.from_product([SSP, model, time, lat_dataframe],names=['Experiment', 'Model', 'Date', 'Latitude'])\n",
    "        # multiindex to name the columns\n",
    "        lon_str = ('Longitude',)\n",
    "        cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "        # Create the Dataframe\n",
    "        Variable_dataframe = pd.DataFrame(data = data_dataframe, \n",
    "                                            index = midx,\n",
    "                                            columns = cols)\n",
    "                        # Concatenate former and new dataframe\n",
    "        df = pd.concat([df,Variable_dataframe])\n",
    "\n",
    "        Open_path.close # to spare memory\n",
    "else:\n",
    "    print(\"Path does not exist\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb834214",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST\n",
    "\n",
    "### register data and statistiques\n",
    "\n",
    "# register netCDF file \n",
    "Wind= Dataset(wind_path)\n",
    "near_surface_wind_speed = Wind.variables['sfcWind']\n",
    "Wind.close\n",
    "\n",
    "# get access to the masked data in the netCDF file\n",
    "near_surface_wind_speed = ma.getdata(near_surface_wind_speed)\n",
    "near_surface_wind_speed = near_surface_wind_speed.data\n",
    "\n",
    "# calculate statistiques over the whole study are for the selected period of the data\n",
    "data_wind_avg = np.mean(near_surface_wind_speed, axis=0)# compute average\n",
    "data_wind_med = np.median(near_surface_wind_speed, axis=0)# compute median\n",
    "data_wind_max = np.max(near_surface_wind_speed, axis=0)# compute max\n",
    "data_wind_p10 = np.percentile(near_surface_wind_speed,10, axis=0)# compute p10\n",
    "data_wind_p90 = np.percentile(near_surface_wind_speed,90, axis=0)# compute p90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a3fc6",
   "metadata": {},
   "source": [
    "#### Registering informations concerning wind speed for projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculation of statistiques concerning the daily wind around each project\n",
    "\n",
    "# list of statistiques to perform\n",
    "stats=['mean','median','p10','p90']\n",
    "\n",
    "# for loop register each operation listed before in the projects DataFrame\n",
    "for i in range(len(stats)):\n",
    "    data=read_nc_data(wind_path,stats[i])# converting nc_data to a rater file, type .tif\n",
    "     # create a string to name automatically the column in projects, depend on how much year are asked\n",
    "    if len(year_str)==1:\n",
    "        name = temporal_resolution+'-'+stats[i]+'-'+global_variable+'-'+year_str+'-'+SSP\n",
    "    else:\n",
    "        name = temporal_resolution+'-'+stats[i]+'-'+global_variable+'-'+period+'-'+SSP\n",
    "    \n",
    "    # calculation of statistiques concerning the daily wind around each project\n",
    "    projects[name]=pd.DataFrame(zonal_stats(vectors=projects_buf, raster=data,all_touched=True, stats='mean'))# register the value in the dataFrame projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880719e",
   "metadata": {},
   "source": [
    "#### Mapping average wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7aaccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display Map\n",
    "\n",
    "# register the unit of the variable printed\n",
    "unit=Wind.variables['sfcWind'].__dict__['units']\n",
    "\n",
    "# register and unmask lat and lon\n",
    "lat = np.ma.getdata(Wind.variables['lat']).data\n",
    "lon = np.ma.getdata(Wind.variables['lon']).data\n",
    "\n",
    "# create index for lat and lon\n",
    "indexes_lat = np.arange(0,len(lat)-1)\n",
    "indexes_lon = np.arange(0,len(lat)-1)\n",
    "\n",
    "# parameters for map\n",
    "if len(year_str)==1:\n",
    "    title_to_adapt='Average '+temporal_resolution+' wind for '+year_str[0]+' according to '+SSP\n",
    "else:\n",
    "    title_to_adapt='Average '+temporal_resolution+' wind for the period '+str(first_year)+'-'+str(last_year)+' according to '+SSP\n",
    "label= 'Average '+temporal_resolution+' wind ('+unit+')'\n",
    "title_png='Mean_near_surface_wind_speed.png'\n",
    "\n",
    "# function to display the map\n",
    "Display_map(indexes_lat,indexes_lon,lat,lon,lat_min_wanted,lat_max_wanted,lon_min_wanted,lon_max_wanted,data_wind_avg,title_png,title_to_adapt,label,parallels,meridians)#,projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd546c41",
   "metadata": {},
   "source": [
    "### Maximum wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc287803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for map\n",
    "if len(year_str)==1:\n",
    "    title_to_adapt='Maximum '+temporal_resolution+' wind for '+year_str[0]+' according to '+SSP\n",
    "else:\n",
    "    title_to_adapt='Maximum '+temporal_resolution+' wind for the period '+str(first_year)+'-'+str(last_year)+' according to '+SSP\n",
    "label= 'Average '+temporal_resolution+' wind ('+unit+')'\n",
    "title_png='Mean_near_surface_wind_speed.png'\n",
    "label= 'Maximum '+temporal_resolution+' wind ('+unit+')'\n",
    "title_png='Max_near_surface_wind_speed.png'\n",
    "\n",
    "# function to display the map\n",
    "Display_map(indexes_lat,indexes_lon,lat,lon,lat_min_wanted,lat_max_wanted,lon_min_wanted,lon_max_wanted,data_wind_max,title_png,title_to_adapt,label,parallels,meridians)#,projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b74e8a",
   "metadata": {},
   "source": [
    "## Humidity\n",
    "\n",
    "Source : https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=form\n",
    "\n",
    "### Relative humidity\n",
    "\n",
    "Relative humidity of air: amount if moisture it contains compared to the maximum amount of moisture it can have at a specific temperature\n",
    "\n",
    "### Extraction of Relative humidity data\n",
    "##### User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the copernicus_data function\n",
    "temporal_resolution = 'monthly' \n",
    "\n",
    "if temporal_resolution == 'fixed':\n",
    "    period = 'fixed'\n",
    "else:\n",
    "    period=year_str[0]+'-'+year_str[len(year_str)-1]\n",
    "\n",
    "#SSP='ssp5_8_5'\n",
    "#model='awi_cm_1_1_mr'#'cams_csm1_0' test for some data is not matching\n",
    "global_variable = 'Relative_Humidity'\n",
    "name_variable = 'near_surface_relative_humidity' # only available on a monthly basis\n",
    "\n",
    "for j in index_experiments:\n",
    "    for i in index_models:\n",
    "        # path were the futur downloaded file is registered\n",
    "        path_for_file= os.path.join(out_path,'Datasets', global_variable, name_variable, experiments[j], models[i],period)#,'')\n",
    "        # existence of path_for_file tested in copernicus function\n",
    "        humidity_path=copernicus_data(temporal_resolution,experiments[j],name_variable,models[i],year_str,area,path_for_file)\n",
    "        # area is determined in the \"Load shapefiles and plot\" part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a194e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pd \n",
    "dates = pd.date_range(\"01-01-2023\",\"31-12-2023\")\n",
    "for day in dates:\n",
    "    print(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788310cb",
   "metadata": {},
   "source": [
    "<a id='land_climate_variables'></a>\n",
    "# LAND\n",
    "\n",
    "[Home](#beginning_CRVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338692f1",
   "metadata": {},
   "source": [
    "<a id='landslides'></a>\n",
    "## Landslides - Land\n",
    "\n",
    "source: https://datacatalog.worldbank.org/search/dataset/0037584 (Global Landslide Hazard Map, The World Bank)\n",
    "\n",
    "### Description of LS_TH : \n",
    "The Global Landslide Hazard Map presents a qualitative representation of global landslide hazard at a global scale. It is the combination of the The Global Landslide Hazard Map: Median Annual Rainfall-Triggered Landslide Hazard (1980-2018) and The Global Landslide Hazard Map: Earthquake-Triggered Landslide Hazard which has then been simplified to four rank categories, ranging from Very low to High landslide hazard, based on the existing system used by ThinkHazard! (www.thinkhazard.org)\n",
    "\n",
    "### Description of LS_RF_Median_1980-2018 :\n",
    "The Global Rainfall-Triggered Landslide Hazard Map presents a quantitative representation of landslide hazard. This component is the median annual rainfall-triggered landslide hazard assessment for the period 1980 – 2018. Raster values represent the modelled average annual frequency of significant rainfall-triggered landslides per sq. km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Landslides data path\n",
    "landslidesTH_path=os.path.join(data_folder,'landslides/wb_GlLandslHazard/ls_th/LS_TH.tif') ## Global landslide hazard map (TH ranks --> Think hazard ranks)\n",
    "landslidesRF_path=os.path.join(data_folder,'landslides/wb_GlLandslHazard/ls_rf_median_1980-2018/LS_RF_Median_1980-2018.tif')\n",
    "\n",
    "#spatial stats\n",
    "projects['landslides_THmax']=pd.DataFrame(zonal_stats(vectors=projects_buf, raster=landslidesTH_path, \n",
    "                                                            all_touched=True, stats='max'))\n",
    "projects['landslidesRF_medianMax']=pd.DataFrame(zonal_stats(vectors=projects_buf, raster=landslidesRF_path, \n",
    "                                                            all_touched=True, stats='max'))\n",
    "#show table\n",
    "projects[[project_id,'landslides_THmax','landslidesRF_medianMax']].head()\n",
    "\n",
    "#plot landslides TH\n",
    "fig, ax = plt.subplots()\n",
    "landslides = rioxarray.open_rasterio(landslidesTH_path)\n",
    "landslides_clip = landslides.rio.clip(study_area.geometry, study_area.crs, drop=True)\n",
    "landslides_clip.plot(ax=ax, cmap='Reds',vmin=1, vmax=4)\n",
    "points=projects.plot(ax=ax,cmap='Greys') # what are those points ????\n",
    "\n",
    "#close\n",
    "landslides.close()\n",
    "landslides_clip.close()\n",
    "\n",
    "# didn't achieved to present the RF datas\n",
    "#plot landslides RF\n",
    "#fig, ax = plt.subplots()\n",
    "#landslides = rioxarray.open_rasterio(landslidesRF_path)\n",
    "#landslides_clip = landslides.rio.clip(study_area.geometry, study_area.crs, drop=True)\n",
    "#landslides_clip.plot(ax=ax, cmap='Reds',vmin=1, vmax=4)\n",
    "#points=projects.plot(ax=ax,cmap='Greys') # what are those points ????\n",
    "#landslides.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b840dac",
   "metadata": {},
   "source": [
    "## Global wildfire hazard - Land\n",
    "\n",
    "This jupyter notebook is to study the global wildfire hazard\n",
    "\n",
    "Source : https://datacatalog.worldbank.org/search/dataset/0042058/Global-wildfire-hazard\n",
    "\n",
    "Description of the source : The approach to classify wildfire hazard levels used is based solely on fire weather index climatology. Fire weather indices are used in many countries to assess both the onset of conditions that will allow fires to spread, as well as the likelihood of fire at any point in the landscape. The method presented uses statistical modelling (extreme value analysis) of a 30 year fire weather climatology to assess the predicted fire weather intensity for specific return period intervals. These intensities are classified based on thresholds using conventions to provide hazard classes that correspond to conditions that can support problematic fire spread in the landscape if an ignition and sufficient fuel were to be present.\n",
    "\n",
    "Date of export : 23.03.23\n",
    "\n",
    "Projection FWI for europe : https://cds.climate.copernicus.eu/cdsapp#!/dataset/sis-tourism-fire-danger-indicators?tab=overview\n",
    "\n",
    "Projection seasonal severity rating of for EUrope : https://www.eea.europa.eu/data-and-maps/figures/projected-meteorological-forest-fire-danger\n",
    "\n",
    "Calculation of the FWI : https://cwfis.cfs.nrcan.gc.ca/background/summary/fwi\n",
    "\n",
    "GLOBAL PROJeTIONS NOT FOUND\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1bf95",
   "metadata": {},
   "source": [
    "## Soil erosion - Land\n",
    "\n",
    "Source for global actual situations : https://data.jrc.ec.europa.eu/dataset/jrc-esdac-120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064cf21",
   "metadata": {},
   "source": [
    "<a id='hydrosphere_climate_variables'></a>\n",
    "# HYDROSPHERE\n",
    "\n",
    "[Home](#beginning_CRVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6564dce",
   "metadata": {},
   "source": [
    "<a id='floods'></a>\n",
    "## Floods - Hydrosphere\n",
    "\n",
    "####  Fluvial\n",
    "Sources:\n",
    "1) JRC Flood Hazard Map, http://data.europa.eu/89h/jrc-floods-floodmapgl_rp100y-tif : The map depicts flood prone areas at global scale for flood events with 100-year return period. Resolution is 30 arcseconds (approx. 1km). Cell values indicate water depth (in m). The map can be used to assess flood exposure and risk of population and assets. NOTE: this dataset is based on JRC elaborations and is not an official flood hazard map (for details and limitations please refer to related publications).\n",
    "2) Dataset of tropical cyclone Idai and subsequent flood disaster in Southern Africa (March 2019). National Tibetan Plateau Data Center, https://data.tpdc.ac.cn/en/data/8d836d13-a8e6-492b-9324-00fbfce40619/ \n",
    "\n",
    "#### Coastal \n",
    "Source: WRI Aqueduct database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the path for the image presenting the floods which have a return period of a 100 year\n",
    "flood_path=os.path.join(data_folder,'floodMapGL_rp100y/floodMapGL_rp100y.tif')\n",
    "\n",
    "buffer_flood=1000#meters # pas sure : sert a avoir les \n",
    "if buffer_flood != 0:\n",
    "    projects_bufFlood=projects.to_crs(mCRS)  #project to crs with metric units to get buffer in meters\n",
    "    projects_bufFlood['geometry']=projects.to_crs(mCRS).buffer(buffer_flood) #assign the buffer as the new geometry - \n",
    "    projects_bufFlood=projects_bufFlood.to_crs(bCRS)#project back to orginal crs\n",
    "\n",
    "projects['floods100yr_max']=pd.DataFrame(zonal_stats(vectors=projects_bufFlood, raster=flood_path, \n",
    "                                                            all_touched=True, stats='max'))\n",
    "#plot\n",
    "fig, ax = plt.subplots()\n",
    "floods = rioxarray.open_rasterio(flood_path)\n",
    "floods_clip = floods.rio.clip(study_area.geometry, study_area.crs, drop=True)\n",
    "floods_clip.plot(ax=ax,cmap='Blues',vmin=0,vmax=25)\n",
    "projects.plot(ax=ax,color='Black') # mapping the projects located in the study area\n",
    "#cb.set_label('Water depth (m)') # name for color scale ne fonctionne pas car cette ligne appartient a basemap language\n",
    "ax.set_title('Floods extent with a\\nreturn period of 100 years') # set a title to the image\n",
    "\n",
    "#close\n",
    "floods_clip.close()\n",
    "floods.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2421d5",
   "metadata": {},
   "source": [
    "#### Sea level rise\n",
    "\n",
    "Global Mean Sea Level\n",
    "Source : https://climate.nasa.gov/vital-signs/sea-level/ ; used the Global Mean Sea Level (Global Isostatic Adjustment (GIA) applied) variation (mm) with respect to 20-year TOPEX/Jason collinear mean reference \n",
    "\n",
    "Sea Level Trends\n",
    "Source : https://sealevel.colorado.edu/trend-map : sea level trend from 1992 to 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0881784",
   "metadata": {},
   "source": [
    "<a id='second_effect_climate_variables'></a>\n",
    "# Second effect climate variables\n",
    "\n",
    "[Home](#beginning_CRVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6fbd21",
   "metadata": {},
   "source": [
    "<a id='cyclone_risk'></a>\n",
    "## Cyclones\n",
    "data source: https://wesr.unepgrid.ch/?project=MX-XVK-HPH-OGN-HVE-GGN&language=en UNEP-GRID, \n",
    "\n",
    "cyclone risk level from 1 (low) to 5 (extreme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cyclone data path\n",
    "cyclone_path=os.path.join(data_folder,'cyclones/unep_grid/cy_risk.tif')\n",
    " \n",
    "   # \\\\COWI.net\\projects\\A245000\\A248363\\CRVA\\Datasets\\cyclones\\unep_grid\n",
    "#spatial stats\n",
    "projects['cyclone_risk']=pd.DataFrame(zonal_stats(vectors=projects_buf, raster=cyclone_path, all_touched=True, stats='max'))\n",
    "\n",
    "#show table\n",
    "projects[[project_id,'cyclone_risk']].head()\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots()\n",
    "cyclone = rioxarray.open_rasterio(cyclone_path)\n",
    "cyclone_clip = cyclone.rio.clip(study_area.geometry, study_area.crs, drop=True)\n",
    "cyclone_clip.plot(ax=ax)\n",
    "projects.plot(ax=ax) # plot the projects on the map\n",
    "\n",
    "plt.title('Cyclone risk level') # title of the graph\n",
    "\n",
    "#close\n",
    "cyclone_clip.close()\n",
    "cyclone.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c263a351",
   "metadata": {},
   "source": [
    "<a id='climate_change_info_for_each_project_in_study_area'></a>\n",
    "# Climate change information concerning the projects in the study area\n",
    "\n",
    "Display climate change information concerning the project you are looking into.\n",
    "\n",
    "[Home](#beginning_CRVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b397fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of projects in the study area\n",
    "#RAPY: not useful since study area is just for plotting purposes, projects should be only the relevant ones already\n",
    "#projects_of_interest=projects[projects.within(study_area)]\n",
    "\n",
    "# selection of the information to display\n",
    "list_of_names=['Name','province','climatology-hd40_2020-2039_ssp245_median',\n",
    "               'daily-median-Wind-2100-ssp5_8_5','landslides_THmax','floods100yr_max','cyclone_risk']\n",
    "\n",
    "# display selected climate change information of the projects in the study area\n",
    "projects_of_interest_filtered = projects[list_of_names]\n",
    "projects_of_interest_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae4afe",
   "metadata": {},
   "source": [
    "# Export\n",
    "Exports the calculated climate variables to a csv file\n",
    "\n",
    "[Home](#beginning_CRVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects.T.to_csv('outputs/csv/projects_climate_risks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c912f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1702c5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
