{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64945eb",
   "metadata": {},
   "source": [
    "This file aims to regroup all function involved in file management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27ad997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7bc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gros bug sur cette function\n",
    "\n",
    "def download_extract(path_file,path_for_file):\n",
    "    #if not os.path.isdir(path_for_file): # path_for_file does not exists, need to ensure that is is created\n",
    "    #    os.makedirs(path_for_file) # to ensure the creation of the path\n",
    "    # unzip the downloaded file\n",
    "    from zipfile import ZipFile\n",
    "  \n",
    "    # loading the temp.zip and creating a zip object\n",
    "    os.chdir(path_file)\n",
    "    with ZipFile(path_for_file, 'r') as zObject:\n",
    "      \n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "        print(zObject)\n",
    "        zObject.extractall()\n",
    "    \n",
    "    print('\\n ----------------------------- The downloaded file is extracted in the indicated file -----------------------------')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7bda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_length(str1):\n",
    "    if len(str1)>250:\n",
    "        path = os.path.abspath(str1) # normalize path\n",
    "        if path.startswith(u\"\\\\\\\\\"):\n",
    "            path=u\"\\\\\\\\?\\\\UNC\\\\\"+path[2:]\n",
    "        else:\n",
    "            path=u\"\\\\\\\\?\\\\\"+path\n",
    "        return path\n",
    "    else:\n",
    "        return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b3ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_file(path):\n",
    "    name_variable = find_column_name(path)\n",
    "    \n",
    "    #df=Dataset(path)\n",
    "    \n",
    "    lat=get_data_nc(path,'lat')\n",
    "    lon=get_data_nc(path,'lon')\n",
    "    time=get_data_nc(path,'time')\n",
    "    variable=return_NaN(path,name_variable)\n",
    "    \n",
    "    return lat, lon, time, variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e6537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_nc(path,name_variable):\n",
    "    variable = np.ma.getdata(Dataset(path).variables[name_variable]).data\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38dd9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return column name in the netCDF file\n",
    "# all netCDF file form copernicus have this format for their variables names\n",
    "# ['time', 'time_bnds', 'lat', 'lat_bnds', 'lon', 'lon_bnds', Name of climate variable of interest]\n",
    "# take of 'time', 'time_bnds', 'lat', 'lat_bnds', 'lon', 'lon_bnds'\n",
    "def find_column_name(path):\n",
    "    # make a list with every variables of the netCDF file of interest\n",
    "    climate_variable_variables=list(Dataset(path).variables)\n",
    "    # variables that are not the column name of interest \n",
    "    elements_not_climate_var =['time', 'time_bnds', 'bnds','lat', 'lat_bnds', 'lon', 'lon_bnds','time_bounds','bounds','lat_bounds','lon_bounds','height']\n",
    "    for str in elements_not_climate_var:\n",
    "        if str in climate_variable_variables:\n",
    "            climate_variable_variables.remove(str)\n",
    "    return climate_variable_variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b31939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction ne fonctionne pas\n",
    "\n",
    "def return_NaN(path,name_variable):\n",
    "    variable = get_data_nc(path,name_variable)\n",
    "    value_NaN = Dataset(path).variables[name_variable]._FillValue\n",
    "    import math\n",
    "    #variable[variable==value_NaN] = math.nan#float('NaN')\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1caa2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function aims to convert the vector of the time vector from Unix time to the format '%Y-%m-%d'\n",
    "# the function us the 2 following functions 'extract_start_date' and 'time_conversion'\n",
    "def time_vector_conversion(path,resolution):\n",
    "    from datetime import date # package to work with date and time\n",
    "    (year,month,day) = extract_start_date(path) # next function, returning the year, month and day of the start date\n",
    "    start = date(year,month,day) # the function date, with some number in the format int as input\n",
    "    # the function date return the date in format 'YYYY-MM-DD'\n",
    "    time=get_data_nc(path,'time') # get data from the nc file\n",
    "    time_converted = [] # create an empty list, to register the converted time vector\n",
    "    # for loop to convert the date from Unix to the format '%Y-%m-%d'\n",
    "    for day in time:\n",
    "        # add each converted time in the list 'time_converted'\n",
    "        time_converted.append(time_conversion(day,start,resolution)) # convert time with the function 'time_conversion'\n",
    "        # the function the function 'time_conversion'\n",
    "    return time_converted # return the list with time converted in format '%Y-%m-%d'\n",
    "\n",
    "# this function aims to extract the year, month and day of the start date\n",
    "# the input is the path leading to the file of interest\n",
    "def extract_start_date(path):\n",
    "    # start date is after the str 'days since ' in units in the path indicated as input of the function\n",
    "    start_date=Dataset(path).variables['time'].units.replace('days since ','') # the start_date in string format\n",
    "    # next step is to extract and convert in int format the information in te str start_date\n",
    "    # the year is always the 4 first elements of the str start_date\n",
    "    year = int(start_date[0:4])\n",
    "    # the month is always between the first '-' and the second '-' the str start_date\n",
    "    month = int(start_date[start_date.find('-')+1:start_date.rfind('-')])\n",
    "    # the day is always after the second '-' and before the end the str start_date\n",
    "    day = int(start_date[start_date.rfind('-')+1:len(start_date)])\n",
    "    return year,month,day # return year, mont and day in int format\n",
    "\n",
    "# this function convert time from unix in str format\n",
    "# input : \n",
    "##### days is the number representing a time in Unix\n",
    "##### start is the date in format 'YYYY-MM-DD'\n",
    "##### resolution : in str form. can be 'monthly' or 'daily'\n",
    "def time_conversion(days,start,resolution):\n",
    "    from datetime import timedelta # import the function timedelta\n",
    "    if not days.dtype == int:\n",
    "        # the days is not in int format\n",
    "        days = int(days)\n",
    "    # use the function timedelta, with an int as imput\n",
    "    delta = timedelta(days) # Create a time delta object from the number of days\n",
    "    offset = start + delta # add the delta to the start date\n",
    "    # depending on the resolution, converted the offset in str format with strftime function\n",
    "    if resolution == 'monthly':\n",
    "        offset = offset.strftime('%Y-%m')\n",
    "    if resolution == 'daily':\n",
    "        offset = offset.strftime('%Y-%m-%d')\n",
    "    return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xr_array(data,coordonates):\n",
    "    data_structure = xr.DataArray(data, coords=[times, locs], dims=[\"time\", \"space\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(file_name):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7425d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe_copernicus functions aims to test if the data with the specific parameters exists (with copernicus_data)\n",
    "# and then produce a csv file if the data exists\n",
    "\n",
    "def create_dataframe(temporal_resolution,year_str,experiments,models,out_path, name_variable, name_project,area,period,index_dates,dates,path_for_csv,title_file):    \n",
    "   \n",
    "    df = pd.DataFrame() # create an empty dataframe\n",
    "    for SSP in experiments:\n",
    "        experiment = (SSP,) # create tuple for iteration of dataframe\n",
    "        print('Test with scenario '+SSP)\n",
    "        for model_simulation in models:\n",
    "            model =(model_simulation,) # create tuple for iteration of dataframe\n",
    "            print('Test with model '+model_simulation)\n",
    "            # path were the futur downloaded file is registered\n",
    "            #path_for_file= os.path.join(out_path,'Datasets','NEX-GDDP-CMIP6',name_variable,name_project,SSP,model_simulation,period)\n",
    "            # existence of path_for_file tested in copernicus function\n",
    "            # climate_variable_path=copernicus_data(temporal_resolution,SSP,name_variable,model_simulation,year_str,area,path_for_file,out_path,name_project,source)\n",
    "            climate_variable_path = find_path_file(out_path,url_list,variable,model,scenario,year)\n",
    "            # area is determined in the \"Load shapefiles and plot\" part\n",
    "            if (climate_variable_path is not None):\n",
    "                # register data concerning each project under the form of a csv, with the model, scenario, period, latitude and longitude\n",
    "                df=register_data(climate_variable_path,name_project,index_dates,dates,experiment,model,df)\n",
    "                print('\\nValue were found for the period and the project tested\\n')\n",
    "            else:\n",
    "                print('\\nNo value were found for the period and the project tested\\n')\n",
    "                continue # do the next for loop\n",
    "        # test if dataframe is empty, if values exist for this period\n",
    "    if not df.empty: # if dataframe is not empty, value were registered, the first part is run : a path to register the csv file is created, and the dataframe is registered in a csv file\n",
    "        full_name = os.path.join(path_for_csv,title_file)\n",
    "        print(full_name)\n",
    "        df.to_csv(full_name) # register dataframe in csv file\n",
    "        return #df,period \n",
    "    else: # if the dataframe is empty, no value were found, there is no value to register or to return\n",
    "        #os.remove(path_for_file)# remove path\n",
    "        return #df,period# there is no dataframe to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path_file(out_path,name_file_list,variable,model,scenario,year):\n",
    "    \n",
    "    path = os.path.join(out_path,file_name)\n",
    "    return path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
