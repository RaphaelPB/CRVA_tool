{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7111baef",
   "metadata": {},
   "source": [
    "This notebook is meant to download data from copernicus CMIP6.\n",
    "\n",
    "Data source : https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### User input\n",
    "global_variable = 'sfcWind'\n",
    "name_variable = 'near_surface_wind_speed'\n",
    "\n",
    "temporal_resolution = 'daily'\n",
    "\n",
    "y_start = 1950\n",
    "y_end = 2014\n",
    "\n",
    "# wind register at 10 m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9765116",
   "metadata": {},
   "source": [
    "# Functions and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc77fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import netCDF4 as nc#not directly used but needs to be imported for some nc4 files manipulations, use for nc files\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883436b",
   "metadata": {},
   "source": [
    "# Out path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc2a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path=r'C:\\Users\\CLMRX\\OneDrive - COWI\\Documents\\GitHub\\CRVA_tool\\outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826f7c7",
   "metadata": {},
   "source": [
    "# Project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac290aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_projects = np.array(['WTP_Mutua_EIB', 'Gorongosa_EIB', 'Chimoio_WTP_EIB', 'Pemba_EIB'])\n",
    "\n",
    "lon_projects_data = np.array([34.5927839939706, 34.07824286310398 , 33.47333313659342, 40.52545156033736])\n",
    "lon_projects = pd.Series(lon_projects_data)\n",
    "\n",
    "lat_projects_data = np.array([-19.495079648575242, -18.68063728746643, -19.125095255188334,-12.973942656747809])\n",
    "lat_projects = pd.Series(lat_projects_data)\n",
    "\n",
    "area_projects = [lat_projects - 5, lat_projects+5, lon_projects-5,lon_projects+5] # list format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56523c",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9e014",
   "metadata": {},
   "source": [
    "### Calendar class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f65248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to define parameter of time that remain constant durinf the whole script\n",
    "class calendar:\n",
    "    default_month = [ \n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                ]\n",
    "    default_day = [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "                ]\n",
    "    actual_date = datetime.date.today()\n",
    "    actual_year = actual_date.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6921c5b",
   "metadata": {},
   "source": [
    "### Copernicus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of tuples that will be useful to search which data are available or not\n",
    "# make it tuples to make unchangeable\n",
    "class copernicus_elements:\n",
    "    # there is 58 models\n",
    "    models =('access_cm2','awi_cm_1_1_mr','bcc_csm2_mr','cams_csm1_0','canesm5_canoe','cesm2_fv2','cesm2_waccm_fv2','cmcc_cm2_hr4','cmcc_esm2','cnrm_cm6_1_hr','e3sm_1_0','e3sm_1_1_eca','ec_earth3_aerchem','ec_earth3_veg','fgoals_f3_l','fio_esm_2_0','giss_e2_1_g','hadgem3_gc31_ll','iitm_esm','inm_cm5_0','ipsl_cm6a_lr','kiost_esm','miroc6','miroc_es2l','mpi_esm1_2_hr','mri_esm2_0','norcpm1','noresm2_mm','taiesm1','access_esm1_5','awi_esm_1_1_lr','bcc_esm1','canesm5','cesm2','cesm2_waccm','ciesm','cmcc_cm2_sr5','cnrm_cm6_1','cnrm_esm2_1','e3sm_1_1','ec_earth3','ec_earth3_cc','ec_earth3_veg_lr','fgoals_g3','gfdl_esm4','giss_e2_1_h','hadgem3_gc31_mm','inm_cm4_8','ipsl_cm5a2_inca','kace_1_0_g','mcm_ua_1_0','miroc_es2h','mpi_esm_1_2_ham','mpi_esm1_2_lr','nesm3','noresm2_lm','sam0_unicon','ukesm1_0_ll')\n",
    "    experiments = ('ssp1_1_9','ssp1_2_6','ssp4_3_4','ssp5_3_4os','ssp2_4_5','ssp4_6_0','ssp3_7_0','ssp5_8_5')\n",
    "    #'ssp1_1_9',\n",
    "    experiments_historical=('historical',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a99de",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00666731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separer la fonciton qui download de celle qui fait les files ?? Comme ca possible de choisir closest lat et lon avec fichier existant,\n",
    "# ou bien laisser comme ca mais trouver un momyen pour que ca run pas a chaque fois, ca doit runner une fois pour chaque projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de719fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Register data from nc file of Copernicus ############################################\n",
    "# Aim of the function: this function aims to register in a dataframe and a csv file the data from the nc file downloaded with\n",
    "# the function copernicus_data\n",
    "# Actions of this function\n",
    "#     1) Create the string indicating the period of interest\n",
    "#     2) Creating path and file name to register dataframe in csv file\n",
    "#     3) Register data, with its corresponding experiments and models, in dataframe and csv file\n",
    "#        3 a) Test if path does not exists (if dataframe is not registered) : \n",
    "#                1 . Thanks to copernicus_data, download nc fils from copernicus CMIP6 website for each experiment and each model\n",
    "#                2 . Open the dowloaded nc file in the jupyter notebook if it exists\n",
    "#                3 . In a dataframe, register the value in the nc file, for each experiment, model and day\n",
    "#                4 . If there no value for each experiments and models tested, the datfram is empty and the user is informed\n",
    "#        3 b) Test if path exists (dataframe is registered) : no need to register again, return in dataframe the existing \n",
    "#             csv file in a dataframe\n",
    "\n",
    "# Parameters of the function\n",
    "# temporal_resolution: 'daily', 'monthly', or 'fixed'. String type \n",
    "# year_str: list containing all the years under the string type and in the period of interest\n",
    "# experiments: copernicus_elements.experiments\n",
    "# models: copernicus_elements.models\n",
    "# out_path: path were the outputs are registered. Defined by the user at the beginning of the code \n",
    "# global_variable: global name of the climate variable of interest (example: Wind)\n",
    "# name_variable: name of the elements downloaded from copernicus (example: 'near_surface_wind_speed')\n",
    "# name_project: Name of the project for which the data are taken\n",
    "# area: list containing latitudes and logitudes around the project\n",
    "\n",
    "def csv_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_projects,area,source):    \n",
    "    ### PROBLEM WITH DATES, CAN T just pass one year. year str is a list, so if one year (2020,)\n",
    "    ## PROBLEM WITH PATH: not coherent between data csv, datasets, download. And not achieving to have project name in path for dataset\n",
    "    ## maybe the name for dataset is too long, but even if end at name project, does not work. Try doing one string with name project in it\n",
    "    ## PROBLEM WITH PATH: WORK BUT NOT IDEAL\n",
    "    ## pourquoi mettre toutes les donnees dans un dataframe ?? permet d'avoir cette organisation en multiindex. Sinon, on ne peut pas faire ca\n",
    "    df_final = []\n",
    "    \n",
    "    for name_project in name_projects:\n",
    "        k = 0 # to register closest latitudes and longitude for each project\n",
    "        print('############################### Project name: '+name_project+' ###############################')\n",
    "\n",
    "        # create string for name of folder depending on type of period\n",
    "        if temporal_resolution == 'fixed':\n",
    "            period = 'fixed'\n",
    "        else:\n",
    "            period=year_str[0]+'-'+year_str[len(year_str)-1]\n",
    "\n",
    "        # modification on name_project str to ensure no problem whent using this str as name of a folder\n",
    "        name_project = name_project.replace('-','_') # take off every blank space of project names\n",
    "        name_project = name_project.replace('/','_') # take off every / of project names\n",
    "        name_project = name_project.replace(r'\"\\\"','_') # take off every \\ of project names\n",
    "        # brackets shouldn't be a problem for name projects\n",
    "\n",
    "        (dates, index_dates)=date_copernicus(temporal_resolution,year_str) # create time vector depending on temporal resolution\n",
    "\n",
    "        title_file = name_project +'_' +period+ '_' + temporal_resolution + '_' +name_variable#+'.csv'\n",
    "\n",
    "        path_for_csv = os.path.join(out_path,'csv',source,name_variable,name_project,period) # create path for csv file\n",
    "\n",
    "        if not os.path.isdir(path_for_csv): # test if the data were already downloaded; if not, first part if the if is applied\n",
    "            os.makedirs(path_for_csv) # to ensure the creation of the path\n",
    "            # the dataframe_copernicus functions aims to test if the data with the specific parameters exists (with copernicus_data)\n",
    "            # and then produce a csv file if the data exists\n",
    "            df=dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,area,period,index_dates,dates,path_for_csv,title_file,source)\n",
    "            #return df\n",
    "        else:# test if the data were already downloaded; if yes, this part of the if is applied\n",
    "            if len(os.listdir(path_for_csv)) == 0: #test if the directory is empty\n",
    "                # the csv file does not exist, even if the path exist\n",
    "                # the dataframe_copernicus functions aims to test if the data with the specific parameters exists (with copernicus_data)\n",
    "                # and then produce a csv file if the data exists\n",
    "                df=dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,area,period,index_dates,dates,path_for_csv,title_file,source)\n",
    "            else: # the directory is not empty\n",
    "                df=file_already_downloaded(path_for_csv,title_file)\n",
    "                \n",
    "        df_final = pd.concat([df_final,df])\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe_copernicus functions aims to test if the data with the specific parameters exists (with copernicus_data)\n",
    "# and then produce a csv file if the data exists\n",
    "\n",
    "def dataframe_copernicus(temporal_resolution,year_str,experiments,models,out_path, global_variable, name_variable, name_project,area,period,index_dates,dates,path_for_csv,title_file,source):    \n",
    "    print('FUNCTION DATAFRAME_COPERNICUS')\n",
    "    df = pd.DataFrame() # create an empty dataframe\n",
    "    for SSP in experiments:\n",
    "        experiment = (SSP,) # create tuple for iteration of dataframe\n",
    "        print('Test with scenario '+SSP)\n",
    "        for model_simulation in models:\n",
    "            model =(model_simulation,) # create tuple for iteration of dataframe\n",
    "            print('Test with model '+model_simulation)\n",
    "            # path were the futur downloaded file is registered\n",
    "            path_for_file= os.path.join(out_path,'Datasets',name_variable,name_project,SSP,model_simulation,period)\n",
    "            # existence of path_for_file tested in copernicus function\n",
    "            climate_variable_path=copernicus_data(temporal_resolution,SSP,name_variable,model_simulation,year_str,area,path_for_file,out_path,name_project,source)\n",
    "            # area is determined in the \"Load shapefiles and plot\" part\n",
    "            if (climate_variable_path is not None):\n",
    "                # register data concerning each project under the form of a csv, with the model, scenario, period, latitude and longitude\n",
    "                df=register_data(climate_variable_path,name_project,index_dates,dates,experiment,model,df)\n",
    "                print('\\nValue were found for the period and the project tested\\n')\n",
    "            else:\n",
    "                print('\\nNo value were found for the period and the project tested\\n')\n",
    "                continue # do the next for loop\n",
    "        # test if dataframe is empty, if values exist for this period\n",
    "    if not df.empty: # if dataframe is not empty, value were registered, the first part is run : a path to register the csv file is created, and the dataframe is registered in a csv file\n",
    "        full_name = os.path.join(path_for_csv,title_file)\n",
    "        print(full_name)\n",
    "        df.to_csv(full_name) # register dataframe in csv file\n",
    "        return df \n",
    "    else: # if the dataframe is empty, no value were found, there is no value to register or to return\n",
    "        #os.remove(path_for_file)# remove path\n",
    "        return #df,period# there is no dataframe to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd75d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register data concerning each project under the form of a csv, with the model, scenario, period, latitude and longitude\n",
    "def register_data(climate_variable_path,name_project,index_dates,dates,experiment,model,df):\n",
    "    print('Registering the data in a dataframe')\n",
    "    Open_path = Dataset(climate_variable_path) # open netcdf file\n",
    "    lat_dataframe = np.ma.getdata(Open_path.variables['lat']).data\n",
    "    lon_dataframe = np.ma.getdata(Open_path.variables['lon']).data\n",
    "    column_name = find_column_name(Open_path)\n",
    "    data_with_all = ma.getdata(Open_path.variables[column_name]).data\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### THING TO DO \n",
    "    # register only closest lat lon\n",
    "    # mettre cette function plus tot : index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon = _lat_lon(path,lat_projects,lon_projects)\n",
    "\n",
    "    for moment in index_dates: # case if temporal resolution is daily\n",
    "        data_dataframe = data_with_all[moment,:,:]\n",
    "        Date = (dates[moment],) # create tuple for iteration of dataframe\n",
    "        Name_Project = (name_project,)\n",
    "\n",
    "        # Create the MultiIndex\n",
    "        midx = pd.MultiIndex.from_product([Name_Project,experiment, model, Date, lat_dataframe],names=['Name project','Experiment', 'Model', 'Date', 'Latitude'])\n",
    "        # multiindex to name the columns\n",
    "        lon_str = ('Longitude',)\n",
    "        cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "        # Create the Dataframe\n",
    "        Variable_dataframe = pd.DataFrame(data = data_dataframe, \n",
    "                                    index = midx,\n",
    "                                    columns = cols)\n",
    "        # Concatenate former and new dataframe\n",
    "        df = pd.concat([df,Variable_dataframe])# register information for project\n",
    "\n",
    "    Open_path.close # to spare memory\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return column name in the netCDF file\n",
    "# all netCDF file form copernicus have this format for their variables names\n",
    "# ['time', 'time_bnds', 'lat', 'lat_bnds', 'lon', 'lon_bnds', Name of climate variable of interest]\n",
    "# take of 'time', 'time_bnds', 'lat', 'lat_bnds', 'lon', 'lon_bnds'\n",
    "def find_column_name(Open_path):\n",
    "    # make a list with every variables of the netCDF file of interest\n",
    "    climate_variable_variables=list(Open_path.variables)\n",
    "    # variables that are not the column name of interest \n",
    "    elements_not_climate_var =['time', 'time_bnds', 'bnds','lat', 'lat_bnds', 'lon', 'lon_bnds','time_bounds','bounds','lat_bounds','lon_bounds','height']\n",
    "    for str in elements_not_climate_var:\n",
    "        if str in climate_variable_variables:\n",
    "            climate_variable_variables.remove(str)\n",
    "    return climate_variable_variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_already_downloaded(path_for_csv,title_file):\n",
    "    print('The file was already downloaded')\n",
    "    df = pd.read_csv(os.path.join(path_for_csv,title_file)) # read the downloaded data for the analysis\n",
    "\n",
    "    # changing name of columns\n",
    "    name_columns=df.iloc[0].array\n",
    "    df.rename(columns={'Unnamed: 0':'Experiment','Unnamed: 1':'Model','Unnamed: 2':'Date','Unnamed: 3':'Latitude'}, inplace=True)\n",
    "\n",
    "    lon_dataframe=name_columns[4:len(name_columns)] # register data for columns of multiindex\n",
    "\n",
    "    df.drop([0,1], axis=0,inplace=True) # remove 2 first lines\n",
    "\n",
    "    # recreate multiindex \n",
    "\n",
    "    # .... with columns\n",
    "    df.set_index(['Name project','Experiment', 'Model', 'Date','Latitude'],inplace=True)\n",
    "\n",
    "    # .... with lines\n",
    "    lon_str = ('Longitude',)\n",
    "    cols = pd.MultiIndex.from_product([lon_str,lon_dataframe])\n",
    "    df.columns=cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Period for copernicus function ################################################\n",
    "# Aim of the function: by giving it a first and last year of the period that must analyzed, this function produce several \n",
    "# vectors,containing time informations, useful to download and treat data from CMIP6 projections (https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview )\n",
    "# Those time vectors are used in the copernicus_data and the dataframe_copernicus and csv_copernicus functions\n",
    "\n",
    "# function year_copernicus produce \n",
    "# year: a vector containing all the year in the period of interest\n",
    "# year_str: an array containing all the year in the period of interest in the string format\n",
    "# index: an array containing the index of the year and year_str\n",
    "#### Parameters of the function\n",
    "# first_year: number in int format, of the first year of the period of interest\n",
    "# last_year: number in int format, of the last year of the period of interest\n",
    "def year_copernicus(first_year,last_year):\n",
    "    year = np.arange(first_year,(last_year+1),1) # create vector of years\n",
    "    year_str = [0]*len(year) # create initiale empty vector to convert years in int\n",
    "    index = np.arange(0,len(year)) # create vector of index for year\n",
    "    i = 0 # initialize index\n",
    "    for i in index: # convert all the date in string format\n",
    "        year_str[i]=str(year[i])\n",
    "    return (year, year_str, index)\n",
    "\n",
    "# function date_copernicus produce \n",
    "# dates: the format depend on the temporal reolution, but always contain the dates of the period of interest.\n",
    "#        with temporal_resolution=daily, dates is a DatetimeIndex\n",
    "#        with temporal_resolution=monthly, dates is a list\n",
    "# index_dates: an array containing the index of the dates\n",
    "#### Parameters of the function\n",
    "# temporal_resolution: daily or monthly\n",
    "# year_str: ???? produce by function year_copernicus, containing the year of the period of interest in string format\n",
    "def date_copernicus(temporal_resolution,year_str):\n",
    "    start_date = \"01-01-\"+year_str[0] # string start date based on start year\n",
    "    stop_date = \"31-12-\"+year_str[len(year_str)-1] # string stop date based on stop year\n",
    "    if temporal_resolution =='daily':\n",
    "        # vector of dates between start date and stop date\n",
    "        dates = pd.date_range(start_date,stop_date)# dates is a pandas.core.indexes.datetimes.DatetimeIndex\n",
    "        # By default, freq = 'D', which means calendar day frequency (source : https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases)\n",
    "        #index_dates = np.arange(0,len(dates)) # vector containning index o dates vector\n",
    "    if temporal_resolution =='monthly':\n",
    "        dates = pd.date_range(start_date,stop_date,freq='MS') # vector of dates between start date and stop date\n",
    "        dates=list(dates.strftime('%m-%Y')) # dates is an pandas.core.indexes.base.Index, not a pandas.core.indexes.datetimes.DatetimeIndex\n",
    "    #if temporal_resolution =='fixed': trouver donnees pour gerer cela\n",
    "    index_dates = np.arange(0,len(dates)) # vector containning index o dates vector\n",
    "    return (dates, index_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd15378",
   "metadata": {},
   "source": [
    "### Copernicus function\n",
    "Some data comes from copernicus and can be directly taken form the website thans to CDS. The following functions serves this purpose\n",
    "#### Parameters of the function :\n",
    "projections-cmip6 : name of the web page, in this case, 'projections-cmip6'\n",
    "format : zip or tar.gz\n",
    "temporal_resolution : daily or monthly or fixed\n",
    "SSP : sscenario that is studied \"Historical\", \"SSP1-1.9\", \"SSP1-2.6\" ...\n",
    "Variable : variable to be studied\n",
    "model: model of projection to choose\n",
    "year: year of study to choose\n",
    "area: area of study\n",
    "month: month to be studied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### Copernicus data function ###################################################\n",
    "# Aim of the function : read nc data found on copernicus CMIP6 projections (https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview )\n",
    "# Actions of this function\n",
    "#     1) check which parameters are asked or not in the variables dictionnary, and modify the last depend on the parameters \n",
    "#        chosen by the user before\n",
    "#     2) All this step is done in function try_download_copernicus: thanks to c.retrieve function and the variables dictionnary, \n",
    "#        the chosen data are download in zip format, dezipped and registered in a specific folder. \n",
    "#     3) the function looks in the specific folder for a nc format file, and once found, return the path of this nc format file\n",
    "\n",
    "#### Parameters of the function\n",
    "# temporal_resolution : daily or monthly or fixed\n",
    "# SSP : sscenario that is studied \"Historical\", \"SSP1-1.9\", \"SSP1-2.6\" ...\n",
    "# name_variable : variable to be studied\n",
    "# model: model of projection to choose\n",
    "# year: year(s) of study to choose\n",
    "# area: area of study, if not specific, area should be an empty array area=[]\n",
    "# path_for_file: path where the file must be unzipped\n",
    "# out_path: path were all the outputs are registered, defined by the user in the begining of the main code\n",
    "# name_area : to specify if we are only looking data for a project or for a wider zone\n",
    "\n",
    "def copernicus_data(temporal_resolution,SSP,name_variable,model,year,area,path_for_file,out_path,name_area,source): \n",
    "    # create path for the downloaded file\n",
    "    start_path = os.path.join(out_path,'Data_download_zip')\n",
    "    file_download=create_file_download_path(start_path,name_variable,name_area,SSP,model,year,temporal_resolution,source) \n",
    "    if not os.path.isdir(path_for_file):\n",
    "        print('path_for_file does not exist: the data may not have been downloaded') \n",
    "        if not os.path.isdir(file_download):\n",
    "            print('file_download does not exist: the data were not downloaded')\n",
    "            # function try to download from copernicus\n",
    "            path_file = try_download_copernicus(temporal_resolution,SSP,name_variable,model,area,year,path_for_file,file_download,source)\n",
    "            if path_file is None: # for this climate variable, the parameter do not fit\n",
    "                return path_file\n",
    "            final_path=search_for_nc(path_file) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "            print('\\n')\n",
    "            print('---------------  Path to nc file exists ?? ---------------\\n')\n",
    "            print(os.path.isfile(final_path))\n",
    "            print('\\n')\n",
    "            return final_path\n",
    "            \n",
    "        else: # if the path already exist, the data in zip format should also exists\n",
    "            print('file_download does exist, the data have been downloaded, but not extracted')\n",
    "            #path_file=os.path.join(path_for_file,source)# source was added because of a problem during downloading\n",
    "            final_path=download_extract(path_for_file,file_download,source)\n",
    "            #final_path=search_for_nc(path_file) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "            #if final_path is None:# if no nc file exists, need to check again if the file with those parameters exists\n",
    "            #    final_path = try_download_copernicus(temporal_resolution,SSP,name_variable,model,area,year,path_for_file,file_download,source)\n",
    "            final_path = search_for_nc(final_path) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "            return final_path\n",
    "                \n",
    "    else: # the path for the file exists\n",
    "        if not os.listdir(path_for_file): # if the path is empty\n",
    "            final_path=download_extract(path_for_file,file_download,source)\n",
    "            final_path = search_for_nc(final_path) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "        else: # if the path is not empty\n",
    "            path_file=os.path.join(path_for_file,source)# data was added because of a problem during downloading\n",
    "            final_path=search_for_nc(path_file) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "            if final_path is None: # if no nc file exists, need to check again if the file with those parameters exists\n",
    "                test= os.path.join(file_download,source,'download.zip')\n",
    "                if not os.path.join(test):# the file was not downloaded \n",
    "                    final_path = try_download_copernicus(temporal_resolution,SSP,name_variable,model,area,year,path_for_file,file_download,source)\n",
    "                else: # the file was already downloaded but not not extracted\n",
    "                    final_path=download_extract(path_for_file,file_download,source)\n",
    "                final_path = search_for_nc(final_path) # looking for the netCDF file in format .nc and look if path length is a problem at the same time\n",
    "        return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_download_copernicus(temporal_resolution,SSP,name_variable,model,area,year,path_for_file,file_download,source):\n",
    "    c = cdsapi.Client()# function to use the c.retrieve\n",
    "    # basic needed dictionnary to give to the c.retrieve function the parameters asked by the user\n",
    "    variables = {\n",
    "                'format': 'zip', # this function is only designed to download and unzip zip files\n",
    "                'temporal_resolution': temporal_resolution,\n",
    "                'experiment': SSP,\n",
    "                'variable': name_variable,\n",
    "                'model': model,\n",
    "    }\n",
    "\n",
    "    if area != []: # the user is interested by a sub region and not the whole region \n",
    "        variables.update({'area':area}) \n",
    "\n",
    "    if name_variable == 'air_temperature':\n",
    "        variables['level'] = '1000' # [hPa], value of the standard pressure at sea level is 1013.25 [hPa], so 1000 [hPa] is the neareste value. Other pressure value are available but there is no interest for the aim of this project\n",
    "\n",
    "    if temporal_resolution != 'fixed':# if 'fixed', no year, month, date to choose\n",
    "        variables['year']=year # period chosen by the user\n",
    "        variables['month']= calendar.default_month  # be default, all the months are given; defined in class calendar\n",
    "        if temporal_resolution == 'daily':\n",
    "            variables['day']= calendar.default_day # be default, all the days are given; defined in class calendar\n",
    "    # c.retrieve download the data from the website\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'projections-cmip6',\n",
    "            variables,\n",
    "            'download.zip') # the file in a zip format is registered in the current directory\n",
    "    except:\n",
    "        print('Some parameters are not matching')\n",
    "        return # stop the function, because some data the user entered are not matching\n",
    "    \n",
    "    # function to extract the downloaded zip\n",
    "    path_file=download_extract(path_for_file,file_download,source)\n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_extract functions aims to return the path were the downloaded file in zip format is extracted\n",
    "\n",
    "def download_extract(path_for_file,file_download,source):\n",
    "    if not os.path.isdir(path_for_file): # path_for_file does not exists, need to ensure that is is created\n",
    "        os.makedirs(path_for_file) # to ensure the creation of the path\n",
    "    # unzip the downloaded file\n",
    "    if 'download.zip' not in os.listdir(): # check if download is in the working directory\n",
    "        print('The download zip is moved to the working directory')\n",
    "        path_downloaded_zip=os.path.join(file_download,'download.zip')\n",
    "        shutil.move(path_downloaded_zip,r'C:\\Users\\CLMRX\\OneDrive - COWI\\Documents\\GitHub\\CRVA_tool') # move download fil to working directory\n",
    "    \n",
    "    from zipfile import ZipFile\n",
    "    zf = ZipFile('download.zip', 'r')\n",
    "    zf.extractall(source) # if no precision of directory, extract in current directory\n",
    "    zf.close()\n",
    "\n",
    "    if not os.path.isdir(file_download): # path_for_file does not exists, need to ensure that is is created\n",
    "        os.makedirs(file_download) # to ensure the creation of the path\n",
    "    # moving download to appropriate place\n",
    "    #test = os.path.join(file_download,'download.zip')\n",
    "    #if not os.path.isfile(test):\n",
    "    shutil.move('download.zip',file_download) # no need to delete 'download.zip' from inital place\n",
    "    #test = os.path.join(path_for_file,source)\n",
    "    #if not os.path.isdir(test):\n",
    "    shutil.move(source,path_for_file) # move extracted data to the path created for them\n",
    "    path_file=os.path.join(path_for_file,source)\n",
    "    print('\\n The downloaded file is extracted')\n",
    "    return path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seach_for_nc is a function looking in path_for_file for a document in .nc format\n",
    "\n",
    "def search_for_nc(path_for_file):\n",
    "    print('path_for_file does exist Function copernicus search for nc')\n",
    "    for file in os.listdir(path_for_file):\n",
    "        if file.endswith(\".nc\"):\n",
    "            final_path=os.path.join(path_for_file, file)\n",
    "            \n",
    "            print('The file is in the path Function copernicus search for nc\\n')\n",
    "            print('Before path_length, The final path for the nc file is: '+final_path)\n",
    "            answer = str(os.path.isfile(final_path))\n",
    "            print('\\n The final path for nc file exists ? '+answer+'\\n')\n",
    "            final_path=path_length(final_path) # check if length of path is too long\n",
    "            print('After path_length, The final path for the nc file is: '+final_path)\n",
    "            answer = str(os.path.isfile(final_path))\n",
    "            print('\\n The final path for nc file exists ? '+answer+'\\n')\n",
    "            return final_path # the function returns the path of the nc file of interest\n",
    "            break # stop the function if a nc file was found \n",
    "        else:\n",
    "            pass\n",
    "    # the all folder has been search and there is no nc file in it\n",
    "    print('Problem : No nc file was found Function copernicus Function copernicus search for nc')# this line is out of the for loop, \n",
    "    #because it should only appear once all the folder has been examinated and if the break of the if was not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5800aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions test if the path is too long\n",
    "# if the path is more than 250 char, the path wll be modified in order for windows to accept is as a path\n",
    "\n",
    "def path_length(str1):\n",
    "    if len(str1)>250:\n",
    "        path = os.path.abspath(str1) # normalize path\n",
    "        if path.startswith(u\"\\\\\\\\\"):\n",
    "            path=u\"\\\\\\\\?\\\\UNC\\\\\"+path[2:]\n",
    "        else:\n",
    "            path=u\"\\\\\\\\?\\\\\"+path\n",
    "        return path\n",
    "    else:\n",
    "        return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021050b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create path for the downloaded file\n",
    "def create_file_download_path(start_path,name_variable,name_area,SSP,model,year,temporal_resolution,source):\n",
    "    # adapt the name of the folder fot the period, depending on the type of period\n",
    "    if len(year)==1:\n",
    "        file_download = os.path.join(start_path,name_variable,name_area,SSP,model,year,source)\n",
    "    elif len(year)>1:\n",
    "        period=year[0]+'-'+year[len(year)-1]\n",
    "        file_download = os.path.join(start_path,name_variable,name_area,SSP,model,period,source)\n",
    "    elif temporal_resolution == 'fixed':\n",
    "        file_download = os.path.join(start_path,name_variable,name_area,SSP,model,'fixed_period',source)\n",
    "    return file_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9712663",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify index of latitudes and longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9783b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions aims to return the closest latitudes and longitudes to the projects, and the respectives index \n",
    "#  in the lat and lon vectors of the file\n",
    "def _lat_lon(path,lat_projects,lon_projects):\n",
    "    ds =  xr.open_dataset(path) \n",
    "    # ds.indexes['time'] gives back CFTimeIndex format, with hours. The strftime('%d-%m-%Y') permits to have time \n",
    "    # as an index, with format '%d-%m-%Y'. The .values permits to have an array\n",
    "    lat  = ds.lat.values\n",
    "    lon  = ds.lon.values\n",
    "    ds.close() # to spare memory\n",
    "    # preallocate space for the future vectors\n",
    "    index_closest_lat = []\n",
    "    index_closest_lon = []\n",
    "    closest_value_lat = []\n",
    "    closest_value_lon = []\n",
    "    for j in np.arange(0,len(lat_projects)):\n",
    "        (A,B)=closest_lat_lon_to_proj(lat_projects[j],lat)\n",
    "        index_closest_lat.append(A[0])\n",
    "        closest_value_lat.append(B[0])\n",
    "        (C,D)=closest_lat_lon_to_proj(lon_projects[j],lon)\n",
    "        index_closest_lon.append(C[0])\n",
    "        closest_value_lon.append(D[0])\n",
    "    return index_closest_lat,index_closest_lon,closest_value_lat,closest_value_lon\n",
    "\n",
    "\n",
    "# this function aims to select the closest point to the geographical point of the project\n",
    "# the function takes as input \n",
    "#     location_project, which is a numpy.float64\n",
    "#     vector, which is a numpy.ndarray\n",
    "# the function returns\n",
    "#     closest_value[0], a numpy.float64\n",
    "\n",
    "def closest_lat_lon_to_proj(location_project,vector):\n",
    "    # the function any() returns a boolean value. Here, the function test if there are elements in the array \n",
    "    # containing the difference between the vector and the location_project, equal to the minimum of the absolute \n",
    "    # value of the difference between the vector and the location_project\n",
    "    if any(np.where((vector - location_project) == min(abs(vector - location_project)))[0]):\n",
    "        # the function any() returned True\n",
    "        # there is an element in the vector that is equal to the minimum of the absolute value of the difference \n",
    "        # between the vector and the location_project\n",
    "        \n",
    "        # the function np.where() returns the index for which (vector - location_project) == min(abs(vector - location_project))\n",
    "        index_closest = np.where((vector - location_project) == min(abs(vector - location_project)))[0]\n",
    "        closest_value = vector[index_closest]\n",
    "    else:\n",
    "        # the function any() returned False\n",
    "        # there is NO element in the vector that is equal to the minimum of the absolute value of the difference \n",
    "        # between the vector and the location_project\n",
    "        \n",
    "        # the function np.where() returns the index for which (vector - location_project) == -min(abs(vector - location_project))\n",
    "        index_closest = np.where((vector - location_project) == -min(abs(vector - location_project)))[0]\n",
    "        closest_value = vector[index_closest]\n",
    "    return index_closest, closest_value \n",
    "    # the function returns\n",
    "    #     first, the value of the index of the element of vector, that is the closest to location_project    \n",
    "    #     second, the array containing the element of vector, that is the closest to location_project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847ab01",
   "metadata": {},
   "source": [
    "# Register copernicus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f78a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "(year, year_str, index)=year_copernicus(y_start,y_end)\n",
    "\n",
    "for name_project in name_project:\n",
    "    df = csv_copernicus(temporal_resolution,year_str,copernicus_elements.experiments_historical,copernicus_elements.models,out_path,global_variable, name_variable,name_projects,area_projects,'Copernicus-CMIP6')  \n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bdb349d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0   -24.495080\n",
       " 1   -23.680637\n",
       " 2   -24.125095\n",
       " 3   -17.973943\n",
       " dtype: float64,\n",
       " 0   -14.495080\n",
       " 1   -13.680637\n",
       " 2   -14.125095\n",
       " 3    -7.973943\n",
       " dtype: float64,\n",
       " 0    29.592784\n",
       " 1    29.078243\n",
       " 2    28.473333\n",
       " 3    35.525452\n",
       " dtype: float64,\n",
       " 0    39.592784\n",
       " 1    39.078243\n",
       " 2    38.473333\n",
       " 3    45.525452\n",
       " dtype: float64]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d871cc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-23.68063728746643, -13.680637287466428, 29.07824286310398, 39.07824286310398]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=1\n",
    "\n",
    "[area_projects[0][k],area_projects[1][k],area_projects[2][k],area_projects[3][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
